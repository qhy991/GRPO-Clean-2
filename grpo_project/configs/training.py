from dataclasses import dataclass, field
import os
import torch
from typing import Optional, List, Dict, Any
from datetime import datetime

@dataclass
class ScriptConfig:
    """
    Enhanced configuration for training script with multi-GPU model parallel support.
    """
    # --- Core Model Configuration ---
    model_name_or_path: str = field(
        metadata={"help": "Path to pretrained BASE model or model identifier from huggingface.co/models for GRPO training."}
    )
    
    # --- Dataset Configuration ---
    dataset_path: Optional[str] = field(
        default=None, 
        metadata={"help": "Path to a local dataset manifest JSONL file. Used if dataset_name is not provided."}
    )
    dataset_name: Optional[str] = field(
        default=None, 
        metadata={"help": "The name of the dataset to use (via the datasets library). E.g., 'c4', 'wikitext'."}
    )
    dataset_config_name: Optional[str] = field(
        default=None, 
        metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
    )
    dataset_split: str = field(
        default="train", 
        metadata={"help": "The dataset split to use (e.g., 'train', 'validation')."}
    )
    
    # --- Stage1 Adapter Configuration ---
    stage1_adapter_path: Optional[str] = field(
        default=None,
        metadata={"help": "Path to the LoRA adapters from the first stage of training."}
    )

    # ğŸ”§ å¤šGPUæ¨¡å‹å¹¶è¡Œé…ç½®
    use_model_parallel: bool = field(
        default=True,
        metadata={"help": "Enable multi-GPU model parallelism for distributing model weights across GPUs instead of replicating them."}
    )
    max_memory_per_gpu: str = field(
        default="75GiB",
        metadata={"help": "Maximum memory allocation per GPU. Default: 75GiB for 80GB A100s with 5GB buffer."}
    )
    low_cpu_mem_usage: bool = field(
        default=True,
        metadata={"help": "Enable low CPU memory usage mode during model loading."}
    )
    device_map_strategy: str = field(
        default="auto",
        metadata={"help": "Device mapping strategy: 'auto' (automatic), 'balanced' (manual balancing), 'custom' (user-defined)."}
    )
    force_device_map: Optional[Dict[str, int]] = field(
        default=None,
        metadata={"help": "Custom device mapping dictionary. Only used when device_map_strategy='custom'."}
    )
    disable_quantization_for_multi_gpu: bool = field(
        default=True,
        metadata={"help": "Automatically disable quantization when using multi-GPU to avoid compatibility issues."}
    )

    # ğŸ”§ å¢å¼ºçš„LoRAé…ç½®ï¼ˆé’ˆå¯¹å¤šGPUä¼˜åŒ–ï¼‰
    lora_rank: int = field(
        default=64, 
        metadata={"help": "LoRA rank for GRPO stage. Increased for multi-GPU to utilize additional memory."}
    )
    lora_alpha: int = field(
        default=128, 
        metadata={"help": "LoRA alpha for GRPO stage. Scaled with rank (typically 2x rank)."}
    )
    lora_dropout: float = field(
        default=0.05, 
        metadata={"help": "LoRA dropout for GRPO stage."}
    )
    lora_target_modules: Optional[List[str]] = field(
        default_factory=lambda: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        metadata={"help": "List of modules to target for LoRA in GRPO stage."}
    )

    # ğŸ”§ å¤šGPUä¼˜åŒ–çš„é•¿åº¦é…ç½®
    max_seq_length: int = field(
        default=6144, 
        metadata={"help": "Maximum sequence length. Increased for multi-GPU to utilize additional memory."}
    )
    script_max_prompt_length: int = field(
        default=1536, 
        metadata={"help": "Maximum length for input prompts. Default: 1536 tokens (~25% of 6144)."}
    )
    script_max_completion_length: int = field(
        default=4608, 
        metadata={"help": "Maximum length for model completions. Default: 4608 tokens (~75% of 6144)."}
    )
    
    length_allocation_strategy: str = field(
        default="custom", 
        metadata={"help": "Length allocation strategy: 'balanced', 'prompt_heavy', 'completion_heavy', 'custom'."}
    )

    # ğŸš€ æµå¼å¼•å¯¼é…ç½®
    enable_streaming_guidance: bool = field(
        default=True, 
        metadata={"help": "Enable streaming reasoning guidance for better code generation."}
    )
    min_reasoning_length: int = field(
        default=60, 
        metadata={"help": "Minimum reasoning length requirement for guidance trigger."}
    )
    guidance_trigger_threshold: int = field(
        default=40, 
        metadata={"help": "Reasoning length threshold to trigger guidance intervention."}
    )
    max_guidance_attempts: int = field(
        default=2, 
        metadata={"help": "Maximum number of guidance attempts per generation."}
    )
    guidance_tokens_limit: int = field(
        default=25, 
        metadata={"help": "Maximum tokens per guidance intervention."}
    )

    # ğŸ”§ å¤šGPUä¼˜åŒ–çš„å›è°ƒé…ç½®
    callback_num_samples: int = field(
        default=3, 
        metadata={"help": "Number of samples to generate in InferenceCallback."}
    )
    callback_eval_every_n_steps: int = field(
        default=15, 
        metadata={"help": "Frequency of running InferenceCallback. Reduced for multi-GPU to avoid communication overhead."}
    )

    # ç”Ÿæˆå‚æ•°é…ç½®
    gen_temperature: float = field(
        default=0.7, 
        metadata={"help": "Temperature for generation during GRPO. Optimized for code generation."}
    )
    gen_top_k: int = field(
        default=40, 
        metadata={"help": "Top-k for generation during GRPO."}
    )
    gen_top_p: float = field(
        default=0.8, 
        metadata={"help": "Top-p (nucleus) for generation during GRPO."}
    )
    gen_repetition_penalty: float = field(
        default=1.05, 
        metadata={"help": "Repetition penalty to avoid repetitive code generation."}
    )
    gen_length_penalty: float = field(
        default=1.0, 
        metadata={"help": "Length penalty for generation."}
    )

    # ğŸ”§ å¤šGPUä¼˜åŒ–çš„è¯¾ç¨‹å­¦ä¹ é…ç½®
    enable_curriculum: bool = field(
        default=True, 
        metadata={"help": "Enable dual-layer curriculum learning strategy."}
    )
    curriculum_type: str = field(
        default="dual_layer", 
        metadata={"help": "Type of curriculum: 'dual_layer', 'complexity_only', 'level_only'."}
    )
    curriculum_focus_levels: List[str] = field(
        default_factory=lambda: ["basic", "intermediate", "advanced", "expert"],
        metadata={"help": "Dataset levels to include in curriculum learning."}
    )
    curriculum_complexity_emphasis: str = field(
        default="balanced",
        metadata={"help": "Complexity emphasis: 'simple', 'balanced', 'complex'."}
    )
    curriculum_performance_check_interval: int = field(
        default=10,
        metadata={"help": "Steps between performance checks for curriculum advancement. Increased for multi-GPU to reduce communication."}
    )

    # è¯¾ç¨‹å­¦ä¹ æ€§èƒ½é˜ˆå€¼å‚æ•°
    curriculum_performance_threshold_1: Optional[float] = field(
        default=None,
        metadata={"help": "Performance threshold for curriculum stage 1 (foundation)."}
    )
    curriculum_performance_threshold_2: Optional[float] = field(
        default=None,
        metadata={"help": "Performance threshold for curriculum stage 2 (elementary)."}
    )
    curriculum_performance_threshold_3: Optional[float] = field(
        default=None,
        metadata={"help": "Performance threshold for curriculum stage 3 (intermediate)."}
    )
    curriculum_performance_threshold_4: Optional[float] = field(
        default=None,
        metadata={"help": "Performance threshold for curriculum stage 4 (advanced)."}
    )
    curriculum_performance_threshold_5: Optional[float] = field(
        default=None,
        metadata={"help": "Performance threshold for curriculum stage 5 (expert)."}
    )
    curriculum_min_evaluations: Optional[int] = field(
        default=None,
        metadata={"help": "Minimum evaluations before advancing to next curriculum stage."}
    )

    # ğŸ”§ å¤šGPUä¼˜åŒ–çš„ç»éªŒå›æ”¾é…ç½®
    enable_experience_replay: bool = field(
        default=True, 
        metadata={"help": "Enable experience replay buffer."}
    )
    experience_buffer_size: int = field(
        default=1500, 
        metadata={"help": "Size of experience replay buffer. Increased for multi-GPU."}
    )
    replay_sample_ratio: float = field(
        default=0.2, 
        metadata={"help": "Ratio of replay samples in each batch."}
    )

    # ğŸ”§ å¤šGPUæ€§èƒ½ä¼˜åŒ–é…ç½®
    enable_flash_attention: bool = field(
        default=True,
        metadata={"help": "Enable Flash Attention 2 if available for better memory efficiency."}
    )
    optimize_memory_usage: bool = field(
        default=True,
        metadata={"help": "Enable memory usage optimizations for multi-GPU training."}
    )
    reduce_communication_overhead: bool = field(
        default=True,
        metadata={"help": "Reduce multi-GPU communication overhead by optimizing callback frequency."}
    )
    gradient_checkpointing_compatible: bool = field(
        default=False,
        metadata={"help": "Whether to enable gradient checkpointing with multi-GPU (may cause sync issues)."}
    )

    # é‡åŒ–é…ç½®ï¼ˆå¤šGPUæ—¶é€šå¸¸ç¦ç”¨ï¼‰
    use_quantization: bool = field(
        default=False,
        metadata={"help": "Enable quantization. Automatically disabled for multi-GPU if disable_quantization_for_multi_gpu=True."}
    )
    load_in_4bit: bool = field(
        default=False,
        metadata={"help": "Enable 4-bit quantization."}
    )
    bnb_4bit_quant_type: str = field(
        default="nf4",
        metadata={"help": "4-bit quantization type for BitsAndBytes."}
    )
    bnb_4bit_use_double_quant: bool = field(
        default=True,
        metadata={"help": "Enable double quantization for BitsAndBytes."}
    )

    # è¯¾ç¨‹å­¦ä¹ é˜¶æ®µé…ç½®
    curriculum_stages: Optional[List[Dict[str, Any]]] = field(
        default=None, 
        metadata={"help": "Detailed dual-layer curriculum stages configuration."}
    )

    def __post_init__(self):
        """é…ç½®åå¤„ç†å’ŒéªŒè¯"""
        
        # ğŸ”§ å¤šGPUé‡åŒ–æ£€æŸ¥
        if self.use_model_parallel and self.disable_quantization_for_multi_gpu and self.use_quantization:
            print("âš ï¸ å¤šGPUæ¨¡å¼ä¸‹è‡ªåŠ¨ç¦ç”¨é‡åŒ–ä»¥é¿å…å…¼å®¹æ€§é—®é¢˜")
            self.use_quantization = False
            self.load_in_4bit = False
        
        # ğŸ”§ è‡ªåŠ¨è°ƒæ•´é•¿åº¦é…ç½®
        if self.length_allocation_strategy != "custom":
            if self.length_allocation_strategy == "balanced":
                self.script_max_prompt_length = self.max_seq_length // 2
                self.script_max_completion_length = self.max_seq_length // 2
            elif self.length_allocation_strategy == "prompt_heavy":
                self.script_max_prompt_length = int(self.max_seq_length * 0.6)
                self.script_max_completion_length = self.max_seq_length - self.script_max_prompt_length
            elif self.length_allocation_strategy == "completion_heavy":
                self.script_max_prompt_length = int(self.max_seq_length * 0.4)
                self.script_max_completion_length = self.max_seq_length - self.script_max_prompt_length
        
        # ğŸ”§ éªŒè¯é•¿åº¦é…ç½®
        total_length = self.script_max_prompt_length + self.script_max_completion_length
        if total_length > self.max_seq_length:
            print(f"âš ï¸ è­¦å‘Š: prompté•¿åº¦({self.script_max_prompt_length}) + completioné•¿åº¦({self.script_max_completion_length}) = {total_length} > æœ€å¤§åºåˆ—é•¿åº¦({self.max_seq_length})")
            print(f"   è‡ªåŠ¨è°ƒæ•´completioné•¿åº¦ä¸º: {self.max_seq_length - self.script_max_prompt_length}")
            self.script_max_completion_length = self.max_seq_length - self.script_max_prompt_length
        
        # ğŸ”§ å¤šGPUç¯å¢ƒæ£€æŸ¥å’Œé…ç½®è°ƒæ•´
        if self.use_model_parallel:
            self._validate_multi_gpu_environment()
            self._adjust_config_for_multi_gpu()
    
    def _validate_multi_gpu_environment(self):
        """éªŒè¯å¤šGPUç¯å¢ƒ"""
        try:
            if not torch.cuda.is_available():
                print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œç¦ç”¨å¤šGPUæ¨¡å¼")
                self.use_model_parallel = False
                return
            
            gpu_count = torch.cuda.device_count()
            if gpu_count < 2:
                print(f"âš ï¸ æ£€æµ‹åˆ°{gpu_count}å¼ GPUï¼Œå°‘äº2å¼ ï¼Œç¦ç”¨å¤šGPUæ¨¡å¼")
                self.use_model_parallel = False
                return
            
            print(f"âœ… å¤šGPUç¯å¢ƒéªŒè¯é€šè¿‡ï¼Œæ£€æµ‹åˆ°{gpu_count}å¼ GPU")
            
            # éªŒè¯GPUå†…å­˜
            for i in range(gpu_count):
                props = torch.cuda.get_device_properties(i)
                memory_gb = props.total_memory / 1024**3
                print(f"  GPU {i}: {props.name}, {memory_gb:.1f}GB")
                
                if memory_gb < 40:  # è‡³å°‘40GB
                    print(f"âš ï¸ GPU {i} å†…å­˜ä¸è¶³({memory_gb:.1f}GB)ï¼Œå¯èƒ½å½±å“å¤§æ¨¡å‹è®­ç»ƒ")
            
        except Exception as e:
            print(f"âš ï¸ å¤šGPUç¯å¢ƒæ£€æŸ¥å¤±è´¥: {e}")
            self.use_model_parallel = False
    
    def _adjust_config_for_multi_gpu(self):
        """æ ¹æ®å¤šGPUç¯å¢ƒè°ƒæ•´é…ç½®"""
        if not self.use_model_parallel:
            return
        
        # ğŸ”§ è°ƒæ•´å›è°ƒé¢‘ç‡ä»¥å‡å°‘é€šä¿¡å¼€é”€
        if self.reduce_communication_overhead:
            if self.callback_eval_every_n_steps < 10:
                print(f"ğŸ“Š å¤šGPUæ¨¡å¼ä¸‹è°ƒæ•´å›è°ƒé¢‘ç‡: {self.callback_eval_every_n_steps} -> 10")
                self.callback_eval_every_n_steps = 10
            
            if self.curriculum_performance_check_interval < 10:
                print(f"ğŸ“š å¤šGPUæ¨¡å¼ä¸‹è°ƒæ•´è¯¾ç¨‹æ£€æŸ¥é¢‘ç‡: {self.curriculum_performance_check_interval} -> 10")
                self.curriculum_performance_check_interval = 10
        
        # ğŸ”§ æ ¹æ®GPUæ•°é‡è°ƒæ•´ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°
        try:
            gpu_count = torch.cuda.device_count()
            if gpu_count >= 2:
                # å¤šGPUå¯ä»¥æ”¯æŒæ›´å¤§çš„ç¼“å†²åŒº
                original_size = self.experience_buffer_size
                self.experience_buffer_size = max(self.experience_buffer_size, 1500)
                if self.experience_buffer_size != original_size:
                    print(f"ğŸ’¾ å¤šGPUæ¨¡å¼ä¸‹è°ƒæ•´ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°: {original_size} -> {self.experience_buffer_size}")
        except:
            pass
    
    def validate_config(self) -> bool:
        """éªŒè¯é…ç½®çš„å®Œæ•´æ€§å’Œåˆç†æ€§"""
        errors = []
        warnings = []
        
        # éªŒè¯å¿…è¦å­—æ®µ
        if not self.model_name_or_path:
            errors.append("model_name_or_path ä¸èƒ½ä¸ºç©º")
        
        # éªŒè¯é•¿åº¦é…ç½®
        if self.script_max_prompt_length <= 0:
            errors.append(f"script_max_prompt_length ({self.script_max_prompt_length}) å¿…é¡»å¤§äº0")
        
        if self.script_max_completion_length <= 0:
            errors.append(f"script_max_completion_length ({self.script_max_completion_length}) å¿…é¡»å¤§äº0")
        
        total_length = self.script_max_prompt_length + self.script_max_completion_length
        if total_length > self.max_seq_length:
            errors.append(f"æ€»é•¿åº¦ ({total_length}) è¶…è¿‡æœ€å¤§åºåˆ—é•¿åº¦ ({self.max_seq_length})")
        
        # éªŒè¯LoRAé…ç½®
        if self.lora_rank <= 0 or self.lora_rank > 1024:
            errors.append(f"lora_rank ({self.lora_rank}) åº”è¯¥åœ¨ 1-1024 ä¹‹é—´")
        
        if self.lora_alpha <= 0:
            errors.append(f"lora_alpha ({self.lora_alpha}) å¿…é¡»å¤§äº0")
        
        # éªŒè¯å¤šGPUé…ç½®
        if self.use_model_parallel:
            try:
                if torch.cuda.device_count() < 2:
                    warnings.append("å¯ç”¨äº†å¤šGPUæ¨¡å¼ä½†GPUæ•°é‡å°‘äº2")
            except:
                warnings.append("æ— æ³•æ£€æµ‹GPUæ•°é‡")
        
        # éªŒè¯è¯¾ç¨‹å­¦ä¹ é…ç½®
        if self.enable_curriculum:
            if self.curriculum_performance_check_interval <= 0:
                errors.append(f"curriculum_performance_check_interval ({self.curriculum_performance_check_interval}) å¿…é¡»å¤§äº0")
        
        # è¾“å‡ºç»“æœ
        if errors:
            print("âŒ é…ç½®éªŒè¯å¤±è´¥:")
            for error in errors:
                print(f"  - {error}")
            return False
        
        if warnings:
            print("âš ï¸ é…ç½®è­¦å‘Š:")
            for warning in warnings:
                print(f"  - {warning}")
        
        print("âœ… é…ç½®éªŒè¯é€šè¿‡")
        return True
    
    def get_effective_batch_size(self, per_device_batch_size: int, gradient_accumulation_steps: int) -> int:
        """è®¡ç®—æœ‰æ•ˆæ‰¹æ¬¡å¤§å°"""
        num_gpus = 1
        if self.use_model_parallel:
            try:
                # å¯¹äºæ¨¡å‹å¹¶è¡Œï¼Œæœ‰æ•ˆæ‰¹æ¬¡å¤§å° = per_device_batch_size * gradient_accumulation_steps * num_gpus
                num_gpus = max(1, torch.cuda.device_count())
            except:
                pass
        
        return per_device_batch_size * gradient_accumulation_steps * num_gpus
    
    def get_memory_estimation(self) -> Dict[str, Any]:
        """ä¼°ç®—å†…å­˜éœ€æ±‚"""
        # åŸºäºå¸¸è§çš„8Bå‚æ•°æ¨¡å‹ä¼°ç®—
        model_params = 8e9  # 8Bå‚æ•°
        
        # è®¡ç®—æ¨¡å‹æƒé‡å†…å­˜
        if self.use_quantization and not (self.use_model_parallel and self.disable_quantization_for_multi_gpu):
            bytes_per_param = 1 if self.load_in_4bit else 2  # 4bit or 8bit
        else:
            bytes_per_param = 2  # bfloat16
        
        model_memory_gb = (model_params * bytes_per_param) / (1024**3)
        
        # è®¡ç®—æ¿€æ´»å†…å­˜ï¼ˆç²—ç•¥ä¼°ç®—ï¼‰
        batch_size = 1  # é»˜è®¤å‡è®¾
        activation_memory_gb = (self.max_seq_length * self.max_seq_length * batch_size * 4) / (1024**3)
        
        # è®¡ç®—æ€»å†…å­˜éœ€æ±‚
        total_memory_per_gpu = model_memory_gb + activation_memory_gb + 8  # 8GBç¼“å†²
        
        # å¤šGPUæ—¶åˆ†æ‘Šæ¨¡å‹å†…å­˜
        if self.use_model_parallel:
            try:
                num_gpus = max(1, torch.cuda.device_count())
                model_memory_per_gpu = model_memory_gb / num_gpus
                total_memory_per_gpu = model_memory_per_gpu + activation_memory_gb + 8
            except:
                num_gpus = 1
        else:
            num_gpus = 1
            model_memory_per_gpu = model_memory_gb
        
        return {
            "model_params": f"{model_params/1e9:.1f}B",
            "total_model_memory_gb": model_memory_gb,
            "model_memory_per_gpu_gb": model_memory_per_gpu,
            "activation_memory_gb": activation_memory_gb,
            "total_memory_per_gpu_gb": total_memory_per_gpu,
            "num_gpus": num_gpus,
            "recommended_max_memory": f"{int(total_memory_per_gpu + 5)}GiB",
            "memory_efficiency": "High" if self.use_model_parallel else "Standard"
        }
    
    def print_config_summary(self):
        """æ‰“å°é…ç½®æ‘˜è¦"""
        print("\n" + "="*60)
        print("           è®­ç»ƒé…ç½®æ‘˜è¦")
        print("="*60)
        
        print(f"ğŸ¯ æ¨¡å‹é…ç½®:")
        print(f"  - æ¨¡å‹è·¯å¾„: {self.model_name_or_path}")
        print(f"  - å¤šGPUæ¨¡å¼: {'âœ… å¯ç”¨' if self.use_model_parallel else 'âŒ ç¦ç”¨'}")
        print(f"  - é‡åŒ–: {'âœ… å¯ç”¨' if self.use_quantization else 'âŒ ç¦ç”¨'}")
        
        print(f"\nğŸ“ é•¿åº¦é…ç½®:")
        print(f"  - æ€»åºåˆ—é•¿åº¦: {self.max_seq_length}")
        print(f"  - æç¤ºé•¿åº¦: {self.script_max_prompt_length} ({self.script_max_prompt_length/self.max_seq_length*100:.1f}%)")
        print(f"  - å®Œæˆé•¿åº¦: {self.script_max_completion_length} ({self.script_max_completion_length/self.max_seq_length*100:.1f}%)")
        print(f"  - åˆ†é…ç­–ç•¥: {self.length_allocation_strategy}")
        
        print(f"\nğŸ”§ LoRAé…ç½®:")
        print(f"  - Rank: {self.lora_rank}")
        print(f"  - Alpha: {self.lora_alpha}")
        print(f"  - Dropout: {self.lora_dropout}")
        print(f"  - ç›®æ ‡æ¨¡å—: {len(self.lora_target_modules)} ä¸ª")
        
        print(f"\nğŸ“š è¯¾ç¨‹å­¦ä¹ :")
        print(f"  - å¯ç”¨: {'âœ…' if self.enable_curriculum else 'âŒ'}")
        print(f"  - ç±»å‹: {self.curriculum_type}")
        print(f"  - æ£€æŸ¥é—´éš”: {self.curriculum_performance_check_interval} æ­¥")
        
        print(f"\nğŸ’¾ ç»éªŒå›æ”¾:")
        print(f"  - å¯ç”¨: {'âœ…' if self.enable_experience_replay else 'âŒ'}")
        print(f"  - ç¼“å†²åŒºå¤§å°: {self.experience_buffer_size}")
        
        # å†…å­˜ä¼°ç®—
        memory_info = self.get_memory_estimation()
        print(f"\nğŸ§  å†…å­˜ä¼°ç®—:")
        print(f"  - æ¨¡å‹å‚æ•°: {memory_info['model_params']}")
        print(f"  - æ¯GPUæ¨¡å‹å†…å­˜: {memory_info['model_memory_per_gpu_gb']:.1f}GB")
        print(f"  - æ¯GPUæ€»å†…å­˜: {memory_info['total_memory_per_gpu_gb']:.1f}GB")
        print(f"  - æ¨èé…ç½®: {memory_info['recommended_max_memory']}")
        print(f"  - å†…å­˜æ•ˆç‡: {memory_info['memory_efficiency']}")
        
        print("="*60 + "\n")


@dataclass
class OptimizedTrainingConfig:
    """å¤šGPUä¼˜åŒ–çš„è®­ç»ƒé…ç½®"""

    # ğŸ”§ å¤šGPUä¼˜åŒ–çš„åŸºç¡€è®­ç»ƒå‚æ•°
    max_steps: int = 500  # å¢åŠ è®­ç»ƒæ­¥æ•°ä»¥å……åˆ†åˆ©ç”¨å¤šGPU
    learning_rate: float = 1e-5
    eval_steps: int = 5    # å‡å°‘è¯„ä¼°é¢‘ç‡ä»¥é™ä½é€šä¿¡å¼€é”€
    save_steps: int = 25   # å‡å°‘ä¿å­˜é¢‘ç‡
    warmup_steps: int = 30 # é€‚å½“å¢åŠ warmup

    # ğŸ”§ å¤šGPUä¼˜åŒ–çš„è¯¾ç¨‹å­¦ä¹ å‚æ•°
    curriculum_performance_threshold: float = 0.65  # é€‚å½“æé«˜é˜ˆå€¼
    curriculum_progression_patience: int = 20       # å¢åŠ è€å¿ƒå€¼
    advanced_stage_min_steps: int = 80              # é«˜éš¾åº¦é˜¶æ®µæ›´å¤šæ­¥æ•°

    # ğŸ”§ å¤šGPUä¼˜åŒ–çš„ç›‘æ§å‚æ•°
    detailed_logging_interval: int = 15   # å‡å°‘æ—¥å¿—é¢‘ç‡
    performance_analysis_interval: int = 75  # å‡å°‘åˆ†æé¢‘ç‡

    # å¤šGPUç‰¹å®šä¼˜åŒ–
    enable_gradient_sync_optimization: bool = True
    reduce_callback_frequency: bool = True
    optimize_communication: bool = True

def apply_optimized_config(script_cfg: ScriptConfig, grpo_cfg, opt_config: OptimizedTrainingConfig = None):
    """å°†å¤šGPUä¼˜åŒ–é…ç½®åº”ç”¨åˆ°ç°æœ‰é…ç½®å¯¹è±¡"""
    if opt_config is None:
        opt_config = OptimizedTrainingConfig()

    # åº”ç”¨åˆ° script_cfg
    if hasattr(script_cfg, 'max_steps'):
        script_cfg.max_steps = opt_config.max_steps
    
    # å¤šGPUä¼˜åŒ–è°ƒæ•´
    if script_cfg.use_model_parallel and opt_config.reduce_callback_frequency:
        # å‡å°‘å›è°ƒé¢‘ç‡ä»¥é™ä½å¤šGPUé€šä¿¡å¼€é”€
        script_cfg.callback_eval_every_n_steps = max(
            script_cfg.callback_eval_every_n_steps, 
            15
        )
        script_cfg.curriculum_performance_check_interval = max(
            script_cfg.curriculum_performance_check_interval,
            15
        )
        print("ğŸ”§ å¤šGPUæ¨¡å¼ä¸‹å·²ä¼˜åŒ–å›è°ƒé¢‘ç‡")

    # åº”ç”¨åˆ° grpo_cfg
    grpo_cfg.learning_rate = opt_config.learning_rate
    grpo_cfg.eval_steps = opt_config.eval_steps
    grpo_cfg.save_steps = opt_config.save_steps
    grpo_cfg.warmup_steps = opt_config.warmup_steps
    
    # å¤šGPUç‰¹å®šçš„grpoé…ç½®è°ƒæ•´
    if script_cfg.use_model_parallel:
        # ç¦ç”¨å¯èƒ½å¯¼è‡´åŒæ­¥é—®é¢˜çš„ç‰¹æ€§
        if hasattr(grpo_cfg, 'gradient_checkpointing'):
            if not script_cfg.gradient_checkpointing_compatible:
                grpo_cfg.gradient_checkpointing = False
                print("ğŸ”§ å¤šGPUæ¨¡å¼ä¸‹å·²ç¦ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
        
        # ä¼˜åŒ–æ•°æ®åŠ è½½
        if hasattr(grpo_cfg, 'dataloader_num_workers'):
            grpo_cfg.dataloader_num_workers = min(grpo_cfg.dataloader_num_workers, 2)
            print("ğŸ”§ å¤šGPUæ¨¡å¼ä¸‹å·²ä¼˜åŒ–æ•°æ®åŠ è½½å™¨")

    return script_cfg, grpo_cfg