# grpo_project/utils/streaming_guidance.py
import torch
import re
import logging
from typing import List, Dict, Optional, Tuple, Generator, Any
from transformers import PreTrainedTokenizerBase, PreTrainedModel
from dataclasses import dataclass
import time
import os
import json
from datetime import datetime

logger = logging.getLogger(__name__)

@dataclass
class GuidanceConfig:
    """æµå¼å¼•å¯¼é…ç½®"""
    min_reasoning_length: int = 50  # æœ€å°æ€è€ƒé•¿åº¦
    guidance_trigger_threshold: int = 30  # è§¦å‘å¼•å¯¼çš„é˜ˆå€¼
    max_guidance_attempts: int = 5  # å¢åŠ æœ€å¤§å¼•å¯¼å°è¯•æ¬¡æ•°åˆ°5æ¬¡
    guidance_tokens_limit: int = 20  # æ¯æ¬¡å¼•å¯¼æ·»åŠ çš„æœ€å¤§tokenæ•°
    stop_on_code_start: bool = True  # é‡åˆ°ä»£ç å—å¼€å§‹æ—¶åœæ­¢
    save_failed_generations: bool = True  # æ˜¯å¦ä¿å­˜å¤±è´¥çš„ç”Ÿæˆ
    save_successful_generations: bool = True  # æ˜¯å¦ä¿å­˜æˆåŠŸçš„ç”Ÿæˆ
    failed_generations_dir: str = "failed_generations"  # å¤±è´¥ç”Ÿæˆä¿å­˜ç›®å½•
    successful_generations_dir: str = "successful_generations"  # æˆåŠŸç”Ÿæˆä¿å­˜ç›®å½•
    
class ThinkingGuidanceManager:
    """æ€è€ƒè¿‡ç¨‹å¼•å¯¼ç®¡ç†å™¨"""
    
    def __init__(self, config: GuidanceConfig):
        self.config = config
        self.guidance_phrases = [
            # åˆ†æé˜¶æ®µå¼•å¯¼è¯
            "First, I need to analyze the requirements carefully. ",
            "Let me break down this problem step by step. ",
            "The key requirements for this module are: ",
            "I should start by identifying the input and output signals. ",
            
            # è®¾è®¡é˜¶æ®µå¼•å¯¼è¯
            "For the design approach, I will implement ",
            "The module architecture should include ",
            "The main logic blocks needed are: ",
            "I need to consider the following design constraints: ",
            
            # å®ç°é˜¶æ®µå¼•å¯¼è¯
            "For the implementation, I will use ",
            "The core logic can be implemented using ",
            "I should structure the code with these main sections: ",
            "The signal flow will be organized as follows: ",
            
            # éªŒè¯é˜¶æ®µå¼•å¯¼è¯
            "To ensure correctness, I should verify that ",
            "The edge cases I need to handle include: ",
            "For proper timing, I must ensure ",
            "The output behavior should satisfy these conditions: "
        ]
        
    def select_guidance_phrase(self, current_text: str, attempt: int) -> str:
        """æ ¹æ®å½“å‰æ–‡æœ¬å’Œå°è¯•æ¬¡æ•°é€‰æ‹©åˆé€‚çš„å¼•å¯¼è¯"""
        
        # æ ¹æ®å½“å‰å†…å®¹é€‰æ‹©å¼•å¯¼ç±»å‹
        current_lower = current_text.lower()
        
        # å¦‚æœæ²¡æœ‰åˆ†æå†…å®¹ï¼Œä¼˜å…ˆåˆ†æå¼•å¯¼
        if not any(word in current_lower for word in ['analyze', 'requirement', 'input', 'output']):
            candidates = self.guidance_phrases[:4]  # åˆ†æé˜¶æ®µ
        # å¦‚æœæ²¡æœ‰è®¾è®¡å†…å®¹ï¼Œä¼˜å…ˆè®¾è®¡å¼•å¯¼  
        elif not any(word in current_lower for word in ['design', 'implement', 'architecture', 'logic']):
            candidates = self.guidance_phrases[4:8]  # è®¾è®¡é˜¶æ®µ
        # å¦‚æœæ²¡æœ‰å®ç°å†…å®¹ï¼Œä¼˜å…ˆå®ç°å¼•å¯¼
        elif not any(word in current_lower for word in ['implementation', 'code', 'structure']):
            candidates = self.guidance_phrases[8:12]  # å®ç°é˜¶æ®µ
        # å¦åˆ™ä½¿ç”¨éªŒè¯å¼•å¯¼
        else:
            candidates = self.guidance_phrases[12:]  # éªŒè¯é˜¶æ®µ
            
        # æ ¹æ®å°è¯•æ¬¡æ•°é€‰æ‹©ï¼ˆé¿å…é‡å¤ï¼‰
        index = attempt % len(candidates)
        return candidates[index]

class StreamingGuidanceGenerator:
    """æµå¼å¼•å¯¼ç”Ÿæˆå™¨"""
    
    def __init__(self, 
                 model: PreTrainedModel,
                 tokenizer: PreTrainedTokenizerBase,
                 config: GuidanceConfig = None):
        self.model = model
        self.tokenizer = tokenizer
        self.config = config or GuidanceConfig()
        self.guidance_manager = ThinkingGuidanceManager(self.config)
        
        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()
        
        # åˆ›å»ºç”Ÿæˆä¿å­˜ç›®å½•
        if self.config.save_failed_generations:
            os.makedirs(self.config.failed_generations_dir, exist_ok=True)
        if self.config.save_successful_generations:
            os.makedirs(self.config.successful_generations_dir, exist_ok=True)
        
    def _save_generation(self, prompt: str, generated_text: str, guidance_applied: List[Dict], 
                        step: int = 0, is_successful: bool = True, reason: str = None):
        """ä¿å­˜ç”Ÿæˆç»“æœï¼ˆæˆåŠŸæˆ–å¤±è´¥ï¼‰"""
        if (is_successful and not self.config.save_successful_generations) or \
           (not is_successful and not self.config.save_failed_generations):
            return
            
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        dir_name = self.config.successful_generations_dir if is_successful else self.config.failed_generations_dir
        prefix = "success" if is_successful else "failed"
        
        filename = os.path.join(
            dir_name,
            f"{prefix}_gen_step{step}_{timestamp}.json"
        )
        
        try:
            data = {
                "timestamp": timestamp,
                "step": step,
                "is_successful": is_successful,
                "prompt": prompt,
                "generated_text": generated_text,
                "guidance_applied": guidance_applied,
                "config": {
                    "min_reasoning_length": self.config.min_reasoning_length,
                    "guidance_trigger_threshold": self.config.guidance_trigger_threshold,
                    "max_guidance_attempts": self.config.max_guidance_attempts,
                    "guidance_tokens_limit": self.config.guidance_tokens_limit
                }
            }
            
            if not is_successful and reason:
                data["failure_reason"] = reason
            
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
                
            status = "æˆåŠŸ" if is_successful else "å¤±è´¥"
            logger.info(f"ğŸ’¾ ä¿å­˜{status}ç”Ÿæˆåˆ°: {filename}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç”Ÿæˆç»“æœæ—¶å‡ºé”™: {e}")

    def generate_with_guidance(self, 
                             prompt: str,
                             max_new_tokens: int = 512,
                             temperature: float = 0.8,
                             top_p: float = 0.95,
                             step: int = 0,
                             **generation_kwargs) -> Dict[str, Any]:
        """
        å¸¦å¼•å¯¼çš„æµå¼ç”Ÿæˆ
        
        Returns:
            DictåŒ…å«generated_text, guidance_applied, reasoning_part, code_partç­‰
        """
        
        logger.info(f"ğŸš€ å¼€å§‹æµå¼å¼•å¯¼ç”Ÿæˆï¼Œmax_tokens={max_new_tokens}")
        
        # åˆå§‹åŒ–çŠ¶æ€
        generated_text = ""
        guidance_applied = []
        reasoning_part = ""
        code_part = ""
        guidance_attempts = 0
        
        # å‡†å¤‡åˆå§‹è¾“å…¥
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        input_ids = inputs.input_ids
        attention_mask = inputs.attention_mask
        
        # æµå¼ç”ŸæˆçŠ¶æ€è·Ÿè¸ª
        in_think_block = False
        think_content = ""
        generated_tokens = 0
        
        with torch.no_grad():
            for step in range(max_new_tokens):
                # ç”Ÿæˆä¸‹ä¸€ä¸ªtoken
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    use_cache=True
                )
                
                # åº”ç”¨é‡‡æ ·ç­–ç•¥
                logits = outputs.logits[:, -1, :]
                if temperature > 0:
                    logits = logits / temperature
                    
                # Top-pé‡‡æ ·
                if top_p < 1.0:
                    sorted_logits, sorted_indices = torch.sort(logits, descending=True)
                    cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)
                    sorted_indices_to_remove = cumulative_probs > top_p
                    sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()
                    sorted_indices_to_remove[:, 0] = 0
                    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
                    logits[indices_to_remove] = float('-inf')
                
                # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
                probs = torch.softmax(logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)
                
                # è§£ç token
                next_text = self.tokenizer.decode(next_token[0], skip_special_tokens=True)
                generated_text += next_text
                generated_tokens += 1
                
                # æ›´æ–°è¾“å…¥åºåˆ—
                input_ids = torch.cat([input_ids, next_token], dim=-1)
                attention_mask = torch.cat([attention_mask, torch.ones_like(next_token)], dim=-1)
                
                # è§£æå½“å‰çŠ¶æ€
                parsing_result = self._parse_current_output(generated_text)
                
                # æ£€æŸ¥æ˜¯å¦è¿›å…¥thinkå—
                if '<think>' in generated_text and not in_think_block:
                    in_think_block = True
                    logger.debug("ğŸ¤” æ£€æµ‹åˆ°è¿›å…¥thinkå—")
                    
                # å¦‚æœåœ¨thinkå—ä¸­ï¼Œç´¯ç§¯å†…å®¹
                if in_think_block and not parsing_result['think_closed']:
                    think_content = parsing_result['think_content']
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼•å¯¼
                    should_guide, reason = self._should_apply_guidance(
                        think_content, generated_tokens, guidance_attempts
                    )
                    
                    if should_guide:
                        logger.info(f"ğŸ¯ è§¦å‘å¼•å¯¼ - åŸå› : {reason}")
                        guidance_text = self._apply_guidance(
                            think_content, guidance_attempts
                        )
                        
                        if guidance_text:
                            # å°†å¼•å¯¼æ–‡æœ¬ç¼–ç å¹¶æ·»åŠ åˆ°åºåˆ—ä¸­
                            guidance_tokens = self.tokenizer.encode(
                                guidance_text, 
                                add_special_tokens=False,
                                return_tensors="pt"
                            ).to(self.model.device)
                            
                            # æ›´æ–°åºåˆ—
                            input_ids = torch.cat([input_ids, guidance_tokens], dim=-1)
                            attention_mask = torch.cat([
                                attention_mask, 
                                torch.ones_like(guidance_tokens)
                            ], dim=-1)
                            
                            generated_text += guidance_text
                            guidance_applied.append({
                                'step': step,
                                'reason': reason,
                                'guidance': guidance_text,
                                'attempt': guidance_attempts
                            })
                            
                            guidance_attempts += 1
                            logger.info(f"âœ… åº”ç”¨å¼•å¯¼ (å°è¯• {guidance_attempts}): {guidance_text}")
                
                # æ£€æŸ¥åœæ­¢æ¡ä»¶
                if self._should_stop_generation(generated_text, parsing_result):
                    logger.info(f"ğŸ›‘ ç”Ÿæˆåœæ­¢ - æ£€æµ‹åˆ°å®Œæ•´è¾“å‡ºæˆ–ä»£ç å—")
                    break
                    
                # æ£€æŸ¥thinkå—æ˜¯å¦å…³é—­
                if parsing_result['think_closed'] and in_think_block:
                    in_think_block = False
                    reasoning_part = parsing_result['think_content']
                    logger.debug(f"âœ… Thinkå—å®Œæˆï¼Œé•¿åº¦: {len(reasoning_part)}")
                    
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§å¼•å¯¼æ¬¡æ•°
                if guidance_attempts >= self.config.max_guidance_attempts:
                    logger.warning(f"âš ï¸ è¾¾åˆ°æœ€å¤§å¼•å¯¼æ¬¡æ•° ({guidance_attempts})")
        
        # æœ€ç»ˆè§£æ
        final_result = self._parse_current_output(generated_text)
        
        result = {
            'generated_text': generated_text,
            'reasoning_part': final_result['think_content'],
            'code_part': final_result['code_content'], 
            'guidance_applied': guidance_applied,
            'guidance_count': len(guidance_applied),
            'final_reasoning_length': len(final_result['think_content']),
            'generation_successful': self._is_generation_successful(final_result),
            'tokens_generated': generated_tokens
        }
        
        logger.info(f"ğŸ ç”Ÿæˆå®Œæˆ: tokens={generated_tokens}, "
                   f"guidance_count={len(guidance_applied)}, "
                   f"reasoning_length={len(final_result['think_content'])}")
        
        # åœ¨ç”Ÿæˆç»“æŸæ—¶æ£€æŸ¥æ˜¯å¦æˆåŠŸ
        is_successful = self._is_generation_successful(final_result)
        if not is_successful:
            failure_reason = "generation_incomplete"
            if guidance_attempts >= self.config.max_guidance_attempts:
                failure_reason = "max_guidance_attempts_reached"
            elif not final_result['think_closed']:
                failure_reason = "incomplete_thinking"
            elif not final_result['code_closed']:
                failure_reason = "incomplete_code"
            elif len(final_result['think_content']) < self.config.min_reasoning_length:
                failure_reason = "insufficient_reasoning"
            elif 'module' not in final_result['code_content'].lower():
                failure_reason = "missing_module_declaration"
        else:
            failure_reason = None
        
        # ä¿å­˜ç”Ÿæˆç»“æœï¼ˆæ— è®ºæˆåŠŸè¿˜æ˜¯å¤±è´¥ï¼‰
        self._save_generation(
            prompt=prompt,
            generated_text=generated_text,
            guidance_applied=guidance_applied,
            step=step,
            is_successful=is_successful,
            reason=failure_reason
        )
        
        return result
    
    def _parse_current_output(self, text: str) -> Dict[str, Any]:
        """è§£æå½“å‰è¾“å‡ºçŠ¶æ€"""
        
        # æ£€æŸ¥thinkå—çŠ¶æ€
        think_start = text.find('<think>')
        think_end = text.find('</think>')
        
        think_content = ""
        think_closed = False
        
        if think_start >= 0:
            if think_end > think_start:
                think_content = text[think_start + 7:think_end].strip()
                think_closed = True
            else:
                think_content = text[think_start + 7:].strip()
                think_closed = False
        
        # æ£€æŸ¥ä»£ç å—çŠ¶æ€
        code_start = text.find('```verilog')
        code_end = text.rfind('```')
        
        code_content = ""
        code_closed = False
        
        if code_start >= 0:
            if code_end > code_start + 10:  # ç¡®ä¿ä¸æ˜¯åŒä¸€ä¸ª```
                code_content = text[code_start + 10:code_end].strip()
                code_closed = True
            else:
                code_content = text[code_start + 10:].strip()
                code_closed = False
        
        return {
            'think_content': think_content,
            'think_closed': think_closed,
            'code_content': code_content,
            'code_closed': code_closed,
            'has_think_start': think_start >= 0,
            'has_code_start': code_start >= 0
        }
    
    def _should_apply_guidance(self, 
                             think_content: str, 
                             tokens_generated: int,
                             guidance_attempts: int) -> Tuple[bool, str]:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åº”ç”¨å¼•å¯¼"""
        
        # æ£€æŸ¥å¼•å¯¼æ¬¡æ•°é™åˆ¶
        if guidance_attempts >= self.config.max_guidance_attempts:
            return False, "max_attempts_reached"
        
        # æ£€æŸ¥thinkå†…å®¹é•¿åº¦
        if len(think_content.strip()) < self.config.guidance_trigger_threshold:
            return True, f"think_too_short_{len(think_content)}"
        
        # æ£€æŸ¥å†…å®¹è´¨é‡
        if self._is_low_quality_thinking(think_content):
            return True, "low_quality_content"
            
        return False, "no_guidance_needed"
    
    def _is_low_quality_thinking(self, think_content: str) -> bool:
        """æ£€æŸ¥æ€è€ƒå†…å®¹æ˜¯å¦è´¨é‡è¾ƒä½"""
        
        if not think_content.strip():
            return True
            
        # æ£€æŸ¥æ— æ„ä¹‰å¼€å¤´
        bad_starts = ['well', 'i will', 'let me', 'so', 'ok', 'hmm']
        first_words = think_content.lower().strip().split()[:3]
        if any(start in ' '.join(first_words) for start in bad_starts):
            return True
            
        # æ£€æŸ¥æ˜¯å¦ç¼ºå°‘æŠ€æœ¯è¯æ±‡
        tech_words = ['module', 'input', 'output', 'logic', 'implement', 'design', 'signal']
        has_tech_words = any(word in think_content.lower() for word in tech_words)
        
        return not has_tech_words
    
    def _apply_guidance(self, current_think: str, attempt: int) -> str:
        """åº”ç”¨å¼•å¯¼æ–‡æœ¬"""
        
        guidance_phrase = self.guidance_manager.select_guidance_phrase(
            current_think, attempt
        )
        
        # æ·»åŠ é€‚å½“çš„è¿æ¥è¯
        if current_think.strip() and not current_think.strip().endswith('.'):
            guidance_text = ". " + guidance_phrase
        else:
            guidance_text = guidance_phrase
            
        return guidance_text
    
    def _should_stop_generation(self, generated_text: str, parsing_result: Dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åœæ­¢ç”Ÿæˆ"""
        
        # å¦‚æœæ£€æµ‹åˆ°å®Œæ•´çš„thinkå’Œcodeå—
        if (parsing_result['think_closed'] and 
            parsing_result['code_closed'] and
            len(parsing_result['think_content']) >= self.config.min_reasoning_length):
            return True
            
        # å¦‚æœæ£€æµ‹åˆ°å¤šä¸ª```ï¼ˆå¯èƒ½å‡ºé”™ï¼‰
        if generated_text.count('```') > 2:
            return True
            
        return False
    
    def _is_generation_successful(self, parsing_result: Dict) -> bool:
        """åˆ¤æ–­ç”Ÿæˆæ˜¯å¦æˆåŠŸ"""
        
        return (
            parsing_result['think_closed'] and
            parsing_result['code_closed'] and
            len(parsing_result['think_content']) >= self.config.min_reasoning_length and
            'module' in parsing_result['code_content'].lower()
        )

# grpo_project/utils/streaming_integration.py
class StreamingInferenceIntegration:
    """å°†æµå¼å¼•å¯¼é›†æˆåˆ°ç°æœ‰æ¨ç†ç³»ç»Ÿä¸­"""
    
    def __init__(self, model, tokenizer, config: GuidanceConfig = None):
        self.streaming_generator = StreamingGuidanceGenerator(model, tokenizer, config)
        
    def enhance_inference_callback(self, inference_callback):
        """å¢å¼ºç°æœ‰çš„æ¨ç†å›è°ƒä»¥æ”¯æŒæµå¼å¼•å¯¼"""
        
        original_generate = inference_callback._generate_single_sample
        
        def enhanced_generate(model, prompt, step):
            """æ›¿æ¢åŸæœ‰çš„ç”Ÿæˆæ–¹æ³•"""
            
            # ä½¿ç”¨æµå¼å¼•å¯¼ç”Ÿæˆ
            result = self.streaming_generator.generate_with_guidance(
                prompt=prompt,
                max_new_tokens=inference_callback.max_new_tokens,
                temperature=0.8,
                top_p=0.95
            )
            
            # è½¬æ¢ä¸ºåŸæœ‰æ ¼å¼
            return {
                'reasoning': result['reasoning_part'],
                'code': result['code_part'],
                'raw_output': result['generated_text'],
                'generation_time': 0.0,  # æµå¼ç”Ÿæˆæ—¶é—´è®¡ç®—è¾ƒå¤æ‚ï¼Œå¯ä»¥åç»­æ·»åŠ 
                'step': step,
                'guidance_applied': result['guidance_applied'],
                'guidance_count': result['guidance_count'],
                'error': None if result['generation_successful'] else "Generation incomplete"
            }
        
        # æ›¿æ¢æ–¹æ³•
        inference_callback._generate_single_sample = enhanced_generate
        return inference_callback

# ä½¿ç”¨ç¤ºä¾‹
def create_streaming_inference_callback(model, tokenizer, eval_dataset, **kwargs):
    """åˆ›å»ºæ”¯æŒæµå¼å¼•å¯¼çš„æ¨ç†å›è°ƒ"""
    
    from grpo_project.callbacks.inference import DetailedInferenceCallback
    
    # åˆ›å»ºåŸºç¡€å›è°ƒï¼Œç§»é™¤guidance_configå‚æ•°
    base_callback = DetailedInferenceCallback(
        tokenizer=tokenizer,
        eval_dataset=eval_dataset,
        **{k: v for k, v in kwargs.items() if k != 'guidance_config'}
    )
    
    # åˆ›å»ºæµå¼å¼•å¯¼é…ç½®
    guidance_config = GuidanceConfig(
        min_reasoning_length=60,
        guidance_trigger_threshold=40,
        max_guidance_attempts=5,  # å¢åŠ åˆ°5æ¬¡
        save_failed_generations=True,  # å¯ç”¨å¤±è´¥ç”Ÿæˆä¿å­˜
        save_successful_generations=True,  # å¯ç”¨æˆåŠŸç”Ÿæˆä¿å­˜
        failed_generations_dir=os.path.join(kwargs.get('output_dir', '.'), 'failed_generations'),
        successful_generations_dir=os.path.join(kwargs.get('output_dir', '.'), 'successful_generations')
    )
    
    # é›†æˆæµå¼å¼•å¯¼
    integration = StreamingInferenceIntegration(model, tokenizer, guidance_config)
    enhanced_callback = integration.enhance_inference_callback(base_callback)
    
    return enhanced_callback