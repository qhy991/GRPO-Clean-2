import logging
from transformers import GenerationConfig

logger = logging.getLogger(__name__)

def setup_qwen3_generation_config(model, tokenizer, script_cfg):
    """è®¾ç½®Qwen3çš„ç”Ÿæˆé…ç½®"""

    logger.info("ğŸ¤– è®¾ç½®Qwen3ç”Ÿæˆé…ç½®...")

    # ç¡®ä¿tokenizeré…ç½®æ­£ç¡®
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
        logger.info("è®¾ç½®pad_tokenä¸ºeos_token")

    # æ›´æ–°æ¨¡å‹é…ç½®
    if hasattr(model, 'config'):
        model.config.pad_token_id = tokenizer.pad_token_id
        model.config.eos_token_id = tokenizer.eos_token_id

    # è®¾ç½®ç”Ÿæˆé…ç½®
    generation_config = GenerationConfig(
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
        max_new_tokens=script_cfg.max_seq_length // 2, # Ensure this is a reasonable value
        do_sample=True,
        temperature=getattr(script_cfg, 'gen_temperature', 0.7),
        top_p=getattr(script_cfg, 'gen_top_p', 0.8),
        top_k=getattr(script_cfg, 'gen_top_k', 40),
        repetition_penalty=getattr(script_cfg, 'gen_repetition_penalty', 1.05),
        length_penalty=getattr(script_cfg, 'gen_length_penalty', 1.0),
    )

    model.generation_config = generation_config
    logger.info("âœ… Qwen3ç”Ÿæˆé…ç½®è®¾ç½®å®Œæˆ")

    return model, tokenizer
