import os
import logging
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    GenerationConfig,
)
from peft import get_peft_model, LoraConfig, TaskType, PeftModel, prepare_model_for_kbit_training
from accelerate import dispatch_model
from accelerate.utils import infer_auto_device_map
from typing import Optional, Any, Dict, List
import json

logger = logging.getLogger(__name__)

class ModelManager:
    def __init__(self, script_cfg, grpo_cfg, model_name_or_path: str, cache_dir: Optional[str] = None):
        self.script_cfg = script_cfg
        self.grpo_cfg = grpo_cfg # This is TrainingArguments/GRPOConfig
        self.model_name_or_path = model_name_or_path
        self.cache_dir = cache_dir

        self.model = None
        self.tokenizer = None
        
        # ğŸ”§ æ–°å¢ï¼šå¤šGPUé…ç½®æ£€æµ‹
        self.num_gpus = torch.cuda.device_count()
        self.use_multi_gpu = self.num_gpus >= 2
        
        if self.use_multi_gpu:
            logger.info(f"ğŸš€ æ£€æµ‹åˆ° {self.num_gpus} å¼ GPUï¼Œå¯ç”¨æ¨¡å‹å¹¶è¡Œæ¨¡å¼")
        else:
            logger.info(f"ğŸ“± æ£€æµ‹åˆ° {self.num_gpus} å¼ GPUï¼Œä½¿ç”¨å•GPUæ¨¡å¼")

    def _get_multi_gpu_config(self):
        """ğŸ”§ é…ç½®å¤šGPUæ¨¡å‹å¹¶è¡Œå‚æ•°"""
        if not self.use_multi_gpu:
            return None, None
            
        # ä¸ºæ¯å¼ A100é…ç½®å†…å­˜é™åˆ¶ï¼ˆä¿ç•™5GBç¼“å†²ï¼‰
        max_memory = {}
        for i in range(self.num_gpus):
            max_memory[i] = "75GiB"  # æ¯å¼ 80G A100ç•™5Gç¼“å†²
            
        logger.info(f"ğŸ”§ å¤šGPUå†…å­˜é…ç½®: {max_memory}")
        
        # è®¾å¤‡æ˜ å°„ç­–ç•¥
        device_map = "auto"  # è®©accelerateè‡ªåŠ¨åˆ†é…
        
        return device_map, max_memory

    def _apply_model_optimizations(self, model):
        """ğŸ”§ åº”ç”¨æ¨¡å‹ä¼˜åŒ–"""
        try:
            # å¯ç”¨Flash Attention (å¦‚æœå¯ç”¨)
            if hasattr(model.config, 'use_flash_attention_2'):
                model.config.use_flash_attention_2 = True
                logger.info("âœ… å¯ç”¨ Flash Attention 2")
            
            # ä¼˜åŒ–æ¨¡å‹é…ç½®
            if hasattr(model.config, 'torch_dtype'):
                model.config.torch_dtype = torch.bfloat16 if self.grpo_cfg.bf16 else torch.float16
                
            # è®¾ç½®ç”Ÿæˆé…ç½®
            if hasattr(model, 'generation_config'):
                model.generation_config.do_sample = True
                model.generation_config.temperature = 0.7
                model.generation_config.top_p = 0.9
                model.generation_config.max_new_tokens = 1024
                
            return model
            
        except Exception as e:
            logger.warning(f"âš ï¸ æ¨¡å‹ä¼˜åŒ–å¤±è´¥: {e}")
            return model

    def setup_model_and_tokenizer(self):
        """
        Loads the base model and tokenizer with multi-GPU support.
        Applies quantization and compatibility fixes.
        """
        logger.info(f"Loading base model from: {self.model_name_or_path}")
        model_dtype = torch.bfloat16 if self.grpo_cfg.bf16 else (torch.float16 if self.grpo_cfg.fp16 else torch.float32)

        # ğŸ”§ é‡åŒ–é…ç½®ï¼ˆå¤šGPUæ—¶éœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
        quantization_config_arg = None
        if getattr(self.script_cfg, 'use_quantization', False):
            if self.use_multi_gpu:
                logger.warning("âš ï¸ å¤šGPUæ¨¡å¼ä¸‹ç¦ç”¨é‡åŒ–ä»¥é¿å…å…¼å®¹æ€§é—®é¢˜")
            else:
                try:
                    quant_config_dict = {
                        "load_in_4bit": getattr(self.script_cfg, 'load_in_4bit', True),
                        "bnb_4bit_quant_type": getattr(self.script_cfg, 'bnb_4bit_quant_type', "nf4"),
                        "bnb_4bit_compute_dtype": model_dtype,
                        "bnb_4bit_use_double_quant": getattr(self.script_cfg, 'bnb_4bit_use_double_quant', True),
                    }
                    valid_bnb_keys = BitsAndBytesConfig.__annotations__.keys()
                    filtered_quant_config = {k: v for k, v in quant_config_dict.items() if k in valid_bnb_keys}

                    quantization_config_arg = BitsAndBytesConfig(**filtered_quant_config)
                    logger.info(f"BitsAndBytes quantization enabled: {filtered_quant_config}")
                except ImportError:
                    logger.warning("bitsandbytes not installed, quantization disabled.")
                except Exception as e_quant:
                    logger.warning(f"Failed to create BitsAndBytesConfig: {e_quant}")

        # ğŸ”§ è·å–å¤šGPUé…ç½®
        device_map, max_memory = self._get_multi_gpu_config()
        
        # ğŸ”§ æ¨¡å‹åŠ è½½å‚æ•°é…ç½®
        model_kwargs = {
            "quantization_config": quantization_config_arg,
            "trust_remote_code": True,
            "torch_dtype": model_dtype,
            "use_cache": False if self.grpo_cfg.gradient_checkpointing else True,
            "cache_dir": self.cache_dir,
            "low_cpu_mem_usage": True,  # é‡è¦ï¼šå‡å°‘CPUå†…å­˜ä½¿ç”¨
        }
        
        # ğŸ”§ å¤šGPUé…ç½®
        if self.use_multi_gpu and not quantization_config_arg and not getattr(self.grpo_cfg, 'fsdp', None):
            model_kwargs.update({
                "device_map": device_map,
                "max_memory": max_memory,
            })
            logger.info("ğŸš€ ä½¿ç”¨å¤šGPUæ¨¡å‹å¹¶è¡Œé…ç½®")
        elif torch.cuda.is_available() and self.grpo_cfg.world_size == 1 and not getattr(self.grpo_cfg, 'fsdp', None):
            model_kwargs["device_map"] = "auto"
            logger.info("ğŸ“± ä½¿ç”¨å•GPUè‡ªåŠ¨è®¾å¤‡æ˜ å°„")
        else:
            logger.info("ğŸ’» ä½¿ç”¨é»˜è®¤è®¾å¤‡é…ç½®")

        # ğŸ”§ åŠ è½½æ¨¡å‹
        try:
            logger.info("ğŸ”„ å¼€å§‹åŠ è½½æ¨¡å‹...")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name_or_path,
                **model_kwargs
            )
            logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # ğŸ”§ åº”ç”¨æ¨¡å‹ä¼˜åŒ–
            self.model = self._apply_model_optimizations(self.model)
            
            # ğŸ”§ è®°å½•æ¨¡å‹è®¾å¤‡åˆ†å¸ƒæƒ…å†µ
            self._log_model_device_info()
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # é™çº§åˆ°CPUåŠ è½½ï¼Œç„¶åæ‰‹åŠ¨åˆ†å‘
            logger.info("ğŸ”„ å°è¯•CPUåŠ è½½åæ‰‹åŠ¨åˆ†å‘...")
            model_kwargs.pop('device_map', None)
            model_kwargs.pop('max_memory', None)
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name_or_path,
                **model_kwargs
            )
            
            if self.use_multi_gpu:
                self.model = self._manual_device_dispatch()

        # k-bit training preparation
        model_is_quantized = (quantization_config_arg is not None and
                              hasattr(self.model, "is_loaded_in_4bit") and self.model.is_loaded_in_4bit) or \
                             (hasattr(self.model, "is_loaded_in_8bit") and self.model.is_loaded_in_8bit)

        if model_is_quantized:
            self.model = prepare_model_for_kbit_training(
                self.model, 
                use_gradient_checkpointing=self.grpo_cfg.gradient_checkpointing
            )
            logger.info(f"Base model prepared for k-bit training. Grad checkpointing: {self.grpo_cfg.gradient_checkpointing}.")
        elif self.grpo_cfg.gradient_checkpointing and not self.use_multi_gpu:
            # ğŸ”§ å¤šGPUæ—¶è°¨æ…ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
            self.model.gradient_checkpointing_enable()
            logger.info("Gradient checkpointing enabled for non-k-bit model.")

        # ğŸ”§ åŠ è½½tokenizer
        self._load_tokenizer()

        # Apply Qwen3CompatibilityFixer if available
        self._apply_compatibility_fixes()

        return self.model, self.tokenizer

    def _manual_device_dispatch(self):
        """ğŸ”§ æ‰‹åŠ¨è®¾å¤‡åˆ†å‘ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        try:
            logger.info("ğŸ”§ æ‰§è¡Œæ‰‹åŠ¨è®¾å¤‡åˆ†å‘...")
            
            # æ¨æ–­è®¾å¤‡æ˜ å°„
            device_map = infer_auto_device_map(
                self.model,
                max_memory={i: "75GiB" for i in range(self.num_gpus)},
                no_split_module_classes=["LlamaDecoderLayer", "QWenBlock", "TransformerBlock"]
            )
            
            logger.info(f"ğŸ—ºï¸ è®¾å¤‡æ˜ å°„: {json.dumps(device_map, indent=2)}")
            
            # åˆ†å‘æ¨¡å‹
            model = dispatch_model(self.model, device_map=device_map)
            logger.info("âœ… æ‰‹åŠ¨è®¾å¤‡åˆ†å‘å®Œæˆ")
            
            return model
            
        except Exception as e:
            logger.warning(f"âš ï¸ æ‰‹åŠ¨è®¾å¤‡åˆ†å‘å¤±è´¥: {e}")
            return self.model

    def _log_model_device_info(self):
        """ğŸ”§ è®°å½•æ¨¡å‹è®¾å¤‡åˆ†å¸ƒä¿¡æ¯"""
        try:
            if hasattr(self.model, 'hf_device_map'):
                device_map = self.model.hf_device_map
                logger.info("ğŸ“ æ¨¡å‹è®¾å¤‡åˆ†å¸ƒ:")
                
                # ç»Ÿè®¡æ¯ä¸ªè®¾å¤‡çš„å±‚æ•°
                device_counts = {}
                for layer_name, device in device_map.items():
                    device_str = str(device)
                    device_counts[device_str] = device_counts.get(device_str, 0) + 1
                
                for device, count in device_counts.items():
                    logger.info(f"  - {device}: {count} å±‚")
                    
                # æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨
                if torch.cuda.is_available():
                    for i in range(self.num_gpus):
                        if i < torch.cuda.device_count():
                            allocated = torch.cuda.memory_allocated(i) / 1024**3
                            reserved = torch.cuda.memory_reserved(i) / 1024**3
                            logger.info(f"  - GPU {i}: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved")
            
        except Exception as e:
            logger.debug(f"è®¾å¤‡ä¿¡æ¯è®°å½•å¤±è´¥: {e}")

    def _load_tokenizer(self):
        """ğŸ”§ åŠ è½½tokenizer"""
        tokenizer_load_path = self.model_name_or_path
        if self.script_cfg.stage1_adapter_path and os.path.isdir(self.script_cfg.stage1_adapter_path):
            potential_tokenizer_path = self.script_cfg.stage1_adapter_path
            if os.path.exists(os.path.join(potential_tokenizer_path, "tokenizer_config.json")):
                tokenizer_load_path = potential_tokenizer_path
                logger.info(f"Loading tokenizer from stage1_adapter_path: {tokenizer_load_path}")

        self.tokenizer = AutoTokenizer.from_pretrained(
            tokenizer_load_path, 
            trust_remote_code=True, 
            use_fast=True, 
            cache_dir=self.cache_dir
        )
        
        # ğŸ”§ ç¡®ä¿tokenizeræœ‰pad_token
        if self.tokenizer.pad_token is None:
            if self.tokenizer.eos_token:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                logger.info("è®¾ç½® pad_token = eos_token")
            else:
                self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})
                logger.info("æ·»åŠ æ–°çš„ pad_token: [PAD]")
        
        logger.info(f"Tokenizer loaded from: {tokenizer_load_path}")

    def _apply_compatibility_fixes(self):
        """ğŸ”§ åº”ç”¨å…¼å®¹æ€§ä¿®å¤"""
        try:
            from grpo_project.utils.model_utils import Qwen3CompatibilityFixer
            self.model, self.tokenizer = Qwen3CompatibilityFixer.fix_generation_config(self.model, self.tokenizer)
            logger.info("Applied Qwen3CompatibilityFixer.")
        except ImportError:
            logger.debug("Qwen3CompatibilityFixer not found, skipping this fix.")
        except Exception as e_compat:
            logger.warning(f"Error during Qwen3CompatibilityFixer application: {e_compat}, skipping this fix.")

    def apply_peft_adapter(self, model, is_resuming: bool, resume_checkpoint_path: Optional[str] = None):
        """
        Applies PEFT adapter to the model with multi-GPU support.
        Handles resuming from checkpoint, loading stage1 adapter, or creating a new one.
        """
        if model is None:
            logger.error("Model is None, cannot apply PEFT adapter.")
            raise ValueError("Model not loaded before applying PEFT adapter.")

        # ğŸ”§ å¤šGPUç¯å¢ƒä¸‹çš„PEFTé…ç½®
        peft_config = LoraConfig(
            r=self.script_cfg.lora_rank,
            lora_alpha=self.script_cfg.lora_alpha,
            lora_dropout=self.script_cfg.lora_dropout,
            target_modules=self.script_cfg.lora_target_modules,
            bias="none",
            task_type=TaskType.CAUSAL_LM,
        )

        peft_applied_successfully = False
        is_model_already_peft = isinstance(model, PeftModel)

        # ğŸ”§ å¤„ç†æ–­ç»­è®­ç»ƒ
        if is_resuming and resume_checkpoint_path:
            logger.info(f"Resuming from checkpoint: {resume_checkpoint_path}")
            if os.path.exists(os.path.join(resume_checkpoint_path, 'adapter_config.json')):
                try:
                    if not is_model_already_peft:
                        # ğŸ”§ å¤šGPUç¯å¢ƒä¸‹åŠ è½½PEFTæ¨¡å‹
                        model = PeftModel.from_pretrained(
                            model, 
                            resume_checkpoint_path, 
                            is_trainable=True,
                            # å¯¹äºå¤šGPUåˆ†å¸ƒå¼æ¨¡å‹ï¼ŒPEFTä¼šè‡ªåŠ¨å¤„ç†è®¾å¤‡æ˜ å°„
                        )
                        logger.info("PEFT adapter loaded from checkpoint for base model.")
                        peft_applied_successfully = True
                    else:
                        logger.info("Model is already PEFT. Trainer will handle adapter state from checkpoint.")
                        peft_applied_successfully = True
                except Exception as e:
                    logger.error(f"Failed to load PEFT adapter from checkpoint {resume_checkpoint_path}: {e}.")
            else:
                logger.warning(f"No adapter_config.json found in checkpoint {resume_checkpoint_path}.")

        # ğŸ”§ å¤„ç†Stage 1é€‚é…å™¨åŠ è½½
        if not peft_applied_successfully and self.script_cfg.stage1_adapter_path and \
           os.path.isdir(self.script_cfg.stage1_adapter_path):
            if is_model_already_peft:
                logger.warning("Model is already PEFT, skipping Stage 1 adapter loading.")
            else:
                try:
                    logger.info(f"Loading Stage 1 PEFT adapter from: {self.script_cfg.stage1_adapter_path}")
                    model = PeftModel.from_pretrained(
                        model, 
                        self.script_cfg.stage1_adapter_path, 
                        is_trainable=True
                    )
                    logger.info("Stage 1 PEFT adapter loaded successfully.")
                    peft_applied_successfully = True
                except Exception as e:
                    logger.error(f"Failed to load Stage 1 PEFT adapter: {e}. Will attempt to create a new one.")

        # ğŸ”§ åˆ›å»ºæ–°çš„PEFTé€‚é…å™¨
        if not peft_applied_successfully and not is_model_already_peft:
            try:
                logger.info("Creating new PEFT adapter...")
                model = get_peft_model(model, peft_config)
                logger.info("New PEFT adapter created successfully.")
                
                # ğŸ”§ å¤šGPUç¯å¢ƒä¸‹çš„PEFTä¼˜åŒ–
                if self.use_multi_gpu:
                    self._optimize_peft_for_multi_gpu(model)
                    
            except Exception as e:
                logger.error(f"Failed to create new PEFT adapter: {e}", exc_info=True)
                raise RuntimeError("Could not apply PEFT adapter to model.") from e

        # ğŸ”§ éªŒè¯PEFTæ¨¡å‹
        if isinstance(model, PeftModel):
            model.print_trainable_parameters()
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            if trainable_params == 0:
                logger.error("CRITICAL: No trainable parameters found after PEFT setup.")
                self._fix_peft_trainable_params(model)
        elif not peft_applied_successfully:
            logger.error("Model is not a PeftModel and no PEFT adapter was successfully applied.")

        return model

    def _optimize_peft_for_multi_gpu(self, model):
        """ğŸ”§ å¤šGPUç¯å¢ƒä¸‹çš„PEFTä¼˜åŒ–"""
        try:
            # ç¡®ä¿PEFTé€‚é…å™¨åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            if hasattr(model, 'peft_config'):
                for adapter_name in model.peft_config.keys():
                    # è®¾ç½®é€‚é…å™¨ä¸ºå¯è®­ç»ƒ
                    model.set_adapter(adapter_name)
                    logger.info(f"âœ… è®¾ç½®é€‚é…å™¨ {adapter_name} ä¸ºå½“å‰æ´»è·ƒé€‚é…å™¨")
            
            # æ£€æŸ¥æ¨¡å‹å‚æ•°åˆ†å¸ƒ
            logger.info("ğŸ” æ£€æŸ¥PEFTå‚æ•°åˆ†å¸ƒ:")
            device_param_count = {}
            for name, param in model.named_parameters():
                if param.requires_grad:
                    device = str(param.device)
                    device_param_count[device] = device_param_count.get(device, 0) + param.numel()
            
            for device, count in device_param_count.items():
                logger.info(f"  - {device}: {count:,} å¯è®­ç»ƒå‚æ•°")
                
        except Exception as e:
            logger.warning(f"âš ï¸ PEFTå¤šGPUä¼˜åŒ–å¤±è´¥: {e}")

    def _fix_peft_trainable_params(self, model):
        """ğŸ”§ ä¿®å¤PEFTå¯è®­ç»ƒå‚æ•°é—®é¢˜"""
        try:
            model.train()
            if hasattr(model, 'enable_adapters'): 
                model.enable_adapters()
            
            # ç¡®ä¿æ‰€æœ‰é€‚é…å™¨éƒ½è®¾ç½®ä¸ºå¯è®­ç»ƒ
            for adapter_name in model.peft_config.keys():
                model.set_adapter(adapter_name)
                
            trainable_params_after_fix = sum(p.numel() for p in model.parameters() if p.requires_grad)
            if trainable_params_after_fix > 0:
                logger.info(f"Successfully enabled trainable PEFT parameters: {trainable_params_after_fix}")
                model.print_trainable_parameters()
            else:
                raise RuntimeError("Still no trainable PEFT parameters after attempting fix.")
                
        except Exception as e_fix:
            raise RuntimeError(f"Failed to enable trainable PEFT parameters: {e_fix}") from e_fix

    def get_model_memory_info(self):
        """ğŸ”§ è·å–æ¨¡å‹å†…å­˜ä½¿ç”¨ä¿¡æ¯"""
        info = {
            "model_parameters": sum(p.numel() for p in self.model.parameters()),
            "trainable_parameters": sum(p.numel() for p in self.model.parameters() if p.requires_grad),
            "gpu_memory": {}
        }
        
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                info["gpu_memory"][f"gpu_{i}"] = {
                    "allocated": torch.cuda.memory_allocated(i) / 1024**3,
                    "reserved": torch.cuda.memory_reserved(i) / 1024**3,
                    "max_memory": torch.cuda.get_device_properties(i).total_memory / 1024**3
                }
        
        return info