import logging
import os
import json
from collections import deque # For DetailedWandbCallback from train.py
from typing import Dict, Any, Optional, List # General typing
import numpy as np # For DetailedWandbCallback from train.py
import math # For DetailedWandbCallback from train.py

from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl
# Attempt to import from grpo_project, fallback for placeholders if necessary
try:
    from grpo_project.callbacks.base import BaseCallback
    # For the DetailedWandbCallback from train.py, it needs EnvConfig, ScriptConfig, RewardConfig
    from grpo_project.configs import EnvConfig, ScriptConfig, EnhancedRewardConfig as RewardConfig
    # For GenericWandbCallback (from utils.py), it needs ExperienceBuffer
    from grpo_project.utils import ExperienceBuffer # Updated import
except ImportError:
    logger_init = logging.getLogger(__name__)
    logger_init.warning("Callbacks.wandb: Could not import from grpo_project or utils. Using placeholders.") # Updated path in log
    from transformers import TrainerCallback as BaseCallback # Fallback
    class EnvConfig: pass
    class ScriptConfig: pass
    class RewardConfig: pass
    class ExperienceBuffer: pass


logger = logging.getLogger(__name__)

# Originally from train.py
class DetailedWandbCallback(BaseCallback): # Changed to inherit from BaseCallback
    """å¢å¼ºçš„ W&B æ—¥å¿—å›è°ƒï¼Œæ”¯æŒæ¢å¤è®­ç»ƒæ—¶çš„æ•°æ®è¿ç»­æ€§"""

    def __init__(self, env_cfg: EnvConfig, script_cfg: ScriptConfig, reward_cfg: RewardConfig,
                 experience_buffer: Optional[ExperienceBuffer] = None, output_dir: Optional[str] = None):
        super().__init__(output_dir=output_dir) # Pass output_dir to BaseCallback
        self.env_cfg = env_cfg
        self.script_cfg = script_cfg
        self.reward_cfg = reward_cfg
        self.experience_buffer = experience_buffer
        self.step_count = 0
        self.recent_rewards: deque[float] = deque(maxlen=100)
        self.wandb_initialized = False
        self.wandb_run = None
        
        # ğŸ”§ å…³é”®ï¼šå­˜å‚¨WandB runä¿¡æ¯ç”¨äºæ¢å¤
        self.wandb_run_id = None
        self.wandb_resume_mode = "allow"  # å…è®¸æ¢å¤å·²å­˜åœ¨çš„run
        
        logger.info(f"DetailedWandbCallback initialized. Output dir: {self.output_dir}")

    def _extract_run_id_from_checkpoint(self, checkpoint_path: str) -> Optional[str]:
        """ä»checkpointç›®å½•ä¸­æå–WandB run ID"""
        try:
            # å°è¯•ä»checkpointç›®å½•çš„çˆ¶ç›®å½•åç§°ä¸­æå–run ID
            if checkpoint_path and os.path.exists(checkpoint_path):
                # æŸ¥æ‰¾wandbç›¸å…³æ–‡ä»¶
                parent_dir = os.path.dirname(checkpoint_path)
                wandb_dir = os.path.join(parent_dir, "wandb")
                
                if os.path.exists(wandb_dir):
                    # æŸ¥æ‰¾æœ€æ–°çš„runç›®å½•
                    run_dirs = [d for d in os.listdir(wandb_dir) if d.startswith("run-")]
                    if run_dirs:
                        latest_run_dir = sorted(run_dirs)[-1]
                        run_id = latest_run_dir.replace("run-", "").split("-")[0]
                        logger.info(f"ğŸ”„ ä»checkpointç›®å½•æå–åˆ°WandB run ID: {run_id}")
                        return run_id
                
                # å°è¯•ä»trainer_state.jsonä¸­è¯»å–
                trainer_state_path = os.path.join(checkpoint_path, "trainer_state.json")
                if os.path.exists(trainer_state_path):
                    with open(trainer_state_path, 'r') as f:
                        state_data = json.load(f)
                        if 'log_history' in state_data:
                            # æŸ¥æ‰¾åŒ…å«wandbä¿¡æ¯çš„æ—¥å¿—æ¡ç›®
                            for log_entry in state_data['log_history']:
                                if '_wandb' in str(log_entry):
                                    logger.info("ğŸ”„ åœ¨trainer_state.jsonä¸­æ‰¾åˆ°WandBç›¸å…³ä¿¡æ¯")
                                    break
                
        except Exception as e:
            logger.warning(f"âš ï¸ æå–WandB run IDæ—¶å‡ºé”™: {e}")
        
        return None

    def _initialize_wandb(self, args: TrainingArguments, state: TrainerState):
        """åˆå§‹åŒ–WandBï¼Œæ”¯æŒæ¢å¤å·²å­˜åœ¨çš„run"""
        try:
            import wandb
            
            # ğŸ”§ å…³é”®ï¼šæ£€æŸ¥æ˜¯å¦ä»checkpointæ¢å¤
            is_resuming = (
                hasattr(args, 'resume_from_checkpoint') and 
                args.resume_from_checkpoint and 
                os.path.exists(args.resume_from_checkpoint)
            )
            
            # ğŸ”§ é‡è¦ï¼šä¼˜å…ˆä½¿ç”¨main.pyä¸­è®¾ç½®çš„ç¯å¢ƒå˜é‡
            env_run_id = os.getenv("WANDB_RUN_ID")
            env_resume_mode = os.getenv("WANDB_RESUME", "allow")
            
            # å‡†å¤‡WandBé…ç½®
            wandb_config = {
                # è®­ç»ƒå‚æ•°
                "learning_rate": args.learning_rate,
                "per_device_train_batch_size": args.per_device_train_batch_size,
                "gradient_accumulation_steps": args.gradient_accumulation_steps,
                "num_train_epochs": args.num_train_epochs,
                "warmup_ratio": args.warmup_ratio,
                "weight_decay": args.weight_decay,
                "lr_scheduler_type": args.lr_scheduler_type,
                
                # æ¨¡å‹å‚æ•°
                "model_name_or_path": getattr(self.script_cfg, 'model_name_or_path', 'unknown'),
                "lora_rank": getattr(self.script_cfg, 'lora_rank', 'unknown'),
                "lora_alpha": getattr(self.script_cfg, 'lora_alpha', 'unknown'),
                "max_seq_length": getattr(self.script_cfg, 'max_seq_length', 'unknown'),
                
                # å¥–åŠ±å‚æ•°
                "compilation_success": getattr(self.reward_cfg, 'compilation_success', 'unknown'),
                "test_pass_base_reward": getattr(self.reward_cfg, 'test_pass_base_reward', 'unknown'),
                
                # æ¢å¤ä¿¡æ¯
                "is_resuming": is_resuming,
                "resume_from_checkpoint": args.resume_from_checkpoint if is_resuming else None,
                "auto_resume_configured": env_run_id is not None,
            }
            
            # ğŸ”§ å…³é”®ï¼šè®¾ç½®WandBåˆå§‹åŒ–å‚æ•°
            wandb_init_kwargs = {
                "project": getattr(self.env_cfg, 'wandb_project', 'VerilogGRPO_Enhanced_v3'),
                "entity": getattr(self.env_cfg, 'wandb_entity', None),
                "config": wandb_config,
                "resume": env_resume_mode,
                "save_code": True,
                "tags": ["grpo", "verilog", "enhanced"],
            }
            
            # ğŸ”§ å…³é”®ï¼šä½¿ç”¨main.pyè®¾ç½®çš„run IDï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if env_run_id:
                wandb_init_kwargs["id"] = env_run_id
                logger.info(f"ğŸ”„ ä½¿ç”¨main.pyé…ç½®çš„WandB run ID: {env_run_id}")
                logger.info(f"ğŸ”„ æ¢å¤æ¨¡å¼: {env_resume_mode}")
            elif is_resuming:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šå°è¯•ä»checkpointç›®å½•æå–ï¼ˆè™½ç„¶main.pyåº”è¯¥å·²ç»å¤„ç†äº†ï¼‰
                extracted_run_id = self._extract_run_id_from_checkpoint(args.resume_from_checkpoint)
                if extracted_run_id:
                    wandb_init_kwargs["id"] = extracted_run_id
                    wandb_init_kwargs["resume"] = "must"
                    logger.info(f"ğŸ”„ å¤‡ç”¨æ–¹æ¡ˆï¼šä»checkpointæå–WandB run ID: {extracted_run_id}")
                else:
                    logger.warning("âš ï¸ æ— æ³•ç¡®å®šè¦æ¢å¤çš„WandB run IDï¼Œå°†åˆ›å»ºæ–°çš„run")
            
            # è®¾ç½®runåç§°ï¼ˆå¦‚æœæœªæŒ‡å®šIDï¼‰
            if "id" not in wandb_init_kwargs:
                run_name = os.getenv("WANDB_RUN_NAME")
                if run_name:
                    wandb_init_kwargs["name"] = run_name
            
            # åˆå§‹åŒ–WandB
            self.wandb_run = wandb.init(**wandb_init_kwargs)
            self.wandb_initialized = True
            self.wandb_run_id = self.wandb_run.id
            
            # ğŸ”§ é‡è¦ï¼šä¿å­˜run IDåˆ°ç¯å¢ƒå˜é‡ï¼Œä¾›åç»­æ¢å¤ä½¿ç”¨
            if not env_run_id:  # åªåœ¨æ²¡æœ‰é¢„è®¾æ—¶æ›´æ–°
                os.environ["WANDB_RUN_ID"] = self.wandb_run_id
            
            logger.info(f"âœ… WandBåˆå§‹åŒ–æˆåŠŸ!")
            logger.info(f"  - Run ID: {self.wandb_run_id}")
            logger.info(f"  - Run URL: {self.wandb_run.url}")
            logger.info(f"  - æ¢å¤æ¨¡å¼: {env_resume_mode}")
            logger.info(f"  - è‡ªåŠ¨é…ç½®: {env_run_id is not None}")
            
            # å¦‚æœæ˜¯æ¢å¤è®­ç»ƒï¼Œè®°å½•æ¢å¤ä¿¡æ¯
            if is_resuming:
                self.wandb_run.log({
                    "resume_info/checkpoint_path": args.resume_from_checkpoint,
                    "resume_info/global_step": state.global_step,
                    "resume_info/epoch": state.epoch,
                    "resume_info/auto_configured": env_run_id is not None,
                }, step=state.global_step)
                
        except Exception as e:
            logger.error(f"âŒ WandBåˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            self.wandb_initialized = False

    def on_init_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
        """è®­ç»ƒåˆå§‹åŒ–ç»“æŸæ—¶çš„å›è°ƒ"""
        self._initialize_wandb(args, state)

    def on_log(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, 
               logs: Optional[Dict[str, float]] = None, **kwargs):
        """æ—¥å¿—è®°å½•å›è°ƒ"""
        if not self.wandb_initialized or not logs:
            return
            
        try:
            import wandb
            
            # è¿‡æ»¤å’Œå¤„ç†æ—¥å¿—æ•°æ®
            wandb_logs = {}
            for key, value in logs.items():
                if isinstance(value, (int, float)) and not (math.isnan(value) or math.isinf(value)):
                    wandb_logs[key] = value
            
            # æ·»åŠ æ­¥æ•°ä¿¡æ¯
            if state.global_step:
                wandb_logs["global_step"] = state.global_step
            if state.epoch:
                wandb_logs["epoch"] = state.epoch
                
            # æ·»åŠ å¥–åŠ±ç»Ÿè®¡
            if self.recent_rewards:
                wandb_logs.update({
                    "reward_stats/mean": np.mean(self.recent_rewards),
                    "reward_stats/std": np.std(self.recent_rewards),
                    "reward_stats/min": np.min(self.recent_rewards),
                    "reward_stats/max": np.max(self.recent_rewards),
                })
            
            # è®°å½•åˆ°WandB
            if wandb_logs:
                self.wandb_run.log(wandb_logs, step=state.global_step)
                
        except Exception as e:
            logger.warning(f"âš ï¸ WandBæ—¥å¿—è®°å½•å¤±è´¥: {e}")

    def add_reward(self, reward: float):
        """æ·»åŠ å¥–åŠ±å€¼åˆ°å†å²è®°å½•"""
        if isinstance(reward, (int, float)) and not (math.isnan(reward) or math.isinf(reward)):
            self.recent_rewards.append(reward)

    def log_reward_components(self, reward_components: Dict[str, float]):
        """è®°å½•å¥–åŠ±ç»„ä»¶"""
        if not self.wandb_initialized:
            return
            
        try:
            import wandb
            
            # è¿‡æ»¤æœ‰æ•ˆçš„å¥–åŠ±ç»„ä»¶
            valid_components = {}
            for key, value in reward_components.items():
                if isinstance(value, (int, float)) and not (math.isnan(value) or math.isinf(value)):
                    valid_components[f"reward_components/{key}"] = value
            
            if valid_components:
                self.wandb_run.log(valid_components)
                
        except Exception as e:
            logger.warning(f"âš ï¸ å¥–åŠ±ç»„ä»¶è®°å½•å¤±è´¥: {e}")

    def log_batch_aggregated_metrics(self, metrics: Dict[str, Any], step: Optional[int] = None):
        """è®°å½•æ‰¹æ¬¡èšåˆæŒ‡æ ‡"""
        if not self.wandb_initialized:
            return
            
        try:
            import wandb
            
            # å¤„ç†èšåˆæŒ‡æ ‡
            wandb_metrics = {}
            for key, value in metrics.items():
                if isinstance(value, (int, float)) and not (math.isnan(value) or math.isinf(value)):
                    wandb_metrics[f"batch_metrics/{key}"] = value
                elif isinstance(value, dict):
                    # å¤„ç†åµŒå¥—å­—å…¸
                    for sub_key, sub_value in value.items():
                        if isinstance(sub_value, (int, float)) and not (math.isnan(sub_value) or math.isinf(sub_value)):
                            wandb_metrics[f"batch_metrics/{key}/{sub_key}"] = sub_value
            
            if wandb_metrics:
                self.wandb_run.log(wandb_metrics, step=step)
                
        except Exception as e:
            logger.warning(f"âš ï¸ æ‰¹æ¬¡æŒ‡æ ‡è®°å½•å¤±è´¥: {e}")

    def on_train_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
        """è®­ç»ƒç»“æŸæ—¶çš„å›è°ƒ"""
        if self.wandb_initialized:
            try:
                import wandb
                
                # è®°å½•æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
                final_stats = {
                    "final/global_step": state.global_step,
                    "final/epoch": state.epoch,
                    "final/total_flos": state.total_flos,
                }
                
                if self.recent_rewards:
                    final_stats.update({
                        "final/reward_mean": np.mean(self.recent_rewards),
                        "final/reward_std": np.std(self.recent_rewards),
                    })
                
                self.wandb_run.log(final_stats)
                logger.info("âœ… WandBæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯å·²è®°å½•")
                
            except Exception as e:
                logger.warning(f"âš ï¸ WandBæœ€ç»ˆç»Ÿè®¡è®°å½•å¤±è´¥: {e}")


# Originally from utils.py, renamed to avoid collision
class GenericWandbCallback(DetailedWandbCallback):
    """å‘åå…¼å®¹æ€§åˆ«å"""
    def __init__(self, env_config: EnvConfig, script_config: ScriptConfig, reward_config: RewardConfig,
                 experience_buffer: Optional[ExperienceBuffer] = None, output_dir: Optional[str] = None):
        super().__init__(env_config, script_config, reward_config, experience_buffer, output_dir)
