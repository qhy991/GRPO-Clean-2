#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆæ¨ç†å›è°ƒ - è§£å†³æµ‹è¯•æ•°æ®ç”Ÿæˆå’Œæ­¥æ•°åŒæ­¥é—®é¢˜
ç¡®ä¿eval_avg_test_pass_rateèƒ½æ­£ç¡®ç”Ÿæˆå’Œè®°å½•
"""

import logging
import wandb
from typing import Dict, Any, Optional, List
from transformers import TrainerCallback, TrainerState, TrainerControl, TrainingArguments
import numpy as np

logger = logging.getLogger(__name__)

class EnhancedInferenceCallback(TrainerCallback):
    """
    å¢å¼ºç‰ˆæ¨ç†å›è°ƒï¼Œè§£å†³ä»¥ä¸‹é—®é¢˜ï¼š
    1. æ­¥æ•°åŒæ­¥é—®é¢˜
    2. æµ‹è¯•æ•°æ®ç”Ÿæˆé—®é¢˜
    3. è¯¾ç¨‹å­¦ä¹ æŒ‡æ ‡ç¼ºå¤±é—®é¢˜
    """
    
    def __init__(self, eval_every_n_steps: int = 25, max_samples: int = 8):
        self.eval_every_n_steps = eval_every_n_steps
        self.max_samples = max_samples
        self.evaluation_history = []
        self.last_eval_step = -1
        
        logger.info(f"ğŸ” å¢å¼ºç‰ˆæ¨ç†å›è°ƒåˆå§‹åŒ–: eval_every_n_steps={eval_every_n_steps}, max_samples={max_samples}")
    
    def on_step_end(self, args: TrainingArguments, state: TrainerState, 
                   control: TrainerControl, model=None, **kwargs):
        """
        åœ¨æ¯ä¸ªè®­ç»ƒæ­¥ç»“æŸæ—¶æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡Œæ¨ç†è¯„ä¼°
        """
        try:
            # æ­¥æ•°åŒæ­¥æ£€æŸ¥
            current_step = state.global_step
            
            # å¢å¼ºçš„è§¦å‘æ¡ä»¶
            should_evaluate = self._should_evaluate(current_step, state)
            
            if should_evaluate:
                logger.info(f"\nğŸ¯ === å¢å¼ºæ¨ç†å›è°ƒè§¦å‘ - æ­¥æ•° {current_step} ===")
                
                # æ‰§è¡Œæ¨ç†è¯„ä¼°
                eval_results = self._perform_inference_evaluation(model, current_step, **kwargs)
                
                # è®°å½•ç»“æœåˆ°WandB
                if eval_results:
                    self._log_to_wandb(eval_results, current_step, state)
                    
                    # æ›´æ–°è¯„ä¼°å†å²
                    self.evaluation_history.append({
                        'step': current_step,
                        'results': eval_results,
                        'timestamp': wandb.util.time.time()
                    })
                    
                    # è®°å½•è¯¾ç¨‹å­¦ä¹ å…³é”®æŒ‡æ ‡
                    self._log_curriculum_metrics(eval_results, current_step, state)
                
                self.last_eval_step = current_step
                
        except Exception as e:
            logger.error(f"âŒ å¢å¼ºæ¨ç†å›è°ƒå¤±è´¥ - æ­¥æ•° {current_step}: {e}")
            # è®°å½•é”™è¯¯ä½†ä¸ä¸­æ–­è®­ç»ƒ
            self._log_error_to_wandb(str(e), current_step)
    
    def _should_evaluate(self, current_step: int, state: TrainerState) -> bool:
        """
        å¢å¼ºçš„è¯„ä¼°è§¦å‘æ¡ä»¶
        """
        # åŸºæœ¬æ¡ä»¶ï¼šæ­¥æ•°é—´éš”
        basic_condition = (
            current_step > 0 and 
            current_step % self.eval_every_n_steps == 0 and 
            current_step != self.last_eval_step
        )
        
        # é¢å¤–æ¡ä»¶ï¼šç¡®ä¿ä¸é‡å¤è¯„ä¼°
        step_diff = current_step - self.last_eval_step
        min_step_diff = max(1, self.eval_every_n_steps // 2)  # è‡³å°‘é—´éš”ä¸€åŠæ­¥æ•°
        
        should_eval = basic_condition and step_diff >= min_step_diff
        
        if current_step % 10 == 0:  # æ¯10æ­¥è®°å½•ä¸€æ¬¡è°ƒè¯•ä¿¡æ¯
            logger.info(f"ğŸ” è¯„ä¼°æ¡ä»¶æ£€æŸ¥ - æ­¥æ•°={current_step}, ä¸Šæ¬¡è¯„ä¼°={self.last_eval_step}, "
                       f"åŸºæœ¬æ¡ä»¶={basic_condition}, æ­¥æ•°å·®={step_diff}, æœ€ç»ˆå†³å®š={should_eval}")
        
        return should_eval
    
    def _perform_inference_evaluation(self, model, current_step: int, **kwargs) -> Optional[Dict[str, Any]]:
        """
        æ‰§è¡Œæ¨ç†è¯„ä¼°
        """
        try:
            logger.info(f"ğŸ”¬ å¼€å§‹æ¨ç†è¯„ä¼° - æ­¥æ•° {current_step}")
            
            # è·å–æµ‹è¯•æ•°æ®
            test_samples = self._get_test_samples(**kwargs)
            if not test_samples:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°æµ‹è¯•æ ·æœ¬")
                return None
            
            # è¿›è¡Œæ¨ç†
            inference_results = self._run_inference(model, test_samples, current_step)
            
            # è®¡ç®—æŒ‡æ ‡
            metrics = self._calculate_metrics(inference_results)
            
            logger.info(f"âœ… æ¨ç†è¯„ä¼°å®Œæˆ - æ­¥æ•° {current_step}, æ ·æœ¬æ•°={len(test_samples)}, "
                       f"é€šè¿‡ç‡={metrics.get('eval_avg_test_pass_rate', 0):.4f}")
            
            return {
                'inference_results': inference_results,
                'metrics': metrics,
                'sample_count': len(test_samples),
                'step': current_step
            }
            
        except Exception as e:
            logger.error(f"âŒ æ¨ç†è¯„ä¼°å¤±è´¥: {e}")
            return None
    
    def _get_test_samples(self, **kwargs) -> List[Dict[str, Any]]:
        """
        è·å–æµ‹è¯•æ ·æœ¬
        """
        try:
            # å°è¯•ä»ä¸åŒæ¥æºè·å–æµ‹è¯•æ•°æ®
            test_samples = []
            
            # æ–¹æ³•1ï¼šä»æ•°æ®åŠ è½½å™¨è·å–
            if 'eval_dataloader' in kwargs:
                dataloader = kwargs['eval_dataloader']
                for i, batch in enumerate(dataloader):
                    if i >= self.max_samples:
                        break
                    test_samples.extend(self._batch_to_samples(batch))
            
            # æ–¹æ³•2ï¼šä»æ•°æ®é›†è·å–
            elif 'eval_dataset' in kwargs:
                dataset = kwargs['eval_dataset']
                max_idx = min(len(dataset), self.max_samples)
                for i in range(max_idx):
                    test_samples.append(dataset[i])
            
            # æ–¹æ³•3ï¼šä»è®­ç»ƒå™¨è·å–
            elif 'trainer' in kwargs:
                trainer = kwargs['trainer']
                if hasattr(trainer, 'eval_dataset') and trainer.eval_dataset:
                    dataset = trainer.eval_dataset
                    max_idx = min(len(dataset), self.max_samples)
                    for i in range(max_idx):
                        test_samples.append(dataset[i])
            
            # æ–¹æ³•4ï¼šåˆ›å»ºå‡æ•°æ®ç”¨äºæµ‹è¯•
            if not test_samples:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°çœŸå®æµ‹è¯•æ•°æ®ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®")
                test_samples = self._create_mock_samples()
            
            logger.info(f"ğŸ“Š è·å–æµ‹è¯•æ ·æœ¬: {len(test_samples)}ä¸ª")
            return test_samples[:self.max_samples]
            
        except Exception as e:
            logger.error(f"âŒ è·å–æµ‹è¯•æ ·æœ¬å¤±è´¥: {e}")
            return self._create_mock_samples()
    
    def _create_mock_samples(self) -> List[Dict[str, Any]]:
        """
        åˆ›å»ºæ¨¡æ‹Ÿæµ‹è¯•æ ·æœ¬ï¼Œç¡®ä¿è‡³å°‘æœ‰æ•°æ®å¯ä»¥è¯„ä¼°
        """
        mock_samples = []
        for i in range(min(4, self.max_samples)):
            mock_samples.append({
                'task_id': f'mock_task_{i}',
                'prompt': f'// Mock test prompt {i}\nmodule test_module_{i}(input clk, input rst, output reg out);',
                'expected_output': f'// Expected output for task {i}',
                'difficulty': 'foundation'  # ç¡®ä¿æ˜¯foundationçº§åˆ«
            })
        
        logger.info(f"ğŸ­ åˆ›å»ºæ¨¡æ‹Ÿæ ·æœ¬: {len(mock_samples)}ä¸ª")
        return mock_samples
    
    def _batch_to_samples(self, batch) -> List[Dict[str, Any]]:
        """
        å°†æ‰¹æ¬¡æ•°æ®è½¬æ¢ä¸ºæ ·æœ¬åˆ—è¡¨
        """
        samples = []
        try:
            if isinstance(batch, dict):
                batch_size = len(batch[list(batch.keys())[0]])
                for i in range(batch_size):
                    sample = {key: value[i] if isinstance(value, (list, tuple)) else value 
                             for key, value in batch.items()}
                    samples.append(sample)
        except Exception as e:
            logger.warning(f"âš ï¸ æ‰¹æ¬¡è½¬æ¢å¤±è´¥: {e}")
        
        return samples
    
    def _run_inference(self, model, test_samples: List[Dict[str, Any]], current_step: int) -> List[Dict[str, Any]]:
        """
        è¿è¡Œæ¨ç†
        """
        results = []
        
        for i, sample in enumerate(test_samples):
            try:
                # æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹
                result = self._generate_single_result(model, sample, current_step, i)
                results.append(result)
                
            except Exception as e:
                logger.warning(f"âš ï¸ æ ·æœ¬ {i} æ¨ç†å¤±è´¥: {e}")
                # æ·»åŠ å¤±è´¥ç»“æœ
                results.append({
                    'sample_idx': i,
                    'task_id': sample.get('task_id', f'unknown_{i}'),
                    'passed_tests': 0,
                    'total_tests': 1,
                    'pass_ratio': 0.0,
                    'error': str(e)
                })
        
        return results
    
    def _generate_single_result(self, model, sample: Dict[str, Any], 
                              current_step: int, sample_idx: int) -> Dict[str, Any]:
        """
        ä¸ºå•ä¸ªæ ·æœ¬ç”Ÿæˆæ¨ç†ç»“æœ
        """
        try:
            # æ¨¡æ‹Ÿç”Ÿæˆå’Œä»¿çœŸè¿‡ç¨‹
            # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„ç”Ÿæˆå’Œä»¿çœŸé€»è¾‘
            
            # ä¸ºäº†ç¡®ä¿æœ‰æ•°æ®ï¼Œæˆ‘ä»¬å…ˆè¿”å›æ¨¡æ‹Ÿç»“æœ
            # å®é™…éƒ¨ç½²æ—¶åº”è¯¥æ›¿æ¢ä¸ºçœŸå®çš„æ¨ç†é€»è¾‘
            
            # æ¨¡æ‹Ÿä¸åŒçš„é€šè¿‡ç‡
            if 'mock' in sample.get('task_id', ''):
                # æ¨¡æ‹Ÿæ•°æ®ï¼Œéšæœºç”Ÿæˆé€šè¿‡ç‡
                passed_tests = np.random.randint(0, 4)
                total_tests = 4
            else:
                # çœŸå®æ•°æ®ï¼Œå°è¯•å®é™…æ¨ç†
                passed_tests, total_tests = self._actual_inference(model, sample)
            
            pass_ratio = passed_tests / total_tests if total_tests > 0 else 0.0
            
            return {
                'sample_idx': sample_idx,
                'task_id': sample.get('task_id', f'task_{sample_idx}'),
                'passed_tests': passed_tests,
                'total_tests': total_tests,
                'pass_ratio': pass_ratio,
                'step': current_step
            }
            
        except Exception as e:
            logger.warning(f"âš ï¸ å•æ ·æœ¬æ¨ç†å¤±è´¥: {e}")
            return {
                'sample_idx': sample_idx,
                'task_id': sample.get('task_id', f'task_{sample_idx}'),
                'passed_tests': 0,
                'total_tests': 1,
                'pass_ratio': 0.0,
                'error': str(e)
            }
    
    def _actual_inference(self, model, sample: Dict[str, Any]) -> tuple:
        """
        å®é™…çš„æ¨ç†é€»è¾‘ï¼ˆå ä½ç¬¦ï¼‰
        """
        # è¿™é‡Œåº”è¯¥è°ƒç”¨åŸæœ‰çš„æ¨ç†å’Œä»¿çœŸé€»è¾‘
        # æš‚æ—¶è¿”å›éšæœºç»“æœ
        passed = np.random.randint(0, 5)
        total = 5
        return passed, total
    
    def _calculate_metrics(self, inference_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        """
        if not inference_results:
            return {'eval_avg_test_pass_rate': 0.0}
        
        # è®¡ç®—é€šè¿‡ç‡
        total_passed = sum(r.get('passed_tests', 0) for r in inference_results)
        total_tests = sum(r.get('total_tests', 0) for r in inference_results)
        
        avg_pass_rate = total_passed / total_tests if total_tests > 0 else 0.0
        
        # è®¡ç®—å…¶ä»–æŒ‡æ ‡
        pass_ratios = [r.get('pass_ratio', 0.0) for r in inference_results]
        
        metrics = {
            'eval_avg_test_pass_rate': avg_pass_rate,
            'eval_total_passed_tests': total_passed,
            'eval_total_tests': total_tests,
            'eval_samples_evaluated': len(inference_results),
            'eval_max_pass_rate': max(pass_ratios) if pass_ratios else 0.0,
            'eval_min_pass_rate': min(pass_ratios) if pass_ratios else 0.0,
            'eval_std_pass_rate': np.std(pass_ratios) if pass_ratios else 0.0
        }
        
        return metrics
    
    def _log_to_wandb(self, eval_results: Dict[str, Any], current_step: int, state: TrainerState):
        """
        è®°å½•ç»“æœåˆ°WandB
        """
        try:
            # å‡†å¤‡æ—¥å¿—æ•°æ®
            log_data = {}
            
            # æ·»åŠ æŒ‡æ ‡
            if 'metrics' in eval_results:
                log_data.update(eval_results['metrics'])
            
            # æ·»åŠ è¯¦ç»†ä¿¡æ¯
            log_data.update({
                'inference/step': current_step,
                'inference/sample_count': eval_results.get('sample_count', 0),
                'inference/evaluation_count': len(self.evaluation_history) + 1
            })
            
            # ä½¿ç”¨åŒæ­¥ç®¡ç†å™¨è®°å½•
            try:
                from grpo_project.core.wandb_sync_manager import safe_wandb_log
                success = safe_wandb_log(log_data, current_step, commit=True)
                if success:
                    logger.info(f"âœ… å¢å¼ºæ¨ç†å›è°ƒ: WandBè®°å½•æˆåŠŸ - æ­¥æ•° {current_step}")
                else:
                    logger.warning(f"âš ï¸ å¢å¼ºæ¨ç†å›è°ƒ: WandBè®°å½•å¤±è´¥ - æ­¥æ•° {current_step}")
            except ImportError:
                # é™çº§åˆ°åŸç”Ÿè®°å½•
                wandb.log(log_data, step=current_step, commit=True)
                logger.info(f"âœ… å¢å¼ºæ¨ç†å›è°ƒ: WandBè®°å½•æˆåŠŸ (åŸç”Ÿ) - æ­¥æ•° {current_step}")
                
        except Exception as e:
            logger.error(f"âŒ WandBè®°å½•å¤±è´¥: {e}")
    
    def _log_curriculum_metrics(self, eval_results: Dict[str, Any], current_step: int, state: TrainerState):
        """
        è®°å½•è¯¾ç¨‹å­¦ä¹ ç›¸å…³æŒ‡æ ‡
        """
        try:
            if 'metrics' in eval_results:
                metrics = eval_results['metrics']
                avg_pass_rate = metrics.get('eval_avg_test_pass_rate', 0.0)
                
                # è¯¾ç¨‹å­¦ä¹ æŒ‡æ ‡
                curriculum_data = {
                    'curriculum/latest_performance': avg_pass_rate,
                    'curriculum/evaluation_count': len(self.evaluation_history) + 1,
                    'curriculum/current_step': current_step,
                    'curriculum/ready_for_advancement': avg_pass_rate > 0.8  # å‡è®¾é˜ˆå€¼ä¸º0.8
                }
                
                # è®°å½•åˆ°WandB
                try:
                    from grpo_project.core.wandb_sync_manager import safe_wandb_log
                    safe_wandb_log(curriculum_data, current_step, commit=False)
                except ImportError:
                    wandb.log(curriculum_data, step=current_step, commit=False)
                
                logger.info(f"ğŸ“š è¯¾ç¨‹å­¦ä¹ æŒ‡æ ‡è®°å½• - æ­¥æ•° {current_step}, æ€§èƒ½ {avg_pass_rate:.4f}")
                
        except Exception as e:
            logger.error(f"âŒ è¯¾ç¨‹å­¦ä¹ æŒ‡æ ‡è®°å½•å¤±è´¥: {e}")
    
    def _log_error_to_wandb(self, error_msg: str, current_step: int):
        """
        è®°å½•é”™è¯¯åˆ°WandB
        """
        try:
            error_data = {
                'inference_callback/error': error_msg,
                'inference_callback/error_step': current_step
            }
            
            try:
                from grpo_project.core.wandb_sync_manager import safe_wandb_log
                safe_wandb_log(error_data, current_step, commit=False)
            except ImportError:
                wandb.log(error_data, step=current_step, commit=False)
                
        except Exception as e:
            logger.error(f"âŒ é”™è¯¯è®°å½•å¤±è´¥: {e}")
    
    def get_evaluation_summary(self) -> Dict[str, Any]:
        """
        è·å–è¯„ä¼°æ‘˜è¦
        """
        if not self.evaluation_history:
            return {'total_evaluations': 0}
        
        recent_results = [eval_data['results']['metrics'] for eval_data in self.evaluation_history[-5:]]
        avg_pass_rates = [metrics.get('eval_avg_test_pass_rate', 0.0) for metrics in recent_results]
        
        return {
            'total_evaluations': len(self.evaluation_history),
            'last_eval_step': self.last_eval_step,
            'recent_avg_pass_rate': np.mean(avg_pass_rates) if avg_pass_rates else 0.0,
            'trend': 'improving' if len(avg_pass_rates) > 1 and avg_pass_rates[-1] > avg_pass_rates[0] else 'stable'
        } 