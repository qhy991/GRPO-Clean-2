#!/usr/bin/env python3
"""
æ‰‹åŠ¨DEBUGç›‘æ§è„šæœ¬ - å®æ—¶åˆ†æè®­ç»ƒæ—¥å¿—å¹¶æå–ç”Ÿæˆæ ·æœ¬ä¿¡æ¯
å½“è®­ç»ƒä»£ç æ²¡æœ‰å†…ç½®DEBUGåŠŸèƒ½æ—¶ä½¿ç”¨
"""
import os
import re
import json
import time
from pathlib import Path
from datetime import datetime
from collections import defaultdict

def parse_training_log(log_file):
    """è§£æè®­ç»ƒæ—¥å¿—ï¼Œæå–æœ‰ç”¨ä¿¡æ¯"""
    if not os.path.exists(log_file):
        return {}
    
    with open(log_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æå–è®­ç»ƒæŒ‡æ ‡
    metrics = {
        'steps': [],
        'losses': [],
        'rewards': [],
        'completions': [],
        'warnings': []
    }
    
    # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼ˆæ”¯æŒå¤šè¡ŒåŒ¹é…ï¼‰
    patterns = {
        'step_metrics': r"{'loss': ([-\d.]+).*?'reward': ([-\d.]+).*?'completions/mean_length': ([\d.]+).*?'epoch': ([\d.]+)}",
        'warnings': r"\[RANK \d+\] \d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d+ - WARNING - (.+?)(?=\n|\r|$)",
        'generation_info': r"completions/mean_length': ([\d.]+).*?'completions/min_length': ([\d.]+).*?'completions/max_length': ([\d.]+)"
    }
    
    # æå–æ­¥éª¤æŒ‡æ ‡ (ä½¿ç”¨ DOTALL æ ‡å¿—åŒ¹é…æ¢è¡Œç¬¦)
    for match in re.finditer(patterns['step_metrics'], content, re.DOTALL):
        loss, reward, mean_length, epoch = match.groups()
        metrics['steps'].append({
            'loss': float(loss),
            'reward': float(reward),
            'mean_length': float(mean_length),
            'epoch': float(epoch),
            'timestamp': datetime.now().isoformat()
        })
    
    # æå–è­¦å‘Šä¿¡æ¯
    for match in re.finditer(patterns['warnings'], content, re.DOTALL):
        warning_msg = match.group(1)
        metrics['warnings'].append({
            'message': warning_msg,
            'timestamp': datetime.now().isoformat()
        })
    
    # æå–ç”Ÿæˆä¿¡æ¯
    for match in re.finditer(patterns['generation_info'], content, re.DOTALL):
        mean_len, min_len, max_len = match.groups()
        metrics['completions'].append({
            'mean_length': float(mean_len),
            'min_length': float(min_len),
            'max_length': float(max_len),
            'timestamp': datetime.now().isoformat()
        })
    
    return metrics

def analyze_training_progress(metrics):
    """åˆ†æè®­ç»ƒè¿›åº¦"""
    if not metrics['steps']:
        return {}
    
    latest_step = metrics['steps'][-1]
    
    analysis = {
        'total_steps': len(metrics['steps']),
        'latest_metrics': latest_step,
        'avg_reward': sum(s['reward'] for s in metrics['steps']) / len(metrics['steps']),
        'avg_completion_length': sum(c['mean_length'] for c in metrics['completions']) / len(metrics['completions']) if metrics['completions'] else 0,
        'warning_count': len(metrics['warnings']),
        'estimated_generations': len(metrics['steps']) * 16,  # å‡è®¾æ¯æ­¥16ä¸ªç”Ÿæˆ
        'progress_summary': {
            'current_epoch': latest_step['epoch'],
            'reward_trend': 'improving' if len(metrics['steps']) > 5 and metrics['steps'][-1]['reward'] > metrics['steps'][-5]['reward'] else 'stable',
            'completion_quality': 'good' if latest_step.get('mean_length', 0) > 100 else 'needs_improvement'
        }
    }
    
    return analysis

def save_debug_summary(output_dir, metrics, analysis):
    """ä¿å­˜DEBUGæ‘˜è¦"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜è¯¦ç»†æŒ‡æ ‡
    metrics_file = os.path.join(output_dir, f"manual_debug_metrics_{timestamp}.json")
    with open(metrics_file, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜åˆ†æç»“æœ
    analysis_file = os.path.join(output_dir, f"manual_debug_analysis_{timestamp}.json")
    with open(analysis_file, 'w', encoding='utf-8') as f:
        json.dump(analysis, f, indent=2, ensure_ascii=False)
    
    # ç”Ÿæˆå¯è¯»æ‘˜è¦
    summary_file = os.path.join(output_dir, f"manual_debug_summary_{timestamp}.txt")
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(f"ğŸ› æ‰‹åŠ¨DEBUGæ‘˜è¦ - {timestamp}\n")
        f.write("=" * 50 + "\n\n")
        
        f.write(f"ğŸ“Š è®­ç»ƒè¿›åº¦:\n")
        f.write(f"  æ€»æ­¥æ•°: {analysis['total_steps']}\n")
        f.write(f"  å½“å‰è½®æ¬¡: {analysis['latest_metrics']['epoch']:.3f}\n")
        f.write(f"  ä¼°è®¡ç”Ÿæˆæ ·æœ¬æ•°: {analysis['estimated_generations']}\n")
        f.write(f"  è­¦å‘Šæ•°é‡: {analysis['warning_count']}\n\n")
        
        f.write(f"ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:\n")
        f.write(f"  æœ€æ–°Loss: {analysis['latest_metrics']['loss']:.4f}\n")
        f.write(f"  æœ€æ–°Reward: {analysis['latest_metrics']['reward']:.4f}\n")
        f.write(f"  å¹³å‡Reward: {analysis['avg_reward']:.4f}\n")
        f.write(f"  å¹³å‡ç”Ÿæˆé•¿åº¦: {analysis['avg_completion_length']:.1f}\n\n")
        
        f.write(f"ğŸ¯ è´¨é‡è¯„ä¼°:\n")
        f.write(f"  å¥–åŠ±è¶‹åŠ¿: {analysis['progress_summary']['reward_trend']}\n")
        f.write(f"  ç”Ÿæˆè´¨é‡: {analysis['progress_summary']['completion_quality']}\n\n")
        
        if metrics['warnings']:
            f.write(f"âš ï¸  æœ€è¿‘è­¦å‘Š:\n")
            for warning in metrics['warnings'][-5:]:  # æ˜¾ç¤ºæœ€è¿‘5ä¸ªè­¦å‘Š
                f.write(f"  - {warning['message'][:100]}...\n")
    
    return metrics_file, analysis_file, summary_file

def monitor_training():
    """ç›‘æ§è®­ç»ƒè¿›ç¨‹"""
    # æŸ¥æ‰¾æœ€æ–°çš„è®­ç»ƒæ—¥å¿—
    debug_base = "./model_parallel_only_outputs/debug_data"
    log_dirs = list(Path(debug_base).glob("training_logs/*/"))
    
    if not log_dirs:
        print("âŒ æœªæ‰¾åˆ°è®­ç»ƒæ—¥å¿—ç›®å½•")
        return
    
    latest_log_dir = max(log_dirs, key=lambda p: p.stat().st_mtime)
    log_file = latest_log_dir / "full_training_log.txt"
    
    print(f"ğŸ” ç›‘æ§è®­ç»ƒæ—¥å¿—: {log_file}")
    
    output_dir = latest_log_dir / "manual_debug"
    
    while True:
        try:
            # è§£ææ—¥å¿—
            metrics = parse_training_log(log_file)
            
            if metrics['steps']:
                # åˆ†æè¿›åº¦
                analysis = analyze_training_progress(metrics)
                
                # ä¿å­˜DEBUGä¿¡æ¯
                metrics_file, analysis_file, summary_file = save_debug_summary(output_dir, metrics, analysis)
                
                # æ‰“å°æ‘˜è¦
                print(f"\nğŸ› æ‰‹åŠ¨DEBUGæ›´æ–° ({datetime.now().strftime('%H:%M:%S')})")
                print(f"ğŸ“Š æ­¥æ•°: {analysis['total_steps']}, è½®æ¬¡: {analysis['latest_metrics']['epoch']:.3f}")
                print(f"ğŸ“ˆ Loss: {analysis['latest_metrics']['loss']:.4f}, Reward: {analysis['latest_metrics']['reward']:.4f}")
                print(f"ğŸ“ ç”Ÿæˆé•¿åº¦: {analysis['latest_metrics']['mean_length']:.1f}")
                print(f"âš ï¸  è­¦å‘Š: {analysis['warning_count']} ä¸ª")
                print(f"ğŸ’¾ å·²ä¿å­˜: {os.path.basename(summary_file)}")
            else:
                print(f"â³ ç­‰å¾…è®­ç»ƒæ•°æ®... ({datetime.now().strftime('%H:%M:%S')})")
            
            time.sleep(30)  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
            
        except KeyboardInterrupt:
            print("\nğŸ›‘ ç›‘æ§åœæ­¢")
            break
        except Exception as e:
            print(f"âŒ ç›‘æ§é”™è¯¯: {e}")
            time.sleep(10)

if __name__ == "__main__":
    print("ğŸ” å¯åŠ¨æ‰‹åŠ¨DEBUGç›‘æ§...")
    print("ğŸ“ æ­¤è„šæœ¬å°†åˆ†æè®­ç»ƒæ—¥å¿—å¹¶æå–ç”Ÿæˆä¿¡æ¯")
    print("âš¡ æ¯30ç§’æ›´æ–°ä¸€æ¬¡ï¼ŒæŒ‰Ctrl+Cåœæ­¢")
    print("-" * 50)
    
    monitor_training() 