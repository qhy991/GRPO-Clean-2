#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤åçš„è¯¾ç¨‹å­¦ä¹ å›è°ƒåŠŸèƒ½
"""

import sys
import os
import tempfile
import numpy as np
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('.')

from grpo_project.curriculum.callbacks import CurriculumProgressCallback
from grpo_project.curriculum.stages import CurriculumStageConfig

# æ¨¡æ‹Ÿtrainerç›¸å…³ç±»
class MockTrainerState:
    def __init__(self):
        self.global_step = 0
        self.log_history = []

class MockTrainingArguments:
    def __init__(self):
        self.local_rank = 0

class MockTrainerControl:
    pass

# æ¨¡æ‹Ÿè¯¾ç¨‹ç®¡ç†å™¨
class MockCurriculumManager:
    def __init__(self):
        self.current_stage = 0
        self.curriculum_stages = [
            CurriculumStageConfig(
                name="foundation",
                dataset_levels=["basic"],
                complexity_range=(0.0, 3.0),
                performance_threshold=0.7,
                min_evaluations=3,  # é™ä½ä»¥ä¾¿æµ‹è¯•
                epochs_ratio=0.3
            ),
            CurriculumStageConfig(
                name="intermediate",
                dataset_levels=["intermediate"],
                complexity_range=(3.0, 7.0),
                performance_threshold=0.6,
                min_evaluations=3,
                epochs_ratio=0.4
            ),
            CurriculumStageConfig(
                name="advanced",
                dataset_levels=["advanced"],
                complexity_range=(7.0, 10.0),
                performance_threshold=0.5,
                min_evaluations=3,
                epochs_ratio=0.3
            )
        ]
        self.stage_performance_history = []
    
    def should_advance_stage(self, performance):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿›é˜¶"""
        if self.current_stage >= len(self.curriculum_stages):
            return False
        
        stage = self.curriculum_stages[self.current_stage]
        self.stage_performance_history.append(performance)
        
        if len(self.stage_performance_history) >= stage.min_evaluations:
            recent_avg = np.mean(self.stage_performance_history[-3:])
            return recent_avg >= stage.performance_threshold
        return False
    
    def advance_stage(self):
        """è¿›å…¥ä¸‹ä¸€é˜¶æ®µ"""
        if self.current_stage < len(self.curriculum_stages) - 1:
            self.current_stage += 1
            self.stage_performance_history = []
            return True
        return False
    
    def get_current_stage_dataset(self):
        """æ¨¡æ‹Ÿæ•°æ®é›†"""
        sizes = [2000, 3000, 1500]  # å„é˜¶æ®µæ•°æ®é›†å¤§å°
        return list(range(sizes[min(self.current_stage, len(sizes)-1)]))

def test_performance_calculation():
    """æµ‹è¯•æ€§èƒ½è®¡ç®—åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•1: æ€§èƒ½è®¡ç®—åŠŸèƒ½")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        callback = CurriculumProgressCallback(None, None, temp_dir)
        
        # æµ‹è¯•ä¸åŒç±»å‹çš„æ—¥å¿—
        test_cases = [
            ({'eval_avg_test_pass_rate': 0.8}, 0.8, "ç›´æ¥è¯„ä¼°æŒ‡æ ‡"),
            ({'reward': 5.0}, None, "rewardæŒ‡æ ‡è½¬æ¢"),  # åŠ¨æ€è®¡ç®—
            ({'loss': 0.1}, 0.9, "lossæŒ‡æ ‡è½¬æ¢"),
            ({'train_loss': 0.2}, 0.8, "train_lossæŒ‡æ ‡è½¬æ¢"),
            ({}, 0.0, "æ— æŒ‡æ ‡æƒ…å†µ"),
            (None, 0.0, "Noneè¾“å…¥")
        ]
        
        for logs, expected, description in test_cases:
            performance = callback._calculate_performance_from_logs(logs)
            if expected is not None:
                status = "âœ…" if abs(performance - expected) < 0.1 else "âŒ"
            else:
                status = "ğŸ”"  # åŠ¨æ€è®¡ç®—ï¼Œä»…æ£€æŸ¥åˆç†æ€§
            print(f"  {status} {description}: {logs} -> {performance:.4f}")
    
    print()

def test_stage_advancement():
    """æµ‹è¯•é˜¶æ®µå‡çº§åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•2: é˜¶æ®µå‡çº§åŠŸèƒ½")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        mock_manager = MockCurriculumManager()
        callback = CurriculumProgressCallback(mock_manager, None, temp_dir)
        
        mock_args = MockTrainingArguments()
        mock_state = MockTrainerState()
        mock_control = MockTrainerControl()
        
        print(f"  åˆå§‹é˜¶æ®µ: {mock_manager.current_stage}")
        
        # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
        for step in range(1, 11):
            mock_state.global_step = step * 25
            
            # æ¨¡æ‹Ÿæ€§èƒ½é€æ¸æå‡
            performance = 0.5 + (step * 0.03)  # 0.5 -> 0.8
            
            # æ·»åŠ åˆ°æ—¥å¿—å†å²
            log_entry = {
                'reward': performance * 5,  # è½¬æ¢ä¸ºrewardæ ¼å¼
                'loss': 1.0 - performance,
                'learning_rate': 1e-6
            }
            mock_state.log_history.append(log_entry)
            
            print(f"  æ­¥æ•° {mock_state.global_step:3d}: æ€§èƒ½={performance:.3f}, é˜¶æ®µ={mock_manager.current_stage}")
            
            # è°ƒç”¨å›è°ƒå‡½æ•°
            callback.on_log(mock_args, mock_state, mock_control, log_entry)
            
            # æ¨¡æ‹Ÿè¯„ä¼°
            if step % 3 == 0:
                callback.on_evaluate(mock_args, mock_state, mock_control)
        
        print(f"  æœ€ç»ˆé˜¶æ®µ: {mock_manager.current_stage}")
        print(f"  æ€§èƒ½å†å²é•¿åº¦: {len(callback.performance_history)}")
    
    print()

def test_wandb_logging():
    """æµ‹è¯•W&Bæ—¥å¿—è®°å½•"""
    print("ğŸ§ª æµ‹è¯•3: W&Bæ—¥å¿—åŠŸèƒ½")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        mock_manager = MockCurriculumManager()
        callback = CurriculumProgressCallback(mock_manager, None, temp_dir)
        
        # æ¨¡æ‹Ÿä¸€äº›æ€§èƒ½å†å²
        callback.performance_history = [
            {'step': 25, 'performance': 0.6, 'stage': 0, 'timestamp': datetime.now().isoformat()},
            {'step': 50, 'performance': 0.7, 'stage': 0, 'timestamp': datetime.now().isoformat()},
            {'step': 75, 'performance': 0.8, 'stage': 1, 'timestamp': datetime.now().isoformat()},
        ]
        
        test_logs = {
            'loss': 0.15,
            'reward': 3.5,
            'learning_rate': 1.5e-6
        }
        
        try:
            # å°è¯•W&Bè®°å½•ï¼ˆå³ä½¿æ²¡æœ‰wandbä¹Ÿåº”è¯¥æ­£å¸¸å·¥ä½œï¼‰
            callback._wandb_log(100, test_logs)
            print("  âœ… W&Bè®°å½•åŠŸèƒ½æ­£å¸¸ï¼ˆæ— å¼‚å¸¸æŠ›å‡ºï¼‰")
        except Exception as e:
            print(f"  âŒ W&Bè®°å½•å¼‚å¸¸: {e}")
    
    print()

def test_debug_logging():
    """æµ‹è¯•è°ƒè¯•æ—¥å¿—åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•4: è°ƒè¯•æ—¥å¿—åŠŸèƒ½")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        callback = CurriculumProgressCallback(None, None, temp_dir)
        
        # å†™å…¥ä¸€äº›è°ƒè¯•ä¿¡æ¯
        callback._write_debug("æµ‹è¯•è°ƒè¯•ä¿¡æ¯ 1")
        callback._write_debug("æµ‹è¯•è°ƒè¯•ä¿¡æ¯ 2")
        callback._write_debug("æµ‹è¯•è°ƒè¯•ä¿¡æ¯ 3")
        
        # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        log_file = callback.debug_log_path
        if os.path.exists(log_file):
            with open(log_file, 'r', encoding='utf-8') as f:
                content = f.read()
                if "æµ‹è¯•è°ƒè¯•ä¿¡æ¯" in content:
                    print("  âœ… è°ƒè¯•æ—¥å¿—å†™å…¥æˆåŠŸ")
                    print(f"  ğŸ“„ æ—¥å¿—æ–‡ä»¶: {log_file}")
                    print(f"  ğŸ“ å†…å®¹è¡Œæ•°: {len(content.splitlines())}")
                else:
                    print("  âŒ è°ƒè¯•æ—¥å¿—å†…å®¹ä¸åŒ¹é…")
        else:
            print("  âŒ è°ƒè¯•æ—¥å¿—æ–‡ä»¶æœªåˆ›å»º")
    
    print()

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ”§ è¯¾ç¨‹å­¦ä¹ å›è°ƒä¿®å¤æµ‹è¯•")
    print("=" * 50)
    
    test_performance_calculation()
    test_stage_advancement()
    test_wandb_logging()
    test_debug_logging()
    
    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ“‹ æµ‹è¯•æ€»ç»“:")
    print("- æ€§èƒ½è®¡ç®—åŠŸèƒ½å·²ä¿®å¤ï¼Œæ”¯æŒ reward/loss/eval æŒ‡æ ‡")
    print("- é˜¶æ®µå‡çº§é€»è¾‘å·²å®Œå–„ï¼Œæ­£ç¡®å¤„ç† stage å­—æ®µ")
    print("- W&B æ—¥å¿—è®°å½•å¢å¼ºï¼ŒåŒ…å«æ›´å¤šè°ƒè¯•ä¿¡æ¯")
    print("- è°ƒè¯•æ—¥å¿—ç³»ç»Ÿå·¥ä½œæ­£å¸¸")

if __name__ == "__main__":
    main() 