#!/usr/bin/env python3
"""
æ”¹è¿›çš„GRPOè®­ç»ƒè„šæœ¬
è§£å†³æ–­ç»­è®­ç»ƒæ—¶çš„æ­¥æ•°åŒæ­¥å’Œæµ‹è¯•æ•°æ®ç”Ÿæˆé—®é¢˜
"""

import os
import sys
import logging
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥GRPOç»„ä»¶
from grpo_project.core.wandb_sync_manager import initialize_wandb_sync_manager, get_wandb_sync_manager
from grpo_project.callbacks.enhanced_inference_callback import EnhancedInferenceCallback
from grpo_project.curriculum.manager import setup_fixed_curriculum_manager

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def setup_improved_training():
    """
    è®¾ç½®æ”¹è¿›çš„è®­ç»ƒé…ç½®
    """
    parser = argparse.ArgumentParser(description="æ”¹è¿›çš„GRPOè®­ç»ƒè„šæœ¬")
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument("--output_dir", type=str, default="./output", 
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--model_name", type=str, default="deepseek-ai/deepseek-coder-1.3b-instruct",
                       help="æ¨¡å‹åç§°")
    parser.add_argument("--dataset_path", type=str, default="./data/train_dataset.json",
                       help="æ•°æ®é›†è·¯å¾„")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--num_train_epochs", type=int, default=3,
                       help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--per_device_train_batch_size", type=int, default=1,
                       help="æ¯è®¾å¤‡è®­ç»ƒæ‰¹æ¬¡å¤§å°")
    parser.add_argument("--learning_rate", type=float, default=5e-5,
                       help="å­¦ä¹ ç‡")
    parser.add_argument("--save_steps", type=int, default=50,
                       help="ä¿å­˜é—´éš”æ­¥æ•°")
    
    # è¯„ä¼°å‚æ•°
    parser.add_argument("--eval_every_n_steps", type=int, default=25,
                       help="è¯„ä¼°é—´éš”æ­¥æ•°")
    parser.add_argument("--max_eval_samples", type=int, default=8,
                       help="æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°")
    
    # æ¢å¤è®­ç»ƒå‚æ•°
    parser.add_argument("--resume_from_checkpoint", type=str, default=None,
                       help="ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ")
    parser.add_argument("--force_new_wandb_run", action="store_true",
                       help="å¼ºåˆ¶åˆ›å»ºæ–°çš„WandB run")
    
    # WandBå‚æ•°
    parser.add_argument("--wandb_project", type=str, default="grpo-training-improved",
                       help="WandBé¡¹ç›®åç§°")
    parser.add_argument("--wandb_run_name", type=str, default=None,
                       help="WandBè¿è¡Œåç§°")
    
    # è¯¾ç¨‹å­¦ä¹ å‚æ•°
    parser.add_argument("--enable_curriculum", action="store_true", default=True,
                       help="å¯ç”¨è¯¾ç¨‹å­¦ä¹ ")
    parser.add_argument("--curriculum_config", type=str, default=None,
                       help="è¯¾ç¨‹å­¦ä¹ é…ç½®æ–‡ä»¶")
    
    return parser.parse_args()

def create_training_config(args):
    """
    åˆ›å»ºè®­ç»ƒé…ç½®
    """
    from transformers import TrainingArguments
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯æ–­ç»­è®­ç»ƒ
    is_resuming = args.resume_from_checkpoint and os.path.exists(args.resume_from_checkpoint)
    
    training_args = TrainingArguments(
        output_dir=str(output_dir),
        num_train_epochs=args.num_train_epochs,
        per_device_train_batch_size=args.per_device_train_batch_size,
        learning_rate=args.learning_rate,
        save_steps=args.save_steps,
        logging_steps=10,
        evaluation_strategy="steps" if not is_resuming else "no",  # æ–­ç»­è®­ç»ƒæ—¶æš‚æ—¶ç¦ç”¨å†…ç½®è¯„ä¼°
        eval_steps=args.eval_every_n_steps if not is_resuming else None,
        save_total_limit=3,
        load_best_model_at_end=False,  # é¿å…ä¸è¯¾ç¨‹å­¦ä¹ å†²çª
        metric_for_best_model="eval_avg_test_pass_rate",
        greater_is_better=True,
        dataloader_num_workers=2,
        remove_unused_columns=False,
        # æ–­ç»­è®­ç»ƒç›¸å…³
        resume_from_checkpoint=args.resume_from_checkpoint if is_resuming else None,
        # æ—¥å¿—ç›¸å…³
        report_to=["wandb"] if not args.force_new_wandb_run else [],  # æ§åˆ¶WandBåˆå§‹åŒ–
        run_name=args.wandb_run_name,
    )
    
    return training_args, is_resuming

def setup_wandb_integration(args, is_resuming: bool):
    """
    è®¾ç½®WandBé›†æˆ
    """
    logger.info("ğŸ”„ è®¾ç½®WandBé›†æˆ...")
    
    # åˆå§‹åŒ–WandBåŒæ­¥ç®¡ç†å™¨
    sync_manager = initialize_wandb_sync_manager(
        output_dir=args.output_dir,
        project_name=args.wandb_project,
        run_name=args.wandb_run_name or f"grpo_run_{Path(args.output_dir).name}"
    )
    
    # è®¾ç½®WandBè¿è¡Œ
    config = {
        "model_name": args.model_name,
        "learning_rate": args.learning_rate,
        "batch_size": args.per_device_train_batch_size,
        "eval_every_n_steps": args.eval_every_n_steps,
        "max_eval_samples": args.max_eval_samples,
        "enable_curriculum": args.enable_curriculum,
        "is_resuming": is_resuming,
    }
    
    success = sync_manager.setup_wandb_run(
        resume_from_checkpoint=args.resume_from_checkpoint if is_resuming else None,
        config=config
    )
    
    if not success:
        logger.warning("âš ï¸ WandBåˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨æœ¬åœ°æ—¥å¿—")
    
    return sync_manager

def setup_enhanced_callbacks(args):
    """
    è®¾ç½®å¢å¼ºçš„å›è°ƒ
    """
    logger.info("ğŸ“ è®¾ç½®å¢å¼ºå›è°ƒ...")
    
    callbacks = []
    
    # 1. å¢å¼ºæ¨ç†å›è°ƒ
    inference_callback = EnhancedInferenceCallback(
        eval_every_n_steps=args.eval_every_n_steps,
        max_samples=args.max_eval_samples
    )
    callbacks.append(inference_callback)
    
    # 2. å…¶ä»–å¿…è¦å›è°ƒå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 
    # ä¾‹å¦‚ï¼šè¯¾ç¨‹å­¦ä¹ å›è°ƒã€ä¿å­˜å›è°ƒç­‰
    
    logger.info(f"âœ… è®¾ç½®å®Œæˆï¼Œå…± {len(callbacks)} ä¸ªå›è°ƒ")
    return callbacks

def load_dataset(args):
    """
    åŠ è½½æ•°æ®é›†
    """
    logger.info(f"ğŸ“Š åŠ è½½æ•°æ®é›†: {args.dataset_path}")
    
    try:
        # è¿™é‡Œåº”è¯¥åŠ è½½å®é™…çš„æ•°æ®é›†
        # æš‚æ—¶è¿”å›ç©ºï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦å®ç°
        logger.warning("âš ï¸ æ•°æ®é›†åŠ è½½åŠŸèƒ½éœ€è¦å®ç°")
        return None, None  # train_dataset, eval_dataset
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        return None, None

def setup_curriculum_learning(args, dataset):
    """
    è®¾ç½®è¯¾ç¨‹å­¦ä¹ 
    """
    if not args.enable_curriculum:
        logger.info("ğŸ“š è¯¾ç¨‹å­¦ä¹ å·²ç¦ç”¨")
        return None
    
    logger.info("ğŸ“š è®¾ç½®è¯¾ç¨‹å­¦ä¹ ...")
    
    try:
        # è¿™é‡Œåº”è¯¥è®¾ç½®å®é™…çš„è¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨
        # curriculum_manager = setup_fixed_curriculum_manager(script_cfg, dataset)
        logger.warning("âš ï¸ è¯¾ç¨‹å­¦ä¹ è®¾ç½®åŠŸèƒ½éœ€è¦å®ç°")
        return None
        
    except Exception as e:
        logger.error(f"âŒ è¯¾ç¨‹å­¦ä¹ è®¾ç½®å¤±è´¥: {e}")
        return None

def create_trainer(args, training_args, model, train_dataset, eval_dataset, callbacks):
    """
    åˆ›å»ºè®­ç»ƒå™¨
    """
    logger.info("ğŸš€ åˆ›å»ºè®­ç»ƒå™¨...")
    
    try:
        # è¿™é‡Œåº”è¯¥åˆ›å»ºå®é™…çš„GRPOè®­ç»ƒå™¨
        # æš‚æ—¶è¿”å›Noneï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦å®ç°
        logger.warning("âš ï¸ è®­ç»ƒå™¨åˆ›å»ºåŠŸèƒ½éœ€è¦å®ç°")
        return None
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå™¨åˆ›å»ºå¤±è´¥: {e}")
        return None

def main():
    """
    ä¸»å‡½æ•°
    """
    logger.info("ğŸš€ å¯åŠ¨æ”¹è¿›çš„GRPOè®­ç»ƒ")
    
    # 1. è§£æå‚æ•°
    args = setup_improved_training()
    logger.info(f"ğŸ“‹ è®­ç»ƒå‚æ•°: {vars(args)}")
    
    # 2. åˆ›å»ºè®­ç»ƒé…ç½®
    training_args, is_resuming = create_training_config(args)
    logger.info(f"âš™ï¸ è®­ç»ƒé…ç½®å®Œæˆ, æ–­ç»­è®­ç»ƒ: {is_resuming}")
    
    # 3. è®¾ç½®WandBé›†æˆ
    sync_manager = setup_wandb_integration(args, is_resuming)
    
    # 4. åŠ è½½æ•°æ®é›†
    train_dataset, eval_dataset = load_dataset(args)
    
    # 5. è®¾ç½®è¯¾ç¨‹å­¦ä¹ 
    curriculum_manager = setup_curriculum_learning(args, train_dataset)
    
    # 6. è®¾ç½®å›è°ƒ
    callbacks = setup_enhanced_callbacks(args)
    
    # 7. åŠ è½½æ¨¡å‹ï¼ˆéœ€è¦å®ç°ï¼‰
    model = None  # å®é™…ä½¿ç”¨æ—¶éœ€è¦åŠ è½½æ¨¡å‹
    
    # 8. åˆ›å»ºè®­ç»ƒå™¨
    trainer = create_trainer(args, training_args, model, train_dataset, eval_dataset, callbacks)
    
    if trainer is None:
        logger.error("âŒ è®­ç»ƒå™¨åˆ›å»ºå¤±è´¥ï¼Œæ— æ³•ç»§ç»­è®­ç»ƒ")
        return
    
    # 9. å¼€å§‹è®­ç»ƒ
    logger.info("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
    try:
        # trainer.train()
        logger.info("âš ï¸ è®­ç»ƒé€»è¾‘éœ€è¦å®ç°")
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        return
    
    # 10. è®­ç»ƒå®Œæˆ
    logger.info("âœ… è®­ç»ƒå®Œæˆ")
    
    # 11. ä¿å­˜æœ€ç»ˆçŠ¶æ€
    if sync_manager:
        status = sync_manager.get_sync_status()
        logger.info(f"ğŸ“Š æœ€ç»ˆåŒæ­¥çŠ¶æ€: {status}")

def create_training_script_template():
    """
    åˆ›å»ºè®­ç»ƒè„šæœ¬æ¨¡æ¿
    """
    script_content = '''#!/bin/bash
# æ”¹è¿›çš„GRPOè®­ç»ƒè„šæœ¬

# è®¾ç½®ç¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES=0
export WANDB_PROJECT="grpo-training-improved"

# åŸºæœ¬å‚æ•°
OUTPUT_DIR="./enhanced_grpo_output"
MODEL_NAME="deepseek-ai/deepseek-coder-1.3b-instruct"
DATASET_PATH="./data/train_dataset.json"

# è®­ç»ƒå‚æ•°
NUM_EPOCHS=3
BATCH_SIZE=1
LEARNING_RATE=5e-5
SAVE_STEPS=50

# è¯„ä¼°å‚æ•°
EVAL_EVERY_N_STEPS=25
MAX_EVAL_SAMPLES=8

# æ¢å¤è®­ç»ƒå‚æ•°ï¼ˆå¦‚æœéœ€è¦ï¼‰
# RESUME_FROM_CHECKPOINT="./enhanced_grpo_output/checkpoint-xxx"

# è¿è¡Œè®­ç»ƒ
python improved_training_script.py \\
    --output_dir "$OUTPUT_DIR" \\
    --model_name "$MODEL_NAME" \\
    --dataset_path "$DATASET_PATH" \\
    --num_train_epochs "$NUM_EPOCHS" \\
    --per_device_train_batch_size "$BATCH_SIZE" \\
    --learning_rate "$LEARNING_RATE" \\
    --save_steps "$SAVE_STEPS" \\
    --eval_every_n_steps "$EVAL_EVERY_N_STEPS" \\
    --max_eval_samples "$MAX_EVAL_SAMPLES" \\
    --enable_curriculum \\
    --wandb_project "$WANDB_PROJECT" \\
    --wandb_run_name "enhanced_grpo_$(date +%Y%m%d_%H%M%S)"
    # --resume_from_checkpoint "$RESUME_FROM_CHECKPOINT"  # æ–­ç»­è®­ç»ƒæ—¶å–æ¶ˆæ³¨é‡Š

echo "âœ… è®­ç»ƒè„šæœ¬æ‰§è¡Œå®Œæˆ"
'''
    
    with open("run_improved_training.sh", "w") as f:
        f.write(script_content)
    
    # è®¾ç½®æ‰§è¡Œæƒé™
    os.chmod("run_improved_training.sh", 0o755)
    
    logger.info("ğŸ“ å·²åˆ›å»ºæ”¹è¿›çš„è®­ç»ƒè„šæœ¬: run_improved_training.sh")

if __name__ == "__main__":
    # åˆ›å»ºè®­ç»ƒè„šæœ¬æ¨¡æ¿
    create_training_script_template()
    
    # è¿è¡Œä¸»ç¨‹åº
    main() 