# enhanced_curriculum.py - åŒå±‚è¯¾ç¨‹å­¦ä¹ ç³»ç»Ÿ
import numpy as np
import logging
from typing import List, Dict, Any, Tuple, Optional
from datasets import Dataset
from dataclasses import dataclass
import wandb

logger = logging.getLogger(__name__)

@dataclass
class CurriculumStageConfig:
    """è¯¾ç¨‹å­¦ä¹ é˜¶æ®µé…ç½®"""
    name: str
    dataset_levels: List[str]           # æ•°æ®é›†ç­‰çº§è¿‡æ»¤ ['basic', 'intermediate', 'advanced', 'expert']
    complexity_range: Tuple[float, float]  # å¤æ‚åº¦èŒƒå›´ (min, max)
    epochs_ratio: float                 # è¯¥é˜¶æ®µè®­ç»ƒepochæ¯”ä¾‹
    performance_threshold: float = 0.6  # è¿›å…¥ä¸‹ä¸€é˜¶æ®µçš„æ€§èƒ½é˜ˆå€¼
    min_evaluations: int = 5           # æœ€å°‘è¯„ä¼°æ¬¡æ•°
    description: str = ""              # é˜¶æ®µæè¿°

class EnhancedCurriculumManager:
    """å¢å¼ºçš„åŒå±‚è¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨"""
    
    def __init__(self, curriculum_stages: List[CurriculumStageConfig], dataset: Dataset):
        # ğŸ”§ ä¼˜åŒ–è¿›é˜¶æ¡ä»¶ - é’ˆå¯¹ä¸åŒé˜¶æ®µä½¿ç”¨ä¸åŒçš„æ ‡å‡†
        self.stage_progression_configs = {
            0: {  # foundationé˜¶æ®µ
                "performance_threshold": 0.7,
                "min_evaluations": 8,
                "stability_window": 4
            },
            1: {  # elementaryé˜¶æ®µ  
                "performance_threshold": 0.65,
                "min_evaluations": 10,
                "stability_window": 5
            },
            2: {  # intermediateé˜¶æ®µ
                "performance_threshold": 0.6,
                "min_evaluations": 15,  # å¢åŠ æœ€å°è¯„ä¼°æ¬¡æ•°
                "stability_window": 6
            },
            3: {  # advancedé˜¶æ®µ - æœ€ä¸¥æ ¼çš„æ¡ä»¶
                "performance_threshold": 0.55,
                "min_evaluations": 20,  # å¤§å¹…å¢åŠ 
                "stability_window": 8,
                "max_stay_steps": 200   # æœ€å¤§åœç•™æ­¥æ•°
            }
        }
        self.curriculum_stages = curriculum_stages
        self.full_dataset = dataset
        self.current_stage = 0
        self.stage_performance_history = []
        self.stage_statistics = []
        
        # åˆ†ææ•°æ®é›†åˆ†å¸ƒ
        self._analyze_dataset_distribution()
        
        # éªŒè¯è¯¾ç¨‹è®¾è®¡
        self._validate_curriculum_design()
        
        logger.info(f"Enhanced Curriculum Manager initialized with {len(curriculum_stages)} stages")
        for i, stage in enumerate(curriculum_stages):
            logger.info(f"  Stage {i}: {stage.name} - Levels: {stage.dataset_levels}, "
                       f"Complexity: {stage.complexity_range}, Ratio: {stage.epochs_ratio}")
    def should_advance_to_next_stage(self, current_loss, training_step):
        """å¢å¼ºçš„è¿›é˜¶åˆ¤æ–­é€»è¾‘ï¼Œé’ˆå¯¹é«˜éš¾åº¦é˜¶æ®µä¼˜åŒ–"""
        current_stage = self.current_stage
        
        # å¦‚æœå·²ç»æ˜¯æœ€åé˜¶æ®µï¼Œä¸å†è¿›é˜¶
        if current_stage >= len(self.curriculum_stages) - 1:
            logger.debug(f"å·²å¤„äºæœ€åé˜¶æ®µ ({current_stage})ï¼Œä¸å†è¿›é˜¶")
            return False
        
        # ğŸ”§ æ–°å¢ï¼šåˆå§‹åŒ–é˜¶æ®µå¼€å§‹æ­¥æ•°è·Ÿè¸ª
        if not hasattr(self, 'stage_start_steps'):
            self.stage_start_steps = {}
        
        if current_stage not in self.stage_start_steps:
            self.stage_start_steps[current_stage] = training_step
            logger.info(f"ğŸ¯ å¼€å§‹è·Ÿè¸ªé˜¶æ®µ{current_stage}ï¼Œèµ·å§‹æ­¥æ•°: {training_step}")
        
        stage_start_step = self.stage_start_steps[current_stage]
        steps_in_current_stage = training_step - stage_start_step
        
        # ğŸ”§ æ–°å¢ï¼šä¸åŒé˜¶æ®µçš„æœ€å°åœç•™æ—¶é—´è¦æ±‚
        min_steps_by_stage = {
            0: 15,   # foundation - è‡³å°‘15æ­¥
            1: 20,   # elementary - è‡³å°‘20æ­¥  
            2: 30,   # intermediate - è‡³å°‘30æ­¥
            3: 50    # advanced - è‡³å°‘50æ­¥
        }
        
        min_steps_required = min_steps_by_stage.get(current_stage, 20)
        
        if steps_in_current_stage < min_steps_required:
            logger.info(f"â±ï¸ é˜¶æ®µ{current_stage}éœ€è¦æ›´å¤šè®­ç»ƒ: {steps_in_current_stage}/{min_steps_required} æ­¥")
            return False
        
        # è·å–é˜¶æ®µé…ç½®
        config = self.stage_progression_configs.get(current_stage, {})
        
        # è·å–é…ç½®å‚æ•°
        performance_threshold = config.get("performance_threshold", 0.6)
        min_evaluations = config.get("min_evaluations", 10)
        stability_window = config.get("stability_window", 5)
        max_stay_steps = config.get("max_stay_steps", None)
        
        # è®¡ç®—å½“å‰æ€§èƒ½
        performance_estimate = 1.0 - min(current_loss, 1.0)
        
        # æ·»åŠ åˆ°æ€§èƒ½å†å²
        self.stage_performance_history.append({
            'step': training_step,
            'performance': performance_estimate,
            'loss': current_loss,
            'stage': current_stage,
            'steps_in_stage': steps_in_current_stage  # æ–°å¢ï¼šè®°å½•åœ¨å½“å‰é˜¶æ®µçš„æ­¥æ•°
        })
        
        # åªä¿ç•™å½“å‰é˜¶æ®µçš„æ€§èƒ½å†å²
        current_stage_history = [
            h for h in self.stage_performance_history 
            if h['stage'] == current_stage
        ]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„è¯„ä¼°æ¬¡æ•°
        if len(current_stage_history) < min_evaluations:
            logger.debug(f"é˜¶æ®µ{current_stage}: è¯„ä¼°æ¬¡æ•°ä¸è¶³ ({len(current_stage_history)}/{min_evaluations})")
            return False
        
        # æ£€æŸ¥æœ€è¿‘çš„æ€§èƒ½ç¨³å®šæ€§
        recent_performance = [h['performance'] for h in current_stage_history[-stability_window:]]
        
        if len(recent_performance) < stability_window:
            logger.debug(f"é˜¶æ®µ{current_stage}: ç¨³å®šæ€§çª—å£æ•°æ®ä¸è¶³ ({len(recent_performance)}/{stability_window})")
            return False
            
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        avg_performance = np.mean(recent_performance)
        performance_std = np.std(recent_performance)
        
        # ğŸ”§ é«˜éš¾åº¦é˜¶æ®µçš„ç‰¹æ®Šå¤„ç†
        if current_stage == 3:  # advancedé˜¶æ®µ
            logger.info(f"ğŸ”¥ é«˜éš¾åº¦é˜¶æ®µæ£€æŸ¥ - æ­¥æ•°: {training_step}, é˜¶æ®µå†…æ­¥æ•°: {steps_in_current_stage}")
            
            # æ›´ä¸¥æ ¼çš„ç¨³å®šæ€§è¦æ±‚
            if performance_std > 0.05:  # æ ‡å‡†å·®ä¸èƒ½å¤ªå¤§
                logger.info(f"   ğŸ“Š æ€§èƒ½ç¨³å®šæ€§ä¸è¶³: std={performance_std:.4f} (è¦æ±‚â‰¤0.05)")
                return False
            
            # æ›´ä¸¥æ ¼çš„æ€§èƒ½è¦æ±‚
            if avg_performance < 0.85:  # é«˜éš¾åº¦é˜¶æ®µè¦æ±‚æ›´é«˜çš„å¹³å‡æ€§èƒ½
                logger.info(f"   ğŸ“ˆ å¹³å‡æ€§èƒ½ä¸è¶³: {avg_performance:.4f} (è¦æ±‚â‰¥0.85)")
                return False
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§åœç•™æ­¥æ•°
            if max_stay_steps and steps_in_current_stage >= max_stay_steps:
                logger.info(f"   â° è¾¾åˆ°æœ€å¤§åœç•™æ­¥æ•° ({max_stay_steps})ï¼Œå¼ºåˆ¶ä¿æŒåœ¨é«˜éš¾åº¦é˜¶æ®µ")
                return False  # ä¸è¿›é˜¶ï¼Œç»§ç»­åœ¨é«˜éš¾åº¦é˜¶æ®µè®­ç»ƒ
            
            # é«˜éš¾åº¦é˜¶æ®µè¦æ±‚è¿ç»­ä¼˜ç§€æ€§èƒ½
            if len(recent_performance) >= 5:
                excellent_performance_count = sum(1 for p in recent_performance[-5:] if p >= 0.9)
                if excellent_performance_count < 3:  # æœ€è¿‘5æ¬¡ä¸­è‡³å°‘3æ¬¡ä¼˜ç§€
                    logger.info(f"   ğŸ¯ ä¼˜ç§€æ€§èƒ½æ¬¡æ•°ä¸è¶³: {excellent_performance_count}/5 æ¬¡â‰¥0.9")
                    return False
            
            logger.info(f"   âœ… é«˜éš¾åº¦é˜¶æ®µæ‰€æœ‰æ£€æŸ¥é€šè¿‡")
        
        # ğŸ”§ ä¸­ç­‰éš¾åº¦é˜¶æ®µçš„ç‰¹æ®Šå¤„ç†
        elif current_stage == 2:  # intermediateé˜¶æ®µ
            # ä¸­ç­‰éš¾åº¦é˜¶æ®µéœ€è¦ç¨å¾®ä¸¥æ ¼ä¸€äº›
            if performance_std > 0.08:
                logger.info(f"ä¸­ç­‰éš¾åº¦é˜¶æ®µæ€§èƒ½æ³¢åŠ¨è¿‡å¤§: std={performance_std:.4f}")
                return False
            
            # è¦æ±‚è‡³å°‘æœ‰ä¸€åŠçš„è¯„ä¼°è¾¾åˆ°è‰¯å¥½æ°´å¹³
            good_performance_count = sum(1 for p in recent_performance if p >= 0.75)
            if good_performance_count < len(recent_performance) * 0.6:
                logger.info(f"ä¸­ç­‰éš¾åº¦é˜¶æ®µè‰¯å¥½æ€§èƒ½æ¯”ä¾‹ä¸è¶³: {good_performance_count}/{len(recent_performance)}")
                return False
        
        # ğŸ”§ åŸºç¡€é˜¶æ®µçš„æ¸è¿›å¼è¦æ±‚
        elif current_stage <= 1:  # foundationå’Œelementaryé˜¶æ®µ
            # åŸºç¡€é˜¶æ®µç›¸å¯¹å®½æ¾ï¼Œä½†ä»éœ€è¦åŸºæœ¬çš„ç¨³å®šæ€§
            if performance_std > 0.15:  # å…è®¸æ›´å¤§çš„æ³¢åŠ¨
                logger.info(f"åŸºç¡€é˜¶æ®µæ€§èƒ½æ³¢åŠ¨è¿‡å¤§: std={performance_std:.4f}")
                return False
            
            # åŸºç¡€é˜¶æ®µåªè¦å¤§éƒ¨åˆ†è¯„ä¼°è¾¾åˆ°åŸºæœ¬æ°´å¹³å³å¯
            acceptable_performance_count = sum(1 for p in recent_performance if p >= 0.5)
            if acceptable_performance_count < len(recent_performance) * 0.7:
                logger.info(f"åŸºç¡€é˜¶æ®µå¯æ¥å—æ€§èƒ½æ¯”ä¾‹ä¸è¶³: {acceptable_performance_count}/{len(recent_performance)}")
                return False
        
        # æ ‡å‡†è¿›é˜¶æ¡ä»¶æ£€æŸ¥
        should_advance = avg_performance >= performance_threshold
        
        # ğŸ”§ æ–°å¢ï¼šè¿›é˜¶å†³ç­–çš„è¯¦ç»†æ—¥å¿—
        current_stage_name = self.curriculum_stages[current_stage].name if current_stage < len(self.curriculum_stages) else "Unknown"
        
        logger.info(f"""
        ğŸ“‹ è¿›é˜¶æ£€æŸ¥è¯¦æƒ… - é˜¶æ®µ{current_stage} ({current_stage_name}):
        â”œâ”€ æ—¶é—´ç»Ÿè®¡:
        â”‚  â”œâ”€ å½“å‰æ­¥æ•°: {training_step}
        â”‚  â”œâ”€ é˜¶æ®µèµ·å§‹: {stage_start_step}
        â”‚  â”œâ”€ é˜¶æ®µå†…æ­¥æ•°: {steps_in_current_stage}
        â”‚  â””â”€ æœ€å°è¦æ±‚: {min_steps_required}
        â”œâ”€ æ€§èƒ½åˆ†æ:
        â”‚  â”œâ”€ å½“å‰æ€§èƒ½: {performance_estimate:.4f}
        â”‚  â”œâ”€ å¹³å‡æ€§èƒ½: {avg_performance:.4f} (é˜ˆå€¼: {performance_threshold})
        â”‚  â”œâ”€ æ€§èƒ½ç¨³å®šæ€§: {performance_std:.4f}
        â”‚  â””â”€ æœ€è¿‘è¶‹åŠ¿: {recent_performance[-3:] if len(recent_performance) >= 3 else recent_performance}
        â”œâ”€ è¯„ä¼°ç»Ÿè®¡:
        â”‚  â”œâ”€ è¯„ä¼°æ¬¡æ•°: {len(current_stage_history)}/{min_evaluations}
        â”‚  â”œâ”€ ç¨³å®šæ€§çª—å£: {len(recent_performance)}/{stability_window}
        â”‚  â””â”€ å†å²é•¿åº¦: {len(self.stage_performance_history)}
        â””â”€ è¿›é˜¶å†³ç­–: {'âœ… å¯ä»¥è¿›é˜¶' if should_advance else 'âŒ ç»§ç»­å½“å‰é˜¶æ®µ'}
        """)
        
        # ğŸ”§ æ–°å¢ï¼šå¦‚æœå†³å®šè¿›é˜¶ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
        if should_advance:
            logger.info(f"""
            ğŸš€ é˜¶æ®µ{current_stage}è¿›é˜¶æ¡ä»¶æ»¡è¶³:
            â”œâ”€ è®­ç»ƒå……åˆ†: {steps_in_current_stage} â‰¥ {min_steps_required} æ­¥
            â”œâ”€ æ€§èƒ½è¾¾æ ‡: {avg_performance:.4f} â‰¥ {performance_threshold}
            â”œâ”€ è¡¨ç°ç¨³å®š: std={performance_std:.4f}
            â””â”€ å‡†å¤‡è¿›å…¥é˜¶æ®µ{current_stage + 1}
            """)
        else:
            # åˆ†æä¸ºä»€ä¹ˆä¸èƒ½è¿›é˜¶
            reasons = []
            if steps_in_current_stage < min_steps_required:
                reasons.append(f"è®­ç»ƒæ—¶é—´ä¸è¶³({steps_in_current_stage}/{min_steps_required})")
            if avg_performance < performance_threshold:
                reasons.append(f"æ€§èƒ½æœªè¾¾æ ‡({avg_performance:.4f}<{performance_threshold})")
            if performance_std > (0.05 if current_stage == 3 else 0.1):
                reasons.append(f"æ€§èƒ½ä¸ç¨³å®š(std={performance_std:.4f})")
            
            logger.info(f"â¸ï¸ ç»§ç»­é˜¶æ®µ{current_stage}è®­ç»ƒï¼ŒåŸå› : {', '.join(reasons)}")
        
        return should_advance
    def get_curriculum_state(self) -> Dict[str, Any]:
        """è·å–è¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨çš„å½“å‰çŠ¶æ€ï¼Œç”¨äºä¿å­˜ã€‚"""
        return {
            "current_stage": self.current_stage,  # å‡è®¾ current_stage æ˜¯é˜¶æ®µç´¢å¼•æˆ–å¯åºåˆ—åŒ–æ ‡è¯†
            "stage_performance_history": self.stage_performance_history,
            # æ·»åŠ å…¶ä»–ä»»ä½•éœ€è¦æŒä¹…åŒ–çš„å†…éƒ¨çŠ¶æ€å˜é‡
            # ä¾‹å¦‚ï¼Œå¦‚æœä½ çš„ curriculum_stages åˆ—è¡¨æ˜¯åŠ¨æ€ç”Ÿæˆçš„æˆ–ä¿®æ”¹çš„ï¼Œä¹Ÿå¯èƒ½éœ€è¦ä¿å­˜
        }

    def load_curriculum_state(self, state_dict: Dict[str, Any]):
        """ä»å­—å…¸åŠ è½½è¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨çš„çŠ¶æ€ã€‚"""
        self.current_stage = state_dict.get("current_stage", 0) # ä½¿ç”¨ get æä¾›é»˜è®¤å€¼ä»¥é˜² key ä¸å­˜åœ¨
        self.stage_performance_history = state_dict.get("stage_performance_history", [])
        # åŠ è½½å…¶ä»–å·²ä¿å­˜çš„çŠ¶æ€å˜é‡
        logger.info(f"è¯¾ç¨‹å­¦ä¹ çŠ¶æ€å·²åŠ è½½ã€‚ä»é˜¶æ®µ {self.current_stage} (åç§°: {self.curriculum_stages[self.current_stage].name if self.current_stage < len(self.curriculum_stages) else 'æœªçŸ¥'}) æ¢å¤ã€‚")
    def get_curriculum_state(self) -> Dict[str, Any]:
        """è·å–è¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨çš„å½“å‰çŠ¶æ€ï¼Œç”¨äºä¿å­˜ã€‚"""
        return {
            "current_stage": self.current_stage,  # å‡è®¾ current_stage æ˜¯é˜¶æ®µç´¢å¼•æˆ–å¯åºåˆ—åŒ–æ ‡è¯†
            "stage_performance_history": self.stage_performance_history,
            # æ·»åŠ å…¶ä»–ä»»ä½•éœ€è¦æŒä¹…åŒ–çš„å†…éƒ¨çŠ¶æ€å˜é‡
            # ä¾‹å¦‚ï¼Œå¦‚æœä½ çš„ curriculum_stages åˆ—è¡¨æ˜¯åŠ¨æ€ç”Ÿæˆçš„æˆ–ä¿®æ”¹çš„ï¼Œä¹Ÿå¯èƒ½éœ€è¦ä¿å­˜
        }

    def load_curriculum_state(self, state_dict: Dict[str, Any]):
        """ä»å­—å…¸åŠ è½½è¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨çš„çŠ¶æ€ã€‚"""
        self.current_stage = state_dict.get("current_stage", 0) # ä½¿ç”¨ get æä¾›é»˜è®¤å€¼ä»¥é˜² key ä¸å­˜åœ¨
        self.stage_performance_history = state_dict.get("stage_performance_history", [])
        # åŠ è½½å…¶ä»–å·²ä¿å­˜çš„çŠ¶æ€å˜é‡
        logger.info(f"è¯¾ç¨‹å­¦ä¹ çŠ¶æ€å·²åŠ è½½ã€‚ä»é˜¶æ®µ {self.current_stage} (åç§°: {self.curriculum_stages[self.current_stage].name if self.current_stage < len(self.curriculum_stages) else 'æœªçŸ¥'}) æ¢å¤ã€‚")

    def _analyze_dataset_distribution(self):
        """åˆ†ææ•°æ®é›†çš„ç­‰çº§å’Œå¤æ‚åº¦åˆ†å¸ƒ"""
        if len(self.full_dataset) == 0:
            logger.warning("Empty dataset provided to curriculum manager")
            return
        
        # ç»Ÿè®¡æ•°æ®é›†ç­‰çº§åˆ†å¸ƒ
        level_counts = {}
        complexity_by_level = {}
        
        for example in self.full_dataset:
            level = example.get('level', 'unknown').lower()
            complexity = example.get('complexity_score', 5.0)
            
            # ç»Ÿè®¡ç­‰çº§åˆ†å¸ƒ
            level_counts[level] = level_counts.get(level, 0) + 1
            
            # ç»Ÿè®¡æ¯ä¸ªç­‰çº§çš„å¤æ‚åº¦åˆ†å¸ƒ
            if level not in complexity_by_level:
                complexity_by_level[level] = []
            complexity_by_level[level].append(complexity)
        
        self.dataset_distribution = {
            'level_counts': level_counts,
            'complexity_by_level': complexity_by_level,
            'total_samples': len(self.full_dataset)
        }
        
        # æ‰“å°åˆ†å¸ƒä¿¡æ¯
        logger.info("Dataset Distribution Analysis:")
        logger.info(f"  Total samples: {self.dataset_distribution['total_samples']}")
        for level, count in level_counts.items():
            if level in complexity_by_level and complexity_by_level[level]:
                avg_complexity = np.mean(complexity_by_level[level])
                complexity_range = (np.min(complexity_by_level[level]), np.max(complexity_by_level[level]))
                logger.info(f"  {level.capitalize()}: {count} samples, "
                           f"avg complexity: {avg_complexity:.2f}, range: {complexity_range}")
    
    def _validate_curriculum_design(self):
        """éªŒè¯è¯¾ç¨‹è®¾è®¡çš„åˆç†æ€§"""
        # æ£€æŸ¥æ‰€æœ‰æ•°æ®é›†ç­‰çº§æ˜¯å¦éƒ½è¢«è¦†ç›–
        available_levels = set(self.dataset_distribution['level_counts'].keys())
        covered_levels = set()
        
        for stage in self.curriculum_stages:
            covered_levels.update([level.lower() for level in stage.dataset_levels])
        
        uncovered_levels = available_levels - covered_levels
        if uncovered_levels:
            logger.warning(f"Dataset levels not covered by curriculum: {uncovered_levels}")
        
        # æ£€æŸ¥epochæ¯”ä¾‹æ€»å’Œ
        total_ratio = sum(stage.epochs_ratio for stage in self.curriculum_stages)
        if abs(total_ratio - 1.0) > 0.01:
            logger.warning(f"Curriculum epochs ratios sum to {total_ratio:.3f}, not 1.0")
    
    def get_current_stage_dataset(self) -> Dataset:
        """è·å–å½“å‰é˜¶æ®µçš„æ•°æ®é›†"""
        if self.current_stage >= len(self.curriculum_stages):
            logger.info("Curriculum completed, using full dataset")
            return self.full_dataset
        
        stage = self.curriculum_stages[self.current_stage]
        
        # åŒå±‚è¿‡æ»¤ï¼šæ•°æ®é›†ç­‰çº§ + å¤æ‚åº¦èŒƒå›´
        filtered_indices = []
        level_filter_count = 0
        complexity_filter_count = 0
        
        for i, example in enumerate(self.full_dataset):
            # ç¬¬ä¸€å±‚è¿‡æ»¤ï¼šæ•°æ®é›†ç­‰çº§
            example_level = example.get('level', 'unknown').lower()
            if example_level not in [level.lower() for level in stage.dataset_levels]:
                continue
            level_filter_count += 1
            
            # ç¬¬äºŒå±‚è¿‡æ»¤ï¼šå¤æ‚åº¦èŒƒå›´
            complexity = example.get('complexity_score', 5.0)
            min_complexity, max_complexity = stage.complexity_range
            if not (min_complexity <= complexity <= max_complexity):
                continue
            complexity_filter_count += 1
            
            filtered_indices.append(i)
        
        if not filtered_indices:
            logger.warning(f"No examples found for stage {self.current_stage} ({stage.name}), using full dataset")
            return self.full_dataset
        
        stage_dataset = self.full_dataset.select(filtered_indices)
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        stage_stats = {
            'stage_index': self.current_stage,
            'stage_name': stage.name,
            'total_examples': len(self.full_dataset),
            'level_filtered': level_filter_count,
            'complexity_filtered': complexity_filter_count,
            'final_selected': len(stage_dataset),
            'selection_ratio': len(stage_dataset) / len(self.full_dataset),
            'target_levels': stage.dataset_levels,
            'complexity_range': stage.complexity_range
        }
        self.stage_statistics.append(stage_stats)
        
        logger.info(f"Curriculum Stage {self.current_stage} ({stage.name}):")
        logger.info(f"  Target levels: {stage.dataset_levels}")
        logger.info(f"  Complexity range: {stage.complexity_range}")
        logger.info(f"  Selected examples: {len(stage_dataset)}/{len(self.full_dataset)} ({stage_stats['selection_ratio']:.1%})")
        logger.info(f"  Level filtering: {level_filter_count} examples passed")
        logger.info(f"  Complexity filtering: {complexity_filter_count} examples passed")
        
        return stage_dataset
    
    def should_advance_stage(self, recent_performance: float) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿›å…¥ä¸‹ä¸€é˜¶æ®µ.

        Args:
            recent_performance: The latest performance metric (e.g., avg_test_pass_rate).
        """
        if self.current_stage >= len(self.curriculum_stages) - 1:
            logger.debug(f"Already at the final stage ({self.current_stage}). Cannot advance further.")
            return False
        
        stage = self.curriculum_stages[self.current_stage]
        self.stage_performance_history.append(recent_performance) # Add current performance to history for this stage
        
        logger.debug(f"Stage {self.current_stage} ('{stage.name}'): Received performance {recent_performance:.4f}. "
                    f"History size: {len(self.stage_performance_history)}. Min evals required: {stage.min_evaluations}.")

        # Check if minimum number of evaluations for this stage has been met
        if len(self.stage_performance_history) < stage.min_evaluations:
            logger.info(f"Stage {self.current_stage} ('{stage.name}'): Not enough evaluations yet. "
                        f"Have {len(self.stage_performance_history)}, need {stage.min_evaluations}.")
            return False
        
        # If enough evaluations, consider the average of recent performance scores
        # Using all recorded performances for this stage for the average, or a sliding window if preferred.
        # For simplicity here, let's use all available history for the current stage.
        # A more sophisticated approach might use a decaying average or only the last N evaluations *after* min_evaluations is met.

        # Let's use the performance scores collected *after* min_evaluations were met, or just the most recent ones
        # if that's simpler. The current logic uses a window of the last 3, which is fine.

        # Consider the average of the performance history for this stage (or a recent window of it)
        # The history is reset when a stage advances.
        # Let's use the average of all recorded performances for the current stage
        # if len(self.stage_performance_history) >= stage.min_evaluations:
        # The previous logic of using a recent window seems fine.

        # Let's take the average of the performance metrics collected for this stage,
        # but only those collected *after* meeting the min_evaluations count, or a fixed window.
        # The existing logic takes min(3, len(history)) which means if min_evaluations is 5, it will
        # average the last 3 of those 5 (or more). This seems reasonable.

        recent_window_size = min(len(self.stage_performance_history), max(3, stage.min_evaluations)) # Ensure window is at least min_evals if history is long enough, or all history if shorter
        # Or, more simply, average all performances recorded for this stage so far, if count >= min_evaluations
        # Let's stick to a simpler interpretation: average of the last `stage.min_evaluations` (or all if fewer than that many *additional* evals)
        
        # The previous logic: "recent_window = min(3, len(self.stage_performance_history))"
        # This means it only looks at the last 3 evaluations *once min_evaluations condition is met*.
        # This is a common way to do it to ensure sustained performance.

        # Let's refine: average performance of the window that satisfies min_evaluations.
        # If min_evaluations is 10, we should average at least 10 evaluations.
        # The current history already includes the `recent_performance`.

        # We need to ensure we are looking at a stable performance, so averaging the last few
        # (e.g. 3, or up to `min_evaluations`) makes sense.

        # Let's use the average of the last `stage.min_evaluations` scores if available,
        # otherwise all scores if fewer than `stage.min_evaluations` have been recorded (but this case is handled by the check above).
        # If more than `stage.min_evaluations` are present, average the most recent `stage.min_evaluations` ones.
        num_scores_to_average = stage.min_evaluations

        # Ensure we only average available scores if history is shorter than num_scores_to_average (but longer than initial check)
        # This part is actually covered by `len(self.stage_performance_history) < stage.min_evaluations` check.
        # So, if we are here, len(self.stage_performance_history) >= stage.min_evaluations.

        # Average the most recent 'num_scores_to_average' performance scores.
        relevant_performances = self.stage_performance_history[-num_scores_to_average:]
        current_average_performance = np.mean(relevant_performances)

        logger.info(f"Stage {self.current_stage} ('{stage.name}'): Avg performance over last {len(relevant_performances)} evals: {current_average_performance:.4f}. "
                    f"Threshold: {stage.performance_threshold:.4f}.")

        should_advance = current_average_performance >= stage.performance_threshold
        
        if should_advance:
            logger.info(f"Stage {self.current_stage} ('{stage.name}') performance criteria MET. Advancing.")
        else:
            logger.info(f"Stage {self.current_stage} ('{stage.name}') performance criteria NOT YET MET.")

        return should_advance

    def advance_stage(self) -> bool:
        """è¿›å…¥ä¸‹ä¸€é˜¶æ®µ"""
        if self.current_stage < len(self.curriculum_stages) - 1:
            # è®°å½•å½“å‰é˜¶æ®µçš„æœ€ç»ˆç»Ÿè®¡
            final_stats = {
                'completed_stage': self.current_stage,
                'stage_name': self.curriculum_stages[self.current_stage].name,
                'total_evaluations': len(self.stage_performance_history),
                'final_performance': self.stage_performance_history[-1] if self.stage_performance_history else 0,
                'average_performance': np.mean(self.stage_performance_history) if self.stage_performance_history else 0,
                'performance_history': self.stage_performance_history.copy()
            }
            
            self.current_stage += 1
            self.stage_performance_history = []  # é‡ç½®æ€§èƒ½å†å²
            
            new_stage = self.curriculum_stages[self.current_stage]
            logger.info(f"ğŸ¯ Advanced to curriculum stage {self.current_stage}: {new_stage.name}")
            logger.info(f"   Previous stage stats: {final_stats['total_evaluations']} evaluations, "
                       f"final performance: {final_stats['final_performance']:.3f}")
            logger.info(f"   New stage targets: levels {new_stage.dataset_levels}, "
                       f"complexity {new_stage.complexity_range}")
            
            return True
        
        logger.info("ğŸ† Curriculum learning completed! Using full dataset.")
        return False
    
    def get_current_stage_info(self) -> Dict[str, Any]:
        """è·å–å½“å‰é˜¶æ®µä¿¡æ¯"""
        if self.current_stage >= len(self.curriculum_stages):
            return {
                'stage_index': self.current_stage,
                'stage_name': 'completed',
                'dataset_levels': 'all',
                'complexity_range': 'all',
                'is_completed': True
            }
        
        stage = self.curriculum_stages[self.current_stage]
        return {
            'stage_index': self.current_stage,
            'stage_name': stage.name,
            'dataset_levels': stage.dataset_levels,
            'complexity_range': stage.complexity_range,
            'epochs_ratio': stage.epochs_ratio,
            'performance_threshold': stage.performance_threshold,
            'current_evaluations': len(self.stage_performance_history),
            'min_evaluations': stage.min_evaluations,
            'is_completed': False
        }
    
    def get_curriculum_progress(self) -> Dict[str, Any]:
        """è·å–æ•´ä½“è¯¾ç¨‹è¿›åº¦"""
        total_stages = len(self.curriculum_stages)
        progress_ratio = (self.current_stage + 1) / total_stages if total_stages > 0 else 1.0
        
        return {
            'current_stage': self.current_stage,
            'total_stages': total_stages,
            'progress_ratio': progress_ratio,
            'completed_stages': self.current_stage,
            'stage_statistics': self.stage_statistics,
            'dataset_distribution': self.dataset_distribution
        }
    
    def log_to_wandb(self, step: int):
        """è®°å½•åˆ°W&B"""
        if not hasattr(wandb, 'run') or wandb.run is None:
            return
        
        current_info = self.get_current_stage_info()
        progress_info = self.get_curriculum_progress()
        
        # åŸºç¡€ä¿¡æ¯
        wandb.log({
            'curriculum/current_stage': current_info['stage_index'],
            'curriculum/stage_name': current_info['stage_name'],
            'curriculum/progress_ratio': progress_info['progress_ratio'],
            'curriculum/completed_stages': progress_info['completed_stages'],
            'curriculum/is_completed': current_info['is_completed']
        }, step=step)
        
        # å½“å‰é˜¶æ®µè¯¦ç»†ä¿¡æ¯
        if not current_info['is_completed']:
            wandb.log({
                'curriculum/current_evaluations': current_info['current_evaluations'],
                'curriculum/min_evaluations': current_info['min_evaluations'],
                'curriculum/performance_threshold': current_info['performance_threshold'],
                'curriculum/dataset_levels': str(current_info['dataset_levels']),
                'curriculum/complexity_range': str(current_info['complexity_range'])
            }, step=step)
        
        # æ€§èƒ½å†å²
        if self.stage_performance_history:
            wandb.log({
                'curriculum/stage_performance_mean': np.mean(self.stage_performance_history),
                'curriculum/stage_performance_latest': self.stage_performance_history[-1],
                'curriculum/stage_performance_trend': np.mean(self.stage_performance_history[-3:]) if len(self.stage_performance_history) >= 3 else self.stage_performance_history[-1]
            }, step=step)


def create_default_curriculum_stages() -> List[CurriculumStageConfig]:
    """åˆ›å»ºé»˜è®¤çš„åŒå±‚è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ"""
    stages = [
        CurriculumStageConfig(
            name="foundation",
            dataset_levels=["basic"],
            complexity_range=(0.0, 3.0),
            epochs_ratio=0.25,
            performance_threshold=0.7,
            min_evaluations=10, # Changed
            description="åŸºç¡€é˜¶æ®µï¼šå­¦ä¹ ç®€å•çš„åŸºç¡€çº§è®¾è®¡"
        ),
        CurriculumStageConfig(
            name="elementary",
            dataset_levels=["basic", "intermediate"],
            complexity_range=(0.0, 5.0),
            epochs_ratio=0.25,
            performance_threshold=0.65,
            min_evaluations=10, # Changed
            description="åˆçº§é˜¶æ®µï¼šåŸºç¡€çº§+ç®€å•ä¸­çº§è®¾è®¡"
        ),
        CurriculumStageConfig(
            name="intermediate",
            dataset_levels=["intermediate"],
            complexity_range=(3.0, 7.0),
            epochs_ratio=0.25,
            performance_threshold=0.6,
            min_evaluations=10, # Changed
            description="ä¸­çº§é˜¶æ®µï¼šä¸­ç­‰å¤æ‚åº¦çš„ä¸­çº§è®¾è®¡"
        ),
        CurriculumStageConfig(
            name="advanced",
            dataset_levels=["intermediate", "advanced"],
            complexity_range=(5.0, 9.0),
            epochs_ratio=0.15,
            performance_threshold=0.55,
            min_evaluations=10, # Changed
            description="é«˜çº§é˜¶æ®µï¼šå¤æ‚çš„ä¸­çº§å’Œé«˜çº§è®¾è®¡"
        ),
        CurriculumStageConfig(
            name="expert",
            dataset_levels=["advanced", "expert"],
            complexity_range=(7.0, 10.0),
            epochs_ratio=0.1,
            performance_threshold=0.5,
            min_evaluations=10, # Changed
            description="ä¸“å®¶é˜¶æ®µï¼šæœ€å¤æ‚çš„é«˜çº§å’Œä¸“å®¶çº§è®¾è®¡"
        )
    ]
    return stages


def create_custom_curriculum_stages(
    dataset_distribution: Dict[str, Any],
    focus_levels: List[str] = None,
    complexity_emphasis: str = "balanced"  # "simple", "balanced", "complex"
) -> List[CurriculumStageConfig]:
    """æ ¹æ®æ•°æ®é›†åˆ†å¸ƒåˆ›å»ºè‡ªå®šä¹‰è¯¾ç¨‹é˜¶æ®µ"""
    
    if focus_levels is None:
        focus_levels = ["basic", "intermediate", "advanced", "expert"]
    
    # æ ¹æ®å¤æ‚åº¦åå¥½è°ƒæ•´å¤æ‚åº¦èŒƒå›´
    if complexity_emphasis == "simple":
        complexity_ranges = [(0, 3), (0, 4), (2, 6), (4, 8), (6, 10)]
    elif complexity_emphasis == "complex":
        complexity_ranges = [(0, 4), (2, 6), (4, 8), (6, 10), (8, 10)]
    else:  # balanced
        complexity_ranges = [(0, 3), (0, 5), (3, 7), (5, 9), (7, 10)]
    
    stages = []
    
    # åŸºç¡€é˜¶æ®µ
    if "basic" in focus_levels:
        stages.append(CurriculumStageConfig(
            name="foundation",
            dataset_levels=["basic"],
            complexity_range=complexity_ranges[0],
            epochs_ratio=0.3, # Will be normalized later
            performance_threshold=0.7,
            min_evaluations=10, # Added
            description="åŸºç¡€é˜¶æ®µï¼šæœ€ç®€å•çš„åŸºç¡€çº§è®¾è®¡"
        ))
    
    # åˆçº§é˜¶æ®µ
    if "basic" in focus_levels and "intermediate" in focus_levels:
        stages.append(CurriculumStageConfig(
            name="elementary",
            dataset_levels=["basic", "intermediate"],
            complexity_range=complexity_ranges[1],
            epochs_ratio=0.25, # Will be normalized
            performance_threshold=0.65,
            min_evaluations=10, # Added
            description="åˆçº§é˜¶æ®µï¼šåŸºç¡€åˆ°ä¸­çº§çš„è¿‡æ¸¡"
        ))
    
    # ä¸­çº§é˜¶æ®µ
    if "intermediate" in focus_levels:
        stages.append(CurriculumStageConfig(
            name="intermediate",
            dataset_levels=["intermediate"],
            complexity_range=complexity_ranges[2],
            epochs_ratio=0.25, # Will be normalized
            performance_threshold=0.6,
            min_evaluations=10, # Added
            description="ä¸­çº§é˜¶æ®µï¼šä¸­ç­‰å¤æ‚åº¦è®¾è®¡"
        ))
    
    # é«˜çº§é˜¶æ®µ
    if "advanced" in focus_levels:
        stages.append(CurriculumStageConfig(
            name="advanced",
            dataset_levels=["intermediate", "advanced"],
            complexity_range=complexity_ranges[3],
            epochs_ratio=0.15, # Will be normalized
            performance_threshold=0.55,
            min_evaluations=10, # Added
            description="é«˜çº§é˜¶æ®µï¼šå¤æ‚è®¾è®¡"
        ))
    
    # ä¸“å®¶é˜¶æ®µ
    if "expert" in focus_levels:
        stages.append(CurriculumStageConfig(
            name="expert",
            dataset_levels=["advanced", "expert"],
            complexity_range=complexity_ranges[4],
            epochs_ratio=0.05, # Will be normalized
            performance_threshold=0.5,
            min_evaluations=10, # Added
            description="ä¸“å®¶é˜¶æ®µï¼šæœ€å¤æ‚è®¾è®¡"
        ))
    
    if not stages: # Ensure at least one stage if focus_levels is empty or misconfigured
        logger.warning("No custom stages generated based on focus_levels. Falling back to a single default stage.")
        stages.append(CurriculumStageConfig(
            name="default_full_range",
            dataset_levels=focus_levels if focus_levels else ["basic", "intermediate", "advanced", "expert"],
            complexity_range=(0.0, 10.0),
            epochs_ratio=1.0,
            performance_threshold=0.6, # Default threshold
            min_evaluations=10,        # Default min_evaluations
            description="Default stage covering all specified levels and full complexity range."
        ))

    # æ ‡å‡†åŒ–epochæ¯”ä¾‹
    total_ratio = sum(stage.epochs_ratio for stage in stages)
    for stage in stages:
        stage.epochs_ratio /= total_ratio
    
    return stages

class DynamicDifficultyAdjuster:
    """åŠ¨æ€è°ƒæ•´å½“å‰é˜¶æ®µçš„éš¾åº¦å­é›†"""
    
    def __init__(self, curriculum_manager):
        self.curriculum_manager = curriculum_manager
        self.difficulty_adjustment_history = []
    
    def adjust_current_stage_difficulty(self, performance_metrics):
        """æ ¹æ®æ€§èƒ½åŠ¨æ€è°ƒæ•´å½“å‰é˜¶æ®µçš„æ•°æ®å­é›†"""
        current_stage = self.curriculum_manager.current_stage
        
        if current_stage < 3:  # åªåœ¨éæœ€é«˜é˜¶æ®µè°ƒæ•´
            return
            
        # è·å–å½“å‰æ€§èƒ½
        recent_avg_performance = np.mean([
            m['performance'] for m in performance_metrics[-10:]  # æœ€è¿‘10æ¬¡è¯„ä¼°
        ]) if len(performance_metrics) >= 10 else 0.5
        
        # åŠ¨æ€è°ƒæ•´ç­–ç•¥
        if recent_avg_performance > 0.95:  # æ€§èƒ½å¾ˆå¥½ï¼Œå¢åŠ éš¾åº¦
            self._increase_difficulty()
        elif recent_avg_performance < 0.8:  # æ€§èƒ½ä¸ä½³ï¼Œç¨å¾®é™ä½éš¾åº¦
            self._decrease_difficulty()
    
    def _increase_difficulty(self):
        """å¢åŠ å½“å‰é˜¶æ®µçš„éš¾åº¦"""
        current_stage_obj = self.curriculum_manager.curriculum_stages[self.curriculum_manager.current_stage]
        
        # å¢åŠ å¤æ‚åº¦èŒƒå›´çš„ä¸Šé™
        if current_stage_obj.complexity_range[1] < 10:
            new_max_complexity = min(10, current_stage_obj.complexity_range[1] + 1)
            current_stage_obj.complexity_range = (current_stage_obj.complexity_range[0], new_max_complexity)
            logger.info(f"ğŸ”¥ å¢åŠ éš¾åº¦: å¤æ‚åº¦èŒƒå›´è°ƒæ•´ä¸º {current_stage_obj.complexity_range}")
    
    def _decrease_difficulty(self):
        """ç¨å¾®é™ä½å½“å‰é˜¶æ®µçš„éš¾åº¦"""
        current_stage_obj = self.curriculum_manager.curriculum_stages[self.curriculum_manager.current_stage]
        
        # é™ä½å¤æ‚åº¦èŒƒå›´çš„ä¸‹é™
        if current_stage_obj.complexity_range[0] > 0:
            new_min_complexity = max(0, current_stage_obj.complexity_range[0] - 1)
            current_stage_obj.complexity_range = (new_min_complexity, current_stage_obj.complexity_range[1])
            logger.info(f"ğŸ”» é™ä½éš¾åº¦: å¤æ‚åº¦èŒƒå›´è°ƒæ•´ä¸º {current_stage_obj.complexity_range}")
