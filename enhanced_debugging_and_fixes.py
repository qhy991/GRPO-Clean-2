# enhanced_debugging_and_fixes.py - ä¿®å¤è®­ç»ƒæ³¢åŠ¨å’Œè¯¾ç¨‹å­¦ä¹ é—®é¢˜

import logging
import wandb
import json
import os
from typing import Dict, Any, Optional, List
from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl
import numpy as np
from datetime import datetime

logger = logging.getLogger(__name__)

# 1. ä¿®å¤è¯¾ç¨‹å­¦ä¹ çŠ¶æ€ç›‘æ§å’Œæ—¥å¿—
# EnhancedCurriculumDebugCallback MOVED to grpo_project.curriculum.callbacks


# 2. ä¿®å¤Qwen3å…¼å®¹æ€§é—®é¢˜
class Qwen3CompatibilityFixer:
    """ä¿®å¤Qwen3æ¨¡å‹å…¼å®¹æ€§é—®é¢˜"""
    
    @staticmethod
    def fix_generation_config(model, tokenizer):
        """ä¿®å¤Qwen3çš„ç”Ÿæˆé…ç½®"""
        from transformers import GenerationConfig
        
        logger.info("ğŸ”§ ä¿®å¤Qwen3ç”Ÿæˆé…ç½®...")
        
        # ç¡®ä¿tokenizerè®¾ç½®æ­£ç¡®
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.pad_token_id = tokenizer.eos_token_id
            logger.info("è®¾ç½®pad_tokenä¸ºeos_token")
        
        # ä¿®å¤æ¨¡å‹é…ç½®
        model_config = getattr(model, 'config', None)
        if model_config:
            model_config.pad_token_id = tokenizer.pad_token_id
            model_config.eos_token_id = tokenizer.eos_token_id
        
        # åˆ›å»ºé€‚åˆQwen3çš„ç”Ÿæˆé…ç½®
        if not hasattr(model, 'generation_config') or model.generation_config is None:
            model.generation_config = GenerationConfig()
        
        # Qwen3ç‰¹å®šé…ç½®
        model.generation_config.pad_token_id = tokenizer.pad_token_id
        model.generation_config.eos_token_id = tokenizer.eos_token_id
        model.generation_config.do_sample = True
        model.generation_config.temperature = 0.7
        model.generation_config.top_p = 0.8
        model.generation_config.top_k = 40
        model.generation_config.repetition_penalty = 1.05
        
        logger.info("âœ… Qwen3ç”Ÿæˆé…ç½®ä¿®å¤å®Œæˆ")
        return model, tokenizer
    
    @staticmethod
    def create_qwen3_prompt(content: str) -> str:
        """åˆ›å»ºQwen3æ ¼å¼çš„prompt"""
        # Qwen3ä½¿ç”¨çš„å¯¹è¯æ ¼å¼
        system_message = """You are a Verilog expert. Please provide your solution in the following format:

<think>
Your detailed thinking process here
</think>

```verilog
Your complete Verilog code here
```"""
        
        prompt = f"<|im_start|>system\n{system_message}<|im_end|>\n<|im_start|>user\n{content}<|im_end|>\n<|im_start|>assistant\n"
        return prompt


# 3. å¢å¼ºçš„å¥–åŠ±ç¨³å®šæ€§ç›‘æ§
# class RewardStabilityMonitor(TrainerCallback): # Already MOVED to grpo_project.callbacks.monitoring in previous step
#     """ç›‘æ§å¥–åŠ±ç¨³å®šæ€§ï¼Œå‡å°‘è®­ç»ƒæ³¢åŠ¨"""
# ... (rest of the class was here)

# 4. ä¿®å¤çš„å¥–åŠ±è®¡ç®—å‡½æ•°ï¼ˆå‡å°‘æ³¢åŠ¨ï¼‰
def create_stabilized_reward_calculator(reward_config, stability_monitor: Optional[RewardStabilityMonitor] = None):
    """åˆ›å»ºç¨³å®šåŒ–çš„å¥–åŠ±è®¡ç®—å™¨"""
    
    def stabilized_reward_calculator(*args, **kwargs):
        """ç¨³å®šåŒ–çš„å¥–åŠ±è®¡ç®—"""
        # è¿™é‡Œè°ƒç”¨åŸå§‹çš„å¥–åŠ±è®¡ç®—å‡½æ•°
        try:
            # å‡è®¾åŸå§‹å‡½æ•°è¿”å› (rewards, metrics)
            rewards, metrics = enhanced_batch_reward_calculator(*args, **kwargs)
            
            # åº”ç”¨ç¨³å®šåŒ–å¤„ç†
            if rewards:
                # è®°å½•åˆ°ç¨³å®šæ€§ç›‘æ§å™¨
                if stability_monitor:
                    step = kwargs.get('training_step', 0)
                    for reward in rewards:
                        stability_monitor.add_reward(reward, step)
                
                # åº”ç”¨å¥–åŠ±å‰Šå³°å’Œå¹³æ»‘
                stabilized_rewards = []
                for reward in rewards:
                    # å‰Šå³°å¤„ç†ï¼ˆé™åˆ¶æå€¼ï¼‰
                    clipped_reward = np.clip(reward, -15.0, 15.0)
                    
                    # è½»å¾®å¹³æ»‘ï¼ˆå‡å°‘å™ªå£°ï¼‰
                    if len(stabilized_rewards) > 0:
                        smoothed_reward = 0.9 * clipped_reward + 0.1 * stabilized_rewards[-1]
                    else:
                        smoothed_reward = clipped_reward
                    
                    stabilized_rewards.append(smoothed_reward)
                
                return stabilized_rewards, metrics
            
        except Exception as e:
            logger.error(f"å¥–åŠ±è®¡ç®—å¼‚å¸¸: {e}", exc_info=True)
            # è¿”å›å®‰å…¨çš„é»˜è®¤å€¼
            num_items = len(args[0]) if args and len(args) > 0 else 1
            return [-5.0] * num_items, {}
        
        return rewards, metrics
    
    return stabilized_reward_calculator


# 5. ä½¿ç”¨ç¤ºä¾‹å’Œé›†æˆæŒ‡å¯¼
def integrate_enhanced_debugging(trainer, curriculum_manager, output_dir, model, tokenizer):
    """é›†æˆæ‰€æœ‰è°ƒè¯•å¢å¼ºåŠŸèƒ½"""
    
    logger.info("ğŸ”§ é›†æˆå¢å¼ºè°ƒè¯•åŠŸèƒ½...")
    
    # 1. ä¿®å¤Qwen3å…¼å®¹æ€§
    model, tokenizer = Qwen3CompatibilityFixer.fix_generation_config(model, tokenizer)
    
    # 2. åˆ›å»ºè°ƒè¯•å›è°ƒ
    callbacks_to_add = []
    
    # è¯¾ç¨‹å­¦ä¹ è°ƒè¯•å›è°ƒ
    if curriculum_manager:
        curriculum_debug_cb = EnhancedCurriculumDebugCallback(
            curriculum_manager, trainer, output_dir
        )
        callbacks_to_add.append(curriculum_debug_cb)
        logger.info("âœ… æ·»åŠ è¯¾ç¨‹å­¦ä¹ è°ƒè¯•å›è°ƒ")
    
    # å¥–åŠ±ç¨³å®šæ€§ç›‘æ§
    stability_monitor = RewardStabilityMonitor(output_dir)
    callbacks_to_add.append(stability_monitor)
    logger.info("âœ… æ·»åŠ å¥–åŠ±ç¨³å®šæ€§ç›‘æ§")
    
    # 3. å°†å›è°ƒæ·»åŠ åˆ°è®­ç»ƒå™¨
    for callback in callbacks_to_add:
        trainer.add_callback(callback)
    
    logger.info(f"ğŸ¯ æˆåŠŸé›†æˆ{len(callbacks_to_add)}ä¸ªè°ƒè¯•åŠŸèƒ½")
    
    return trainer, stability_monitor


# 6. ä¸»è¦ä¿®å¤ç‚¹æ€»ç»“
"""
ä¸»è¦ä¿®å¤çš„é—®é¢˜ï¼š

1. è¯¾ç¨‹å­¦ä¹ é—®é¢˜ï¼š
   - è¯¾ç¨‹è¿›é˜¶é€»è¾‘ä¿®å¤
   - è¯¦ç»†çš„è¯¾ç¨‹çŠ¶æ€æ—¥å¿—
   - W&Bä½¿ç”¨æ•°å€¼è€Œéæ–‡å­—è®°å½•
   - æ•°æ®é›†æ›´æ–°æœºåˆ¶ä¿®å¤

2. Qwen3å…¼å®¹æ€§ï¼š
   - æ­£ç¡®çš„å¯¹è¯æ ¼å¼
   - ç”Ÿæˆé…ç½®ä¼˜åŒ–
   - tokenizerè®¾ç½®ä¿®å¤

3. è®­ç»ƒç¨³å®šæ€§ï¼š
   - å¥–åŠ±å‰Šå³°å’Œå¹³æ»‘
   - ç¨³å®šæ€§æŒ‡æ ‡ç›‘æ§
   - å¼‚å¸¸æƒ…å†µå¤„ç†

4. è°ƒè¯•ä¿¡æ¯å¢å¼ºï¼š
   - ä¸“ç”¨æ—¥å¿—æ–‡ä»¶
   - è¯¦ç»†çš„çŠ¶æ€è¿½è¸ª
   - å¼‚å¸¸æ•è·å’Œå¤„ç†

ä½¿ç”¨æ–¹æ³•ï¼š
1. åœ¨train.pyä¸­å¯¼å…¥è¿™äº›å‡½æ•°
2. åœ¨traineråˆå§‹åŒ–åè°ƒç”¨integrate_enhanced_debugging
3. åœ¨å¥–åŠ±å‡½æ•°ä¸­ä½¿ç”¨create_stabilized_reward_calculator
"""