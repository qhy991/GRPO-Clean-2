# enhanced_debugging_and_fixes.py - ä¿®å¤è®­ç»ƒæ³¢åŠ¨å’Œè¯¾ç¨‹å­¦ä¹ é—®é¢˜

import logging
import wandb
import json
import os
from typing import Dict, Any, Optional, List
from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl
import numpy as np
from datetime import datetime

logger = logging.getLogger(__name__)

# 1. ä¿®å¤è¯¾ç¨‹å­¦ä¹ çŠ¶æ€ç›‘æ§å’Œæ—¥å¿—
class EnhancedCurriculumDebugCallback(TrainerCallback):
    """å¢å¼ºçš„è¯¾ç¨‹å­¦ä¹ è°ƒè¯•å›è°ƒ"""
    
    def __init__(self, curriculum_manager, trainer_ref=None, output_dir=None):
        self.curriculum_manager = curriculum_manager
        self.trainer_ref = trainer_ref
        self.output_dir = output_dir
        self.last_logged_stage = -1
        self.stage_change_history = []
        
        # åˆ›å»ºè¯¾ç¨‹å­¦ä¹ ä¸“ç”¨æ—¥å¿—æ–‡ä»¶
        if self.output_dir:
            self.curriculum_log_file = os.path.join(output_dir, "curriculum_detailed_log.txt")
            with open(self.curriculum_log_file, 'w', encoding='utf-8') as f:
                f.write(f"=== è¯¾ç¨‹å­¦ä¹ è¯¦ç»†è°ƒè¯•æ—¥å¿— ===\n")
                f.write(f"åˆå§‹åŒ–æ—¶é—´: {datetime.now()}\n")
                f.write(f"è¯¾ç¨‹ç®¡ç†å™¨æ˜¯å¦ä¸ºNone: {self.curriculum_manager is None}\n")
                if self.curriculum_manager:
                    f.write(f"æ€»é˜¶æ®µæ•°: {len(self.curriculum_manager.curriculum_stages)}\n")
                    f.write(f"å½“å‰é˜¶æ®µ: {self.curriculum_manager.current_stage}\n")
                    for i, stage in enumerate(self.curriculum_manager.curriculum_stages):
                        f.write(f"  é˜¶æ®µ{i}: {stage.name} - ç­‰çº§{stage.dataset_levels} - å¤æ‚åº¦{stage.complexity_range}\n")
                f.write("\n")
    
    def _write_curriculum_log(self, message: str):
        """å†™å…¥è¯¾ç¨‹å­¦ä¹ æ—¥å¿—"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_message = f"[{timestamp}] {message}"
        
        # æ§åˆ¶å°è¾“å‡º
        logger.info(f"ğŸ“š CURRICULUM: {message}")
        
        # æ–‡ä»¶è¾“å‡º
        if self.curriculum_log_file:
            try:
                with open(self.curriculum_log_file, 'a', encoding='utf-8') as f:
                    f.write(log_message + "\n")
            except Exception as e:
                logger.warning(f"æ— æ³•å†™å…¥è¯¾ç¨‹æ—¥å¿—æ–‡ä»¶: {e}")
    
    def on_log(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, logs=None, **kwargs):
        if not self.curriculum_manager or logs is None:
            return
        
        current_step = getattr(state, 'global_step', 0) or 0
        current_stage = self.curriculum_manager.current_stage
        
        # æ£€æŸ¥é˜¶æ®µæ˜¯å¦å‘ç”Ÿå˜åŒ–
        if current_stage != self.last_logged_stage:
            self._log_stage_change(current_step, current_stage)
            self.last_logged_stage = current_stage
        
        # æ¯50æ­¥è®°å½•ä¸€æ¬¡è¯¦ç»†çŠ¶æ€
        if current_step % 50 == 0:
            self._log_detailed_status(current_step, logs)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›é˜¶
        self._check_advancement_conditions(current_step, logs)
    
    def _log_stage_change(self, step: int, new_stage: int):
        """è®°å½•é˜¶æ®µå˜åŒ–"""
        if new_stage < len(self.curriculum_manager.curriculum_stages):
            stage_info = self.curriculum_manager.curriculum_stages[new_stage]
            message = f"ğŸ¯ é˜¶æ®µå˜æ›´åˆ° {new_stage}: {stage_info.name}"
            message += f" | ç­‰çº§: {stage_info.dataset_levels}"
            message += f" | å¤æ‚åº¦: {stage_info.complexity_range}"
            message += f" | æ­¥æ•°: {step}"
            
            self._write_curriculum_log(message)
            
            # è®°å½•å˜æ›´å†å²
            self.stage_change_history.append({
                "step": step,
                "new_stage": new_stage,
                "stage_name": stage_info.name,
                "dataset_levels": stage_info.dataset_levels,
                "complexity_range": stage_info.complexity_range,
                "timestamp": datetime.now().isoformat()
            })
            
            # W&Bæ—¥å¿—ï¼ˆä½¿ç”¨æ•°å€¼è€Œéæ–‡å­—ï¼‰
            if hasattr(wandb, 'run') and wandb.run is not None:
                wandb.log({
                    "curriculum/current_stage_index": new_stage,
                    "curriculum/total_stages": len(self.curriculum_manager.curriculum_stages),
                    "curriculum/stage_progress_ratio": new_stage / len(self.curriculum_manager.curriculum_stages),
                    "curriculum/stage_change_step": step,
                    # ä½¿ç”¨æ•°å€¼ç¼–ç ä»£æ›¿æ–‡å­—
                    "curriculum/level_count": len(stage_info.dataset_levels),
                    "curriculum/complexity_min": stage_info.complexity_range[0],
                    "curriculum/complexity_max": stage_info.complexity_range[1],
                }, step=step)
    
    def _log_detailed_status(self, step: int, logs: Dict[str, Any]):
        """è®°å½•è¯¦ç»†çŠ¶æ€"""
        if not self.curriculum_manager:
            return
            
        current_stage = self.curriculum_manager.current_stage
        performance_history = getattr(self.curriculum_manager, 'stage_performance_history', [])
        
        # è·å–å½“å‰é˜¶æ®µé…ç½®
        if current_stage < len(self.curriculum_manager.curriculum_stages):
            stage_config = self.curriculum_manager.curriculum_stages[current_stage]
            
            message = f"è¯¦ç»†çŠ¶æ€ - æ­¥æ•°: {step}"
            message += f" | å½“å‰é˜¶æ®µ: {current_stage} ({stage_config.name})"
            message += f" | æ€§èƒ½å†å²é•¿åº¦: {len(performance_history)}"
            message += f" | æœ€å°è¯„ä¼°æ¬¡æ•°: {stage_config.min_evaluations}"
            message += f" | æ€§èƒ½é˜ˆå€¼: {stage_config.performance_threshold}"
            
            if performance_history:
                recent_perf = np.mean(performance_history[-3:]) if len(performance_history) >= 3 else performance_history[-1]
                message += f" | æœ€è¿‘æ€§èƒ½: {recent_perf:.4f}"
            
            # è·å–å½“å‰æŸå¤±
            current_loss = logs.get('train_loss', logs.get('loss', float('inf')))
            if current_loss != float('inf'):
                performance_estimate = max(0, 1.0 - (current_loss / 10.0))
                message += f" | å½“å‰æŸå¤±: {current_loss:.4f} | æ€§èƒ½ä¼°è®¡: {performance_estimate:.4f}"
            
            self._write_curriculum_log(message)
            
            # W&Bè¯¦ç»†çŠ¶æ€æ—¥å¿—
            if hasattr(wandb, 'run') and wandb.run is not None:
                wandb_data = {
                    "curriculum_detail/performance_history_length": len(performance_history),
                    "curriculum_detail/min_evaluations": stage_config.min_evaluations,
                    "curriculum_detail/performance_threshold": stage_config.performance_threshold,
                }
                
                if performance_history:
                    wandb_data["curriculum_detail/recent_performance"] = recent_perf
                    wandb_data["curriculum_detail/performance_mean"] = np.mean(performance_history)
                    wandb_data["curriculum_detail/performance_std"] = np.std(performance_history)
                
                if current_loss != float('inf'):
                    wandb_data["curriculum_detail/performance_estimate"] = performance_estimate
                
                wandb.log(wandb_data, step=step)
    
    def _check_advancement_conditions(self, step: int, logs: Dict[str, Any]):
        """æ£€æŸ¥è¿›é˜¶æ¡ä»¶"""
        if not self.curriculum_manager:
            return
        
        current_loss = logs.get('train_loss', logs.get('loss', float('inf')))
        if current_loss == float('inf'):
            return
        
        performance_estimate = max(0, 1.0 - (current_loss / 10.0))
        
        try:
            should_advance = self.curriculum_manager.should_advance_stage(performance_estimate)
            
            message = f"è¿›é˜¶æ£€æŸ¥ - æ­¥æ•°: {step}"
            message += f" | æ€§èƒ½ä¼°è®¡: {performance_estimate:.4f}"
            message += f" | æ˜¯å¦åº”è¯¥è¿›é˜¶: {should_advance}"
            
            if should_advance:
                message += " | ğŸš€ æ»¡è¶³è¿›é˜¶æ¡ä»¶!"
                old_stage = self.curriculum_manager.current_stage
                
                # å°è¯•è¿›é˜¶
                if hasattr(self.curriculum_manager, 'advance_stage'):
                    success = self.curriculum_manager.advance_stage()
                else:
                    success = False
                    message += " | âŒ æ²¡æœ‰advance_stageæ–¹æ³•"
                
                if success:
                    new_stage = self.curriculum_manager.current_stage
                    message += f" | âœ… æˆåŠŸä»é˜¶æ®µ{old_stage}è¿›é˜¶åˆ°{new_stage}"
                    
                    # æ›´æ–°è®­ç»ƒå™¨æ•°æ®é›†
                    if self.trainer_ref and hasattr(self.trainer_ref, 'train_dataset'):
                        try:
                            new_dataset = self.curriculum_manager.get_current_stage_dataset()
                            self.trainer_ref.train_dataset = new_dataset
                            message += f" | ğŸ“Š å·²æ›´æ–°æ•°æ®é›†ï¼ŒåŒ…å«{len(new_dataset)}ä¸ªæ ·æœ¬"
                        except Exception as e:
                            message += f" | âš ï¸ æ›´æ–°æ•°æ®é›†å¤±è´¥: {e}"
                else:
                    message += " | âŒ è¿›é˜¶å¤±è´¥"
            
            self._write_curriculum_log(message)
            
        except Exception as e:
            error_message = f"è¿›é˜¶æ£€æŸ¥å¼‚å¸¸ - æ­¥æ•°: {step} | é”™è¯¯: {e}"
            self._write_curriculum_log(error_message)
            logger.error(f"è¯¾ç¨‹å­¦ä¹ è¿›é˜¶æ£€æŸ¥å¼‚å¸¸: {e}", exc_info=True)


# 2. ä¿®å¤Qwen3å…¼å®¹æ€§é—®é¢˜
class Qwen3CompatibilityFixer:
    """ä¿®å¤Qwen3æ¨¡å‹å…¼å®¹æ€§é—®é¢˜"""
    
    @staticmethod
    def fix_generation_config(model, tokenizer):
        """ä¿®å¤Qwen3çš„ç”Ÿæˆé…ç½®"""
        from transformers import GenerationConfig
        
        logger.info("ğŸ”§ ä¿®å¤Qwen3ç”Ÿæˆé…ç½®...")
        
        # ç¡®ä¿tokenizerè®¾ç½®æ­£ç¡®
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.pad_token_id = tokenizer.eos_token_id
            logger.info("è®¾ç½®pad_tokenä¸ºeos_token")
        
        # ä¿®å¤æ¨¡å‹é…ç½®
        model_config = getattr(model, 'config', None)
        if model_config:
            model_config.pad_token_id = tokenizer.pad_token_id
            model_config.eos_token_id = tokenizer.eos_token_id
        
        # åˆ›å»ºé€‚åˆQwen3çš„ç”Ÿæˆé…ç½®
        if not hasattr(model, 'generation_config') or model.generation_config is None:
            model.generation_config = GenerationConfig()
        
        # Qwen3ç‰¹å®šé…ç½®
        model.generation_config.pad_token_id = tokenizer.pad_token_id
        model.generation_config.eos_token_id = tokenizer.eos_token_id
        model.generation_config.do_sample = True
        model.generation_config.temperature = 0.7
        model.generation_config.top_p = 0.8
        model.generation_config.top_k = 40
        model.generation_config.repetition_penalty = 1.05
        
        logger.info("âœ… Qwen3ç”Ÿæˆé…ç½®ä¿®å¤å®Œæˆ")
        return model, tokenizer
    
    @staticmethod
    def create_qwen3_prompt(content: str) -> str:
        """åˆ›å»ºQwen3æ ¼å¼çš„prompt"""
        # Qwen3ä½¿ç”¨çš„å¯¹è¯æ ¼å¼
        system_message = """You are a Verilog expert. Please provide your solution in the following format:

<think>
Your detailed thinking process here
</think>

```verilog
Your complete Verilog code here
```"""
        
        prompt = f"<|im_start|>system\n{system_message}<|im_end|>\n<|im_start|>user\n{content}<|im_end|>\n<|im_start|>assistant\n"
        return prompt


# 3. å¢å¼ºçš„å¥–åŠ±ç¨³å®šæ€§ç›‘æ§
class RewardStabilityMonitor(TrainerCallback):
    """ç›‘æ§å¥–åŠ±ç¨³å®šæ€§ï¼Œå‡å°‘è®­ç»ƒæ³¢åŠ¨"""
    
    def __init__(self, output_dir: str, window_size: int = 100):
        self.output_dir = output_dir
        self.window_size = window_size
        self.reward_history = []
        self.loss_history = []
        self.stability_metrics = []
        
        # åˆ›å»ºå¥–åŠ±ç¨³å®šæ€§æ—¥å¿—æ–‡ä»¶
        self.stability_log_file = os.path.join(output_dir, "reward_stability_log.txt")
        with open(self.stability_log_file, 'w', encoding='utf-8') as f:
            f.write(f"=== å¥–åŠ±ç¨³å®šæ€§ç›‘æ§æ—¥å¿— ===\n")
            f.write(f"åˆå§‹åŒ–æ—¶é—´: {datetime.now()}\n")
            f.write(f"ç›‘æ§çª—å£å¤§å°: {window_size}\n\n")
    
    def add_reward(self, reward: float, step: int):
        """æ·»åŠ å¥–åŠ±å€¼"""
        self.reward_history.append({"reward": reward, "step": step})
        
        # ä¿æŒçª—å£å¤§å°
        if len(self.reward_history) > self.window_size:
            self.reward_history.pop(0)
    
    def add_loss(self, loss: float, step: int):
        """æ·»åŠ æŸå¤±å€¼"""
        self.loss_history.append({"loss": loss, "step": step})
        
        # ä¿æŒçª—å£å¤§å°
        if len(self.loss_history) > self.window_size:
            self.loss_history.pop(0)
    
    def calculate_stability_metrics(self, step: int) -> Dict[str, float]:
        """è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡"""
        if len(self.reward_history) < 10:
            return {}
        
        rewards = [item["reward"] for item in self.reward_history]
        losses = [item["loss"] for item in self.loss_history]
        
        metrics = {
            "reward_mean": np.mean(rewards),
            "reward_std": np.std(rewards),
            "reward_cv": np.std(rewards) / (abs(np.mean(rewards)) + 1e-8),  # å˜å¼‚ç³»æ•°
            "reward_range": np.max(rewards) - np.min(rewards),
            "reward_positive_ratio": np.mean(np.array(rewards) > 0),
        }
        
        if losses:
            metrics.update({
                "loss_mean": np.mean(losses),
                "loss_std": np.std(losses),
                "loss_cv": np.std(losses) / (abs(np.mean(losses)) + 1e-8),
            })
        
        # è®¡ç®—è¶‹åŠ¿
        if len(rewards) >= 20:
            recent_rewards = rewards[-10:]
            earlier_rewards = rewards[-20:-10]
            metrics["reward_trend"] = np.mean(recent_rewards) - np.mean(earlier_rewards)
        
        self.stability_metrics.append({"step": step, "metrics": metrics})
        return metrics
    
    def on_log(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, logs=None, **kwargs):
        if logs is None or args.local_rank > 0:
            return
        
        current_step = getattr(state, 'global_step', 0) or 0
        
        # è®°å½•æŸå¤±
        if 'train_loss' in logs or 'loss' in logs:
            loss = logs.get('train_loss', logs.get('loss'))
            self.add_loss(loss, current_step)
        
        # æ¯50æ­¥è®¡ç®—å’Œè®°å½•ç¨³å®šæ€§æŒ‡æ ‡
        if current_step % 50 == 0 and current_step > 0:
            metrics = self.calculate_stability_metrics(current_step)
            
            if metrics:
                self._log_stability_metrics(current_step, metrics)
                
                # W&Bæ—¥å¿—
                if hasattr(wandb, 'run') and wandb.run is not None:
                    wandb_data = {f"stability/{k}": v for k, v in metrics.items()}
                    wandb.log(wandb_data, step=current_step)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒæ•´è®­ç»ƒå‚æ•°
                self._check_stability_and_suggest_adjustments(current_step, metrics)
    
    def _log_stability_metrics(self, step: int, metrics: Dict[str, float]):
        """è®°å½•ç¨³å®šæ€§æŒ‡æ ‡"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        log_message = f"[{timestamp}] æ­¥æ•°: {step}\n"
        log_message += f"  å¥–åŠ±å‡å€¼: {metrics.get('reward_mean', 0):.4f}\n"
        log_message += f"  å¥–åŠ±æ ‡å‡†å·®: {metrics.get('reward_std', 0):.4f}\n"
        log_message += f"  å¥–åŠ±å˜å¼‚ç³»æ•°: {metrics.get('reward_cv', 0):.4f}\n"
        log_message += f"  æ­£å¥–åŠ±æ¯”ä¾‹: {metrics.get('reward_positive_ratio', 0):.4f}\n"
        if 'reward_trend' in metrics:
            log_message += f"  å¥–åŠ±è¶‹åŠ¿: {metrics['reward_trend']:.4f}\n"
        log_message += "\n"
        
        logger.info(f"ğŸ“Š STABILITY: æ­¥æ•°{step} - å¥–åŠ±CV: {metrics.get('reward_cv', 0):.4f}")
        
        try:
            with open(self.stability_log_file, 'a', encoding='utf-8') as f:
                f.write(log_message)
        except Exception as e:
            logger.warning(f"æ— æ³•å†™å…¥ç¨³å®šæ€§æ—¥å¿—: {e}")
    
    def _check_stability_and_suggest_adjustments(self, step: int, metrics: Dict[str, float]):
        """æ£€æŸ¥ç¨³å®šæ€§å¹¶å»ºè®®è°ƒæ•´"""
        reward_cv = metrics.get('reward_cv', 0)
        reward_positive_ratio = metrics.get('reward_positive_ratio', 0)
        
        suggestions = []
        
        # é«˜å˜å¼‚ç³»æ•° -> è®­ç»ƒä¸ç¨³å®š
        if reward_cv > 2.0:
            suggestions.append("å¥–åŠ±å˜å¼‚ç³»æ•°è¿‡é«˜ï¼Œå»ºè®®é™ä½å­¦ä¹ ç‡æˆ–å¢åŠ batch size")
        
        # è´Ÿå¥–åŠ±è¿‡å¤š -> å¥–åŠ±è®¾è®¡é—®é¢˜
        if reward_positive_ratio < 0.2:
            suggestions.append("æ­£å¥–åŠ±æ¯”ä¾‹è¿‡ä½ï¼Œå»ºè®®æ£€æŸ¥å¥–åŠ±å‡½æ•°è®¾è®¡")
        
        # å¥–åŠ±è¶‹åŠ¿ä¸‹é™ -> å¯èƒ½è¿‡æ‹Ÿåˆ
        if 'reward_trend' in metrics and metrics['reward_trend'] < -1.0:
            suggestions.append("å¥–åŠ±å‘ˆä¸‹é™è¶‹åŠ¿ï¼Œå¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆ")
        
        if suggestions:
            suggestion_msg = f"âš ï¸ ç¨³å®šæ€§å»ºè®® (æ­¥æ•°{step}): " + "; ".join(suggestions)
            logger.warning(suggestion_msg)
            
            try:
                with open(self.stability_log_file, 'a', encoding='utf-8') as f:
                    f.write(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {suggestion_msg}\n\n")
            except Exception as e:
                logger.warning(f"æ— æ³•å†™å…¥å»ºè®®æ—¥å¿—: {e}")


# 4. ä¿®å¤çš„å¥–åŠ±è®¡ç®—å‡½æ•°ï¼ˆå‡å°‘æ³¢åŠ¨ï¼‰
def create_stabilized_reward_calculator(reward_config, stability_monitor: Optional[RewardStabilityMonitor] = None):
    """åˆ›å»ºç¨³å®šåŒ–çš„å¥–åŠ±è®¡ç®—å™¨"""
    
    def stabilized_reward_calculator(*args, **kwargs):
        """ç¨³å®šåŒ–çš„å¥–åŠ±è®¡ç®—"""
        # è¿™é‡Œè°ƒç”¨åŸå§‹çš„å¥–åŠ±è®¡ç®—å‡½æ•°
        try:
            # å‡è®¾åŸå§‹å‡½æ•°è¿”å› (rewards, metrics)
            rewards, metrics = enhanced_batch_reward_calculator(*args, **kwargs)
            
            # åº”ç”¨ç¨³å®šåŒ–å¤„ç†
            if rewards:
                # è®°å½•åˆ°ç¨³å®šæ€§ç›‘æ§å™¨
                if stability_monitor:
                    step = kwargs.get('training_step', 0)
                    for reward in rewards:
                        stability_monitor.add_reward(reward, step)
                
                # åº”ç”¨å¥–åŠ±å‰Šå³°å’Œå¹³æ»‘
                stabilized_rewards = []
                for reward in rewards:
                    # å‰Šå³°å¤„ç†ï¼ˆé™åˆ¶æå€¼ï¼‰
                    clipped_reward = np.clip(reward, -15.0, 15.0)
                    
                    # è½»å¾®å¹³æ»‘ï¼ˆå‡å°‘å™ªå£°ï¼‰
                    if len(stabilized_rewards) > 0:
                        smoothed_reward = 0.9 * clipped_reward + 0.1 * stabilized_rewards[-1]
                    else:
                        smoothed_reward = clipped_reward
                    
                    stabilized_rewards.append(smoothed_reward)
                
                return stabilized_rewards, metrics
            
        except Exception as e:
            logger.error(f"å¥–åŠ±è®¡ç®—å¼‚å¸¸: {e}", exc_info=True)
            # è¿”å›å®‰å…¨çš„é»˜è®¤å€¼
            num_items = len(args[0]) if args and len(args) > 0 else 1
            return [-5.0] * num_items, {}
        
        return rewards, metrics
    
    return stabilized_reward_calculator


# 5. ä½¿ç”¨ç¤ºä¾‹å’Œé›†æˆæŒ‡å¯¼
def integrate_enhanced_debugging(trainer, curriculum_manager, output_dir, model, tokenizer):
    """é›†æˆæ‰€æœ‰è°ƒè¯•å¢å¼ºåŠŸèƒ½"""
    
    logger.info("ğŸ”§ é›†æˆå¢å¼ºè°ƒè¯•åŠŸèƒ½...")
    
    # 1. ä¿®å¤Qwen3å…¼å®¹æ€§
    model, tokenizer = Qwen3CompatibilityFixer.fix_generation_config(model, tokenizer)
    
    # 2. åˆ›å»ºè°ƒè¯•å›è°ƒ
    callbacks_to_add = []
    
    # è¯¾ç¨‹å­¦ä¹ è°ƒè¯•å›è°ƒ
    if curriculum_manager:
        curriculum_debug_cb = EnhancedCurriculumDebugCallback(
            curriculum_manager, trainer, output_dir
        )
        callbacks_to_add.append(curriculum_debug_cb)
        logger.info("âœ… æ·»åŠ è¯¾ç¨‹å­¦ä¹ è°ƒè¯•å›è°ƒ")
    
    # å¥–åŠ±ç¨³å®šæ€§ç›‘æ§
    stability_monitor = RewardStabilityMonitor(output_dir)
    callbacks_to_add.append(stability_monitor)
    logger.info("âœ… æ·»åŠ å¥–åŠ±ç¨³å®šæ€§ç›‘æ§")
    
    # 3. å°†å›è°ƒæ·»åŠ åˆ°è®­ç»ƒå™¨
    for callback in callbacks_to_add:
        trainer.add_callback(callback)
    
    logger.info(f"ğŸ¯ æˆåŠŸé›†æˆ{len(callbacks_to_add)}ä¸ªè°ƒè¯•åŠŸèƒ½")
    
    return trainer, stability_monitor


# 6. ä¸»è¦ä¿®å¤ç‚¹æ€»ç»“
"""
ä¸»è¦ä¿®å¤çš„é—®é¢˜ï¼š

1. è¯¾ç¨‹å­¦ä¹ é—®é¢˜ï¼š
   - è¯¾ç¨‹è¿›é˜¶é€»è¾‘ä¿®å¤
   - è¯¦ç»†çš„è¯¾ç¨‹çŠ¶æ€æ—¥å¿—
   - W&Bä½¿ç”¨æ•°å€¼è€Œéæ–‡å­—è®°å½•
   - æ•°æ®é›†æ›´æ–°æœºåˆ¶ä¿®å¤

2. Qwen3å…¼å®¹æ€§ï¼š
   - æ­£ç¡®çš„å¯¹è¯æ ¼å¼
   - ç”Ÿæˆé…ç½®ä¼˜åŒ–
   - tokenizerè®¾ç½®ä¿®å¤

3. è®­ç»ƒç¨³å®šæ€§ï¼š
   - å¥–åŠ±å‰Šå³°å’Œå¹³æ»‘
   - ç¨³å®šæ€§æŒ‡æ ‡ç›‘æ§
   - å¼‚å¸¸æƒ…å†µå¤„ç†

4. è°ƒè¯•ä¿¡æ¯å¢å¼ºï¼š
   - ä¸“ç”¨æ—¥å¿—æ–‡ä»¶
   - è¯¦ç»†çš„çŠ¶æ€è¿½è¸ª
   - å¼‚å¸¸æ•è·å’Œå¤„ç†

ä½¿ç”¨æ–¹æ³•ï¼š
1. åœ¨train.pyä¸­å¯¼å…¥è¿™äº›å‡½æ•°
2. åœ¨traineråˆå§‹åŒ–åè°ƒç”¨integrate_enhanced_debugging
3. åœ¨å¥–åŠ±å‡½æ•°ä¸­ä½¿ç”¨create_stabilized_reward_calculator
"""