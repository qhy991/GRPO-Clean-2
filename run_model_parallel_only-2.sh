#!/bin/bash
# run_model_parallel_only.sh - çº¯æ¨¡å‹å¹¶è¡Œè®­ç»ƒè„šæœ¬ï¼ˆä¸ä½¿ç”¨FSDPåˆ†å¸ƒå¼ï¼‰

set -e

# ğŸ”§ æ¿€æ´»condaç¯å¢ƒ
echo "ğŸ”§ æ¿€æ´»ReasoningVç¯å¢ƒ..."
source /home/qhy/anaconda3/bin/activate ReasoningV

# æ¸…é™¤åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡
echo "ğŸ§¹ æ¸…é™¤åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡..."
unset RANK 2>/dev/null || true
unset LOCAL_RANK 2>/dev/null || true  
unset WORLD_SIZE 2>/dev/null || true
unset MASTER_ADDR 2>/dev/null || true
unset MASTER_PORT 2>/dev/null || true

# ğŸ”§ è®¾ç½®GPUè®¾å¤‡
export CUDA_VISIBLE_DEVICES=0,1

# ğŸ”§ æ¨¡å‹å¹¶è¡Œä¼˜åŒ–è®¾ç½®
export PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True,max_split_size_mb:128,roundup_power2_divisions:4"
export CUDA_LAUNCH_BLOCKING=0
export TORCH_NCCL_BLOCKING_WAIT=1
export NCCL_TIMEOUT=7200
export NCCL_DEBUG=WARN
export NCCL_P2P_DISABLE=0

# Flash Attention
export FLASH_ATTENTION_V2=1

# ç›®å½•è®¾ç½®
SCRIPT_DIR=$(cd "$(dirname "${BASH_SOURCE[0]}")" &> /dev/null && pwd)
PYTHON_EXECUTABLE="python"

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'  
BLUE='\033[0;34m'
NC='\033[0m'

log_info() { echo -e "${GREEN}[INFO]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARN]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

# é…ç½®å‚æ•°
export WANDB_PROJECT="VerilogGRPO_FullTrain_ModelParallel"  # ä¿®æ”¹é¡¹ç›®åä»¥åŒºåˆ†å…¨é‡è®­ç»ƒ
export WANDB_ENTITY="qhy0227-tsinghua-university"

BASE_MODEL_NAME_OR_PATH="/home/share/Qwen3-8B/models--Qwen--Qwen3-8B/snapshots/a80f5e57cce20e57b65145f4213844dec1a80834"
STAGE1_ADAPTER_PATH="/home/share/ReasoningV/Qwen3/ckpts/sft-qwen3-lora-5epoch-origen200k-S1-20250429_211831/checkpoint-34695/"
DATASET_PATH="/home/qhy/Research/LLM/GRPO-RV/dataset/all-with-module-2.jsonl"
DATASET_BASE_PATH="/home/qhy/Research/LLM/GRPO-RV/dataset"
OUTPUT_DIR_BASE="./full_train_model_parallel_outputs"  # å…¨é‡è®­ç»ƒè¾“å‡ºç›®å½•

# éªŒè¯è·¯å¾„
if [ ! -f "${DATASET_PATH}" ]; then
    log_error "âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: ${DATASET_PATH}"
    exit 1
fi

# ğŸ”§ çº¯æ¨¡å‹å¹¶è¡Œé…ç½®ï¼ˆå…³é”®ï¼šä¸ä½¿ç”¨FSDPï¼‰
USE_MODEL_PARALLEL=true
USE_FSDP=false  # æ˜ç¡®ç¦ç”¨FSDP
MAX_MEMORY_PER_GPU="70GiB"  # å…¨é‡è®­ç»ƒæ—¶ç¨å¾®å‡å°‘å†…å­˜é™åˆ¶
LOW_CPU_MEM_USAGE=true

# ğŸ”§ å…¨é‡è®­ç»ƒé…ç½®ï¼ˆç¦ç”¨LoRAï¼‰
USE_LORA=false  # å…³é”®ï¼šç¦ç”¨LoRAä»¥å¯ç”¨å…¨é‡è®­ç»ƒ
LORA_RANK=0     # è®¾ä¸º0è¡¨ç¤ºä¸ä½¿ç”¨LoRA
LORA_ALPHA=0
LORA_DROPOUT=0.0
LORA_TARGET_MODULES=""  # ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºä¸æŒ‡å®šLoRAç›®æ ‡æ¨¡å—

# é•¿åº¦é…ç½®
MAX_SEQ_LENGTH=6144
MAX_PROMPT_LENGTH=1536
MAX_COMPLETION_LENGTH=4608
LENGTH_ALLOCATION_STRATEGY="custom"

# ğŸ”§ å…¨é‡è®­ç»ƒé…ç½®
PER_DEVICE_TRAIN_BATCH_SIZE=1  # å…¨é‡è®­ç»ƒæ—¶å‡å°æ‰¹æ¬¡å¤§å°
GRADIENT_ACCUMULATION_STEPS=8  # å¢åŠ æ¢¯åº¦ç´¯ç§¯ä»¥ä¿æŒæœ‰æ•ˆæ‰¹æ¬¡å¤§å°
LEARNING_RATE=5e-6             # å…¨é‡è®­ç»ƒæ—¶ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
NUM_TRAIN_EPOCHS=2             # å…¨é‡è®­ç»ƒé€šå¸¸éœ€è¦æ›´å°‘çš„epochs
WARMUP_RATIO=0.05              # å‡å°‘warmupæ¯”ä¾‹
LR_SCHEDULER_TYPE="cosine"
WEIGHT_DECAY=0.01

# æ€§èƒ½è®¾ç½®
BF16_ENABLED=true
FP16_ENABLED=false
GRADIENT_CHECKPOINTING_ENABLED=false  # æ¨¡å‹å¹¶è¡Œæ—¶ç¦ç”¨
OPTIMIZER_TYPE="adamw_torch"

# æ•°æ®åŠ è½½é…ç½®
DATALOADER_NUM_WORKERS=0
DATALOADER_PIN_MEMORY=false

# å…¶ä»–é…ç½®
NUM_GENERATIONS_GRPO=2
CALLBACK_NUM_SAMPLES=2
CALLBACK_EVAL_EVERY_N_STEPS=15
SAVE_STRATEGY="steps"
SAVE_STEPS=50
SAVE_TOTAL_LIMIT=2
LOGGING_STEPS=10

# è¯¾ç¨‹å­¦ä¹ é…ç½®
ENABLE_CURRICULUM=true
CURRICULUM_TYPE="dual_layer"
CURRICULUM_FOCUS_LEVELS="advanced basic intermediate"
CURRICULUM_COMPLEXITY_EMPHASIS="simple"
CURRICULUM_PERFORMANCE_CHECK_INTERVAL=15

# ç»éªŒå›æ”¾é…ç½®
ENABLE_EXPERIENCE_REPLAY=true
EXPERIENCE_BUFFER_SIZE=1500
REPLAY_SAMPLE_RATIO=0.2

# å¥–åŠ±é…ç½®
REWARD_COMPILATION_SUCCESS=2.0
REWARD_COMPILATION_FAILURE=-4.0
REWARD_TEST_PASS_BASE=1.5
REWARD_TEST_PASS_BONUS_MULTIPLIER=1.2
REWARD_MAX_FUNCTIONAL=15.0
REWARD_ALL_TESTS_PASSED_BONUS=5.0

# ç”Ÿæˆå‚æ•°
GEN_TEMPERATURE=0.7
GEN_TOP_K=40
GEN_TOP_P=0.8
GEN_REPETITION_PENALTY=1.05

# åŠ¨æ€WandBè¿è¡Œåç§°
TIMESTAMP=$(date +%Y%m%d-%H%M%S)
if [ "$USE_LORA" = true ]; then
    export WANDB_RUN_NAME="model-parallel-LoRA-LR${LEARNING_RATE}-R${LORA_RANK}-${TIMESTAMP}"
else
    export WANDB_RUN_NAME="model-parallel-FullTrain-LR${LEARNING_RATE}-${TIMESTAMP}"
fi

# æ„å»ºå‘½ä»¤å‚æ•°
CMD_ARGS=""

# åŸºç¡€é…ç½®
CMD_ARGS="${CMD_ARGS} --wandb_project \"${WANDB_PROJECT}\""
CMD_ARGS="${CMD_ARGS} --wandb_entity \"${WANDB_ENTITY}\""
CMD_ARGS="${CMD_ARGS} --model_name_or_path \"${BASE_MODEL_NAME_OR_PATH}\""
CMD_ARGS="${CMD_ARGS} --stage1_adapter_path \"${STAGE1_ADAPTER_PATH}\""
CMD_ARGS="${CMD_ARGS} --dataset_path \"${DATASET_PATH}\""
CMD_ARGS="${CMD_ARGS} --dataset_base_path \"${DATASET_BASE_PATH}\""
CMD_ARGS="${CMD_ARGS} --output_dir_base \"${OUTPUT_DIR_BASE}\""

# ğŸ”§ å…³é”®ï¼šæ˜ç¡®å¯ç”¨æ¨¡å‹å¹¶è¡Œ
CMD_ARGS="${CMD_ARGS} --use_model_parallel true"
CMD_ARGS="${CMD_ARGS} --max_memory_per_gpu \"${MAX_MEMORY_PER_GPU}\""
CMD_ARGS="${CMD_ARGS} --low_cpu_mem_usage ${LOW_CPU_MEM_USAGE}"

# ğŸ”§ å…¨é‡è®­ç»ƒé…ç½®ï¼ˆç¦ç”¨LoRAï¼‰
CMD_ARGS="${CMD_ARGS} --use_lora ${USE_LORA}"
if [ "$USE_LORA" = true ]; then
    CMD_ARGS="${CMD_ARGS} --lora_rank ${LORA_RANK}"
    CMD_ARGS="${CMD_ARGS} --lora_alpha ${LORA_ALPHA}"
    CMD_ARGS="${CMD_ARGS} --lora_dropout ${LORA_DROPOUT}"
    CMD_ARGS="${CMD_ARGS} --lora_target_modules ${LORA_TARGET_MODULES}"
else
    echo "[INFO] ğŸ¯ å…¨é‡è®­ç»ƒæ¨¡å¼ï¼šå·²ç¦ç”¨LoRAé…ç½®"
fi

# é•¿åº¦é…ç½®
CMD_ARGS="${CMD_ARGS} --max_seq_length ${MAX_SEQ_LENGTH}"
CMD_ARGS="${CMD_ARGS} --script_max_prompt_length ${MAX_PROMPT_LENGTH}"
CMD_ARGS="${CMD_ARGS} --script_max_completion_length ${MAX_COMPLETION_LENGTH}"
CMD_ARGS="${CMD_ARGS} --length_allocation_strategy ${LENGTH_ALLOCATION_STRATEGY}"

# è®­ç»ƒé…ç½®
CMD_ARGS="${CMD_ARGS} --per_device_train_batch_size ${PER_DEVICE_TRAIN_BATCH_SIZE}"
CMD_ARGS="${CMD_ARGS} --gradient_accumulation_steps ${GRADIENT_ACCUMULATION_STEPS}"
CMD_ARGS="${CMD_ARGS} --learning_rate ${LEARNING_RATE}"
CMD_ARGS="${CMD_ARGS} --num_train_epochs ${NUM_TRAIN_EPOCHS}"
CMD_ARGS="${CMD_ARGS} --warmup_ratio ${WARMUP_RATIO}"
CMD_ARGS="${CMD_ARGS} --lr_scheduler_type \"${LR_SCHEDULER_TYPE}\""
CMD_ARGS="${CMD_ARGS} --weight_decay ${WEIGHT_DECAY}"

# GRPOé…ç½®
CMD_ARGS="${CMD_ARGS} --num_generations ${NUM_GENERATIONS_GRPO}"
CMD_ARGS="${CMD_ARGS} --max_prompt_length ${MAX_PROMPT_LENGTH}"
CMD_ARGS="${CMD_ARGS} --max_completion_length ${MAX_COMPLETION_LENGTH}"

# å›è°ƒé…ç½®
CMD_ARGS="${CMD_ARGS} --callback_num_samples ${CALLBACK_NUM_SAMPLES}"
CMD_ARGS="${CMD_ARGS} --callback_eval_every_n_steps ${CALLBACK_EVAL_EVERY_N_STEPS}"

# è¯¾ç¨‹å­¦ä¹ é…ç½®
CMD_ARGS="${CMD_ARGS} --enable_curriculum ${ENABLE_CURRICULUM}"
CMD_ARGS="${CMD_ARGS} --curriculum_type ${CURRICULUM_TYPE}"
CMD_ARGS="${CMD_ARGS} --curriculum_focus_levels ${CURRICULUM_FOCUS_LEVELS}"
CMD_ARGS="${CMD_ARGS} --curriculum_complexity_emphasis ${CURRICULUM_COMPLEXITY_EMPHASIS}"
CMD_ARGS="${CMD_ARGS} --curriculum_performance_check_interval ${CURRICULUM_PERFORMANCE_CHECK_INTERVAL}"

# ç»éªŒå›æ”¾é…ç½®
CMD_ARGS="${CMD_ARGS} --enable_experience_replay ${ENABLE_EXPERIENCE_REPLAY}"
CMD_ARGS="${CMD_ARGS} --experience_buffer_size ${EXPERIENCE_BUFFER_SIZE}"
CMD_ARGS="${CMD_ARGS} --replay_sample_ratio ${REPLAY_SAMPLE_RATIO}"

# å¥–åŠ±é…ç½®
CMD_ARGS="${CMD_ARGS} --compilation_success ${REWARD_COMPILATION_SUCCESS}"
CMD_ARGS="${CMD_ARGS} --compilation_failure ${REWARD_COMPILATION_FAILURE}"
CMD_ARGS="${CMD_ARGS} --test_pass_base_reward ${REWARD_TEST_PASS_BASE}"
CMD_ARGS="${CMD_ARGS} --test_pass_bonus_multiplier ${REWARD_TEST_PASS_BONUS_MULTIPLIER}"
CMD_ARGS="${CMD_ARGS} --max_functional_reward ${REWARD_MAX_FUNCTIONAL}"
CMD_ARGS="${CMD_ARGS} --all_tests_passed_bonus ${REWARD_ALL_TESTS_PASSED_BONUS}"

# ç”Ÿæˆå‚æ•°
CMD_ARGS="${CMD_ARGS} --gen_temperature ${GEN_TEMPERATURE}"
CMD_ARGS="${CMD_ARGS} --gen_top_k ${GEN_TOP_K}"
CMD_ARGS="${CMD_ARGS} --gen_top_p ${GEN_TOP_P}"
CMD_ARGS="${CMD_ARGS} --gen_repetition_penalty ${GEN_REPETITION_PENALTY}"

# æ€§èƒ½è®¾ç½®
if [ "$BF16_ENABLED" = true ]; then CMD_ARGS="${CMD_ARGS} --bf16"; fi
if [ "$FP16_ENABLED" = true ] && [ "$BF16_ENABLED" = false ]; then CMD_ARGS="${CMD_ARGS} --fp16"; fi
if [ "$GRADIENT_CHECKPOINTING_ENABLED" = true ]; then CMD_ARGS="${CMD_ARGS} --gradient_checkpointing"; fi

# æ•°æ®åŠ è½½é…ç½®
CMD_ARGS="${CMD_ARGS} --dataloader_num_workers ${DATALOADER_NUM_WORKERS}"
if [ "$DATALOADER_PIN_MEMORY" = true ]; then CMD_ARGS="${CMD_ARGS} --dataloader_pin_memory"; fi

# ä¿å­˜å’Œæ—¥å¿—é…ç½®
CMD_ARGS="${CMD_ARGS} --logging_strategy \"steps\""
CMD_ARGS="${CMD_ARGS} --logging_steps ${LOGGING_STEPS}"
CMD_ARGS="${CMD_ARGS} --save_strategy \"${SAVE_STRATEGY}\""
CMD_ARGS="${CMD_ARGS} --save_steps ${SAVE_STEPS}"
CMD_ARGS="${CMD_ARGS} --save_total_limit ${SAVE_TOTAL_LIMIT}"
CMD_ARGS="${CMD_ARGS} --report_to \"wandb\""
CMD_ARGS="${CMD_ARGS} --remove_unused_columns False"
CMD_ARGS="${CMD_ARGS} --optim \"${OPTIMIZER_TYPE}\""
CMD_ARGS="${CMD_ARGS} --ddp_find_unused_parameters False"

# ç¼“å­˜ç›®å½•
CACHE_DIR_BASE="${SCRIPT_DIR}/.model_parallel_cache"
mkdir -p "${CACHE_DIR_BASE}/datasets"
mkdir -p "${CACHE_DIR_BASE}/models"
CMD_ARGS="${CMD_ARGS} --cache_dir \"${CACHE_DIR_BASE}/models\""

# Pythonè„šæœ¬è·¯å¾„
PYTHON_SCRIPT_TO_RUN="${SCRIPT_DIR}/main.py"

# ğŸ”§ å…³é”®ï¼šä½¿ç”¨ç›´æ¥pythonæ‰§è¡Œï¼Œä¸ä½¿ç”¨torchrun
FULL_CMD="${PYTHON_EXECUTABLE} ${PYTHON_SCRIPT_TO_RUN} ${CMD_ARGS}"

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p ${OUTPUT_DIR_BASE}

# è®­ç»ƒå‰æ‘˜è¦
echo ""
echo "========================================================================"
if [ "$USE_LORA" = true ]; then
    echo "                    æ¨¡å‹å¹¶è¡ŒLoRAè®­ç»ƒå¯åŠ¨"
else
    echo "                    æ¨¡å‹å¹¶è¡Œå…¨é‡è®­ç»ƒå¯åŠ¨"
fi
echo "========================================================================"
log_info "ğŸ¯ è®­ç»ƒé…ç½®æ‘˜è¦:"
log_info "  - æ¨¡å¼: çº¯æ¨¡å‹å¹¶è¡Œï¼ˆæ— FSDPï¼Œæ— DDPï¼‰"
log_info "  - è®­ç»ƒç±»å‹: $([ "$USE_LORA" = true ] && echo "LoRAå¾®è°ƒ" || echo "å…¨é‡è®­ç»ƒ")"
log_info "  - å¯åŠ¨æ–¹å¼: ç›´æ¥pythonï¼ˆå•è¿›ç¨‹ï¼‰"
log_info "  - GPUé…ç½®: ${CUDA_VISIBLE_DEVICES}"
log_info "  - æ¨¡å‹: $(basename "${BASE_MODEL_NAME_OR_PATH}")"
log_info "  - æ•°æ®é›†: $(basename "${DATASET_PATH}")"
log_info "  - åºåˆ—é•¿åº¦: ${MAX_SEQ_LENGTH}"
if [ "$USE_LORA" = true ]; then
    log_info "  - LoRA: rank=${LORA_RANK}, alpha=${LORA_ALPHA}"
else
    log_info "  - å…¨é‡è®­ç»ƒ: æ‰€æœ‰å‚æ•°å¯è®­ç»ƒ"
fi
log_info "  - æ‰¹æ¬¡å¤§å°: ${PER_DEVICE_TRAIN_BATCH_SIZE}"
log_info "  - æ¢¯åº¦ç´¯ç§¯: ${GRADIENT_ACCUMULATION_STEPS}"
log_info "  - æœ‰æ•ˆæ‰¹æ¬¡: $((PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS))"
log_info "  - å­¦ä¹ ç‡: ${LEARNING_RATE}"
log_info "  - è®­ç»ƒè½®æ•°: ${NUM_TRAIN_EPOCHS}"
log_info "  - è¾“å‡ºç›®å½•: ${OUTPUT_DIR_BASE}"

echo ""
log_info "ğŸ“Š å†…å­˜é…ç½®:"
log_info "  - æ¯GPUå†…å­˜é™åˆ¶: ${MAX_MEMORY_PER_GPU}"
log_info "  - ä½CPUå†…å­˜ä½¿ç”¨: ${LOW_CPU_MEM_USAGE}"
log_info "  - æ¢¯åº¦æ£€æŸ¥ç‚¹: ${GRADIENT_CHECKPOINTING_ENABLED}"

echo ""
echo "========================================================================"

# ä¿å­˜å®Œæ•´å‘½ä»¤
echo "${FULL_CMD}" > "${OUTPUT_DIR_BASE}/full_training_command.txt"
log_info "ğŸ’¾ å®Œæ•´å‘½ä»¤å·²ä¿å­˜åˆ°: ${OUTPUT_DIR_BASE}/full_training_command.txt"

# è®­ç»ƒå‰GPUçŠ¶æ€
echo ""
log_info "ğŸ” è®­ç»ƒå‰GPUçŠ¶æ€:"
nvidia-smi --query-gpu=index,name,memory.used,memory.total --format=csv,noheader

# æ¸…ç†å‡½æ•°
cleanup_on_exit() {
    echo ""
    log_info "ğŸ›‘ è®­ç»ƒç»“æŸ/ä¸­æ–­ï¼Œæ‰§è¡Œæ¸…ç†..."
    log_info "ğŸ“Š æœ€ç»ˆGPUçŠ¶æ€:"
    nvidia-smi --query-gpu=index,name,memory.used,memory.total --format=csv,noheader
    log_info "â° è®­ç»ƒç»“æŸæ—¶é—´: $(date)"
}
trap cleanup_on_exit INT TERM EXIT

# ğŸ”§ æ‰§è¡Œè®­ç»ƒå‘½ä»¤
echo ""
log_info "ğŸš€ å¼€å§‹çº¯æ¨¡å‹å¹¶è¡ŒGRPOè®­ç»ƒ ($(date))..."

eval ${FULL_CMD}

status=$?

# è®­ç»ƒåæ‘˜è¦
echo ""
echo "========================================================================"
echo "                      çº¯æ¨¡å‹å¹¶è¡Œè®­ç»ƒå®Œæˆæ‘˜è¦"
echo "========================================================================"
log_info "â° è®­ç»ƒç»“æŸæ—¶é—´: $(date)"
log_info "ğŸ¯ é€€å‡ºçŠ¶æ€: ${status}"

if [ $status -eq 0 ]; then
    log_info "âœ… çº¯æ¨¡å‹å¹¶è¡ŒGRPOè®­ç»ƒæˆåŠŸå®Œæˆ!"
else
    log_error "âŒ çº¯æ¨¡å‹å¹¶è¡Œè®­ç»ƒå¤±è´¥ï¼Œé€€å‡ºç : ${status}"
fi

echo "========================================================================"

exit ${status} 