#!/bin/bash
# run_enhanced_grpo_training_v2.sh - æ”¯æŒæ–°æ•°æ®é›†æ ¼å¼çš„å¢å¼ºGRPOè®­ç»ƒè„šæœ¬

# --- Exit on error ---
set -e
export CUDA_VISIBLE_DEVICES=0,1
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
# --- Get the directory where the script is located ---
SCRIPT_DIR=$(cd "$(dirname "${BASH_SOURCE[0]}")" &> /dev/null && pwd)
PYTHON_EXECUTABLE="python3"

# --- é›†æˆçš„æ–­ç»­è®­ç»ƒæ£€æŸ¥åŠŸèƒ½ ---
# è®¾ç½®æ§åˆ¶å˜é‡
SKIP_SAFETY_CHECK=${SKIP_SAFETY_CHECK:-false}  # å¯é€šè¿‡ç¯å¢ƒå˜é‡è·³è¿‡æ£€æŸ¥
AUTO_FIX_ISSUES=${AUTO_FIX_ISSUES:-true}       # è‡ªåŠ¨ä¿®å¤å‘ç°çš„é—®é¢˜

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# æ—¥å¿—å‡½æ•°
log_info() { echo -e "${GREEN}[INFO]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARN]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }
log_debug() { echo -e "${BLUE}[DEBUG]${NC} $1"; }

# æ–­ç»­è®­ç»ƒå®‰å…¨æ£€æŸ¥å‡½æ•°
run_safety_checks() {
    log_info "ğŸ” å¼€å§‹æ–­ç»­è®­ç»ƒå®‰å…¨æ£€æŸ¥..."
    
    local total_issues=0
    local critical_issues=0
    local check_passed=true
    
    # 1. å¿«é€Ÿç»“æ„æ£€æŸ¥
    log_debug "æ£€æŸ¥é¡¹ç›®æ–‡ä»¶ç»“æ„..."
    local missing_files=""
    
    if [ ! -f "${SCRIPT_DIR}/main.py" ]; then
        missing_files="${missing_files} main.py"
    fi
    
    if [ ! -d "${SCRIPT_DIR}/grpo_project" ]; then
        missing_files="${missing_files} grpo_project/"
    fi
    
    if [ ! -f "${SCRIPT_DIR}/grpo_project/configs/__init__.py" ]; then
        missing_files="${missing_files} grpo_project/configs/"
    fi
    
    if [ -n "$missing_files" ]; then
        log_error "âŒ ç¼ºå°‘å…³é”®æ–‡ä»¶:$missing_files"
        critical_issues=$((critical_issues + 1))
        check_passed=false
    else
        log_debug "âœ… é¡¹ç›®æ–‡ä»¶ç»“æ„å®Œæ•´"
    fi
    
    # 2. Pythonç¯å¢ƒæ£€æŸ¥
    log_debug "æ£€æŸ¥Pythonç¯å¢ƒ..."
    local missing_packages=""
    
    for package in torch transformers trl datasets wandb numpy peft; do
        if ! ${PYTHON_EXECUTABLE} -c "import $package" 2>/dev/null; then
            missing_packages="${missing_packages} $package"
        fi
    done
    
    if [ -n "$missing_packages" ]; then
        log_error "âŒ ç¼ºå°‘PythonåŒ…:$missing_packages"
        log_error "   è¯·è¿è¡Œ: pip install$missing_packages"
        critical_issues=$((critical_issues + 1))
        check_passed=false
    else
        log_debug "âœ… Pythonç¯å¢ƒæ£€æŸ¥é€šè¿‡"
    fi
    
    # 3. è¿è¡Œè¯¦ç»†çš„Pythonæ£€æŸ¥ï¼ˆå¦‚æœåŸºç¡€æ£€æŸ¥é€šè¿‡ï¼‰
    if [ "$check_passed" = true ]; then
        log_debug "è¿è¡Œè¯¦ç»†é…ç½®æ£€æŸ¥..."
        
        if [ -f "${SCRIPT_DIR}/quick_resume_check.py" ]; then
            # ä½¿ç”¨æˆ‘ä»¬çš„å¿«é€Ÿæ£€æŸ¥å·¥å…·
            if ${PYTHON_EXECUTABLE} "${SCRIPT_DIR}/quick_resume_check.py" > /tmp/quick_check_output.txt 2>&1; then
                log_debug "âœ… è¯¦ç»†é…ç½®æ£€æŸ¥é€šè¿‡"
                
                # æ˜¾ç¤ºé‡è¦ä¿¡æ¯
                if grep -q "ğŸš¨\|âŒ" /tmp/quick_check_output.txt; then
                    log_warning "âš ï¸ å‘ç°ä¸€äº›é—®é¢˜ï¼Œä½†å¯ä»¥ç»§ç»­è®­ç»ƒï¼š"
                    grep "ğŸš¨\|âŒ" /tmp/quick_check_output.txt | head -3 | while read line; do
                        log_warning "   $line"
                    done
                    total_issues=$((total_issues + 1))
                fi
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                rm -f /tmp/quick_check_output.txt
            else
                log_warning "âš ï¸ è¯¦ç»†æ£€æŸ¥å¤±è´¥ï¼Œä½†åŸºç¡€æ£€æŸ¥é€šè¿‡ï¼Œå¯ä»¥ç»§ç»­"
                total_issues=$((total_issues + 1))
                
                # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯ï¼ˆç®€åŒ–ï¼‰
                if [ -f /tmp/quick_check_output.txt ]; then
                    log_debug "æ£€æŸ¥è¾“å‡ºï¼š"
                    tail -n 5 /tmp/quick_check_output.txt | while read line; do
                        log_debug "   $line"
                    done
                    rm -f /tmp/quick_check_output.txt
                fi
            fi
        else
            log_debug "â„¹ï¸ æœªæ‰¾åˆ°è¯¦ç»†æ£€æŸ¥å·¥å…·ï¼Œè·³è¿‡é«˜çº§æ£€æŸ¥"
        fi
        
        # 4. é¡¹ç›®æ¨¡å—å¯¼å…¥æ£€æŸ¥
        log_debug "æ£€æŸ¥é¡¹ç›®æ¨¡å—å¯¼å…¥..."
        if ! ${PYTHON_EXECUTABLE} -c "
import sys
sys.path.insert(0, '${SCRIPT_DIR}')
from grpo_project.configs import EnvConfig, ScriptConfig, EnhancedRewardConfig
from grpo_project.curriculum.manager import setup_fixed_curriculum_manager
print('âœ… é¡¹ç›®æ¨¡å—å¯¼å…¥æˆåŠŸ')
" 2>/dev/null; then
            log_warning "âš ï¸ é¡¹ç›®æ¨¡å—å¯¼å…¥æœ‰é—®é¢˜ï¼Œä½†å¯èƒ½ä»å¯ç»§ç»­è®­ç»ƒ"
            total_issues=$((total_issues + 1))
        else
            log_debug "âœ… é¡¹ç›®æ¨¡å—å¯¼å…¥æˆåŠŸ"
        fi
    fi
    
    # æ£€æŸ¥ç»“æœæ±‡æ€»
    if [ $critical_issues -gt 0 ]; then
        log_error "ğŸš¨ å‘ç° $critical_issues ä¸ªä¸¥é‡é—®é¢˜ï¼Œæ— æ³•ç»§ç»­è®­ç»ƒ"
        log_error "   è¯·ä¿®å¤è¿™äº›é—®é¢˜åé‡æ–°è¿è¡Œ"
        log_info "ğŸ’¡ ä¿®å¤å»ºè®®ï¼š"
        log_info "   1. ç¡®ä¿åœ¨æ­£ç¡®çš„é¡¹ç›®ç›®å½•ä¸‹"
        log_info "   2. å®‰è£…ç¼ºå¤±çš„PythonåŒ…"
        log_info "   3. æ£€æŸ¥é¡¹ç›®æ–‡ä»¶å®Œæ•´æ€§"
        return 1
    elif [ $total_issues -gt 0 ]; then
        log_warning "âš ï¸ å‘ç° $total_issues ä¸ªéä¸¥é‡é—®é¢˜ï¼Œä½†å¯ä»¥ç»§ç»­è®­ç»ƒ"
        log_info "ğŸ’¡ å»ºè®®åœ¨è®­ç»ƒå®ŒæˆåæŸ¥çœ‹è¯¦ç»†æ—¥å¿—å¹¶ä¿®å¤è¿™äº›é—®é¢˜"
    else
        log_info "âœ… æ‰€æœ‰å®‰å…¨æ£€æŸ¥é€šè¿‡ï¼"
    fi
    
    return 0
}

# è¿è¡Œè‡ªåŠ¨ä¿®å¤ï¼ˆå¦‚æœå¯ç”¨ï¼‰
run_auto_fix() {
    if [ "$AUTO_FIX_ISSUES" = true ]; then
        log_info "ğŸ”§ è¿è¡Œè‡ªåŠ¨ä¿®å¤..."
        
        # æ¸…ç†å¯èƒ½æ®‹ç•™çš„ç¯å¢ƒå˜é‡
        unset WANDB_RUN_ID 2>/dev/null || true
        unset WANDB_RESUME 2>/dev/null || true
        unset WANDB_RUN_NAME 2>/dev/null || true
        
        # æ¸…ç†Pythonç¼“å­˜
        find "${SCRIPT_DIR}" -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true
        find "${SCRIPT_DIR}" -name "*.pyc" -delete 2>/dev/null || true
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        mkdir -p "${SCRIPT_DIR}/wandb" 2>/dev/null || true
        
        log_debug "âœ… è‡ªåŠ¨ä¿®å¤å®Œæˆ"
    fi
}

# ä¸»æ£€æŸ¥æµç¨‹
if [ "$SKIP_SAFETY_CHECK" != true ]; then
    echo ""
    echo "ğŸ›¡ï¸ =================================================="
    echo "     GRPOæ–­ç»­è®­ç»ƒå®‰å…¨æ£€æŸ¥"
    echo "=================================================="
    
    # è¿è¡Œè‡ªåŠ¨ä¿®å¤
    run_auto_fix
    
    # è¿è¡Œå®‰å…¨æ£€æŸ¥
    if run_safety_checks; then
        log_info "ğŸ¯ å®‰å…¨æ£€æŸ¥å®Œæˆï¼Œç»§ç»­è®­ç»ƒ..."
        echo ""
    else
        log_error "ğŸ’¥ å®‰å…¨æ£€æŸ¥å¤±è´¥ï¼Œåœæ­¢è®­ç»ƒ"
        echo ""
        log_info "ğŸ”§ å¦‚éœ€è·³è¿‡æ£€æŸ¥ï¼ˆä¸æ¨èï¼‰ï¼Œå¯è®¾ç½®ç¯å¢ƒå˜é‡ï¼š"
        log_info "   export SKIP_SAFETY_CHECK=true"
        log_info "   ç„¶åé‡æ–°è¿è¡Œè®­ç»ƒè„šæœ¬"
        exit 1
    fi
else
    log_warning "âš ï¸ å·²è·³è¿‡å®‰å…¨æ£€æŸ¥ï¼ˆSKIP_SAFETY_CHECK=trueï¼‰"
    echo ""
fi

# --- Enhanced Environment Setup ---
export WANDB_PROJECT="VerilogGRPO_Enhanced_8B"
export WANDB_ENTITY="qhy0227-tsinghua-university"

# --- WandB Step Sync Fix ---
# å¯ç”¨WandBæ­¥æ•°åŒæ­¥ä¿®å¤æ¨¡å—ï¼Œè§£å†³stepä¸åŒ¹é…é—®é¢˜
export WANDB_STEP_FIX_ENABLED=true

# Enable better CUDA memory management
export PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:128"

# --- Distributed Training Parameters ---
NUM_GPUS_PER_NODE=2
MASTER_ADDR="localhost"
MASTER_PORT=$(shuf -i 10000-19999 -n 1)

# --- Enhanced Model and Data Configuration ---
BASE_MODEL_NAME_OR_PATH="/home/share/Qwen3-8B/models--Qwen--Qwen3-8B/snapshots/a80f5e57cce20e57b65145f4213844dec1a80834"
# "/home/qhy/Research/LLM/GRPO-RV/QWEN3-4B"
STAGE1_ADAPTER_PATH="/home/share/ReasoningV/Qwen3/ckpts/sft-qwen3-lora-5epoch-origen200k-S1-20250429_211831/checkpoint-34695/"
# "/home/qhy/Research/LLM/GRPO-RV/GRPO-v4/S1/trainer_output/checkpoint-138795"

# ğŸ”¥ é‡è¦ï¼šä½¿ç”¨å¤„ç†åçš„å¢å¼ºæ•°æ®é›†
# æ ¹æ®ä½ çš„æ•°æ®é›†å¤„ç†æ–¹å¼é€‰æ‹©ä¸€ä¸ªï¼š
DATASET_PATH="/home/qhy/Research/LLM/GRPO-RV/dataset/all-2.jsonl"
DATASET_BASE_PATH=$(dirname "${DATASET_PATH}")

# --- RESUME FROM CHECKPOINT CONFIGURATION ---
# ğŸ”„ è®¾ç½®æ­¤å˜é‡ä¸ºä½ æƒ³è¦ä»ä¸­æ¢å¤çš„ checkpoint ç›®å½•çš„è·¯å¾„
# ä¾‹å¦‚: RESUME_FROM_CHECKPOINT_DIR="/home/qhy/Research/LLM/GRPO-Clean-2/enhanced_grpo_8B_runs/v3-LR6e-6-R16-20250612-175657/checkpoint-540"
# å°†æ­¤ç•™ç©ºä»¥å¼€å§‹æ–°çš„è®­ç»ƒã€‚å°†å…¶è®¾ç½®ä¸ºä¸€ä¸ªä¸å­˜åœ¨çš„è·¯å¾„ä¹Ÿä¼šå¼€å§‹æ–°çš„è®­ç»ƒï¼ˆä¼šæœ‰è­¦å‘Šï¼‰ã€‚
RESUME_FROM_CHECKPOINT_DIR="/home/qhy/Research/LLM/GRPO-Clean-2/enhanced_grpo_v3_runs/v3-_home_qhy_Research_LLM_GRPO-RV_QWEN3-4B-LR1e-5-R64-20250604-232819-2/checkpoint-144"

# ğŸ”§ å…³é”®ï¼šWandBæ¢å¤é…ç½® + é›†æˆcheckpointéªŒè¯
if [ -n "${RESUME_FROM_CHECKPOINT_DIR}" ]; then
    if [ -d "${RESUME_FROM_CHECKPOINT_DIR}" ]; then
        log_info "ğŸ”„ æ£€æµ‹åˆ°checkpointæ¢å¤ï¼Œå¼€å§‹éªŒè¯..."
        
        # é›†æˆcheckpointå®Œæ•´æ€§æ£€æŸ¥
        checkpoint_validation_passed=true
        
        # æ£€æŸ¥å¿…è¦æ–‡ä»¶
        if [ ! -f "${RESUME_FROM_CHECKPOINT_DIR}/trainer_state.json" ]; then
            log_error "âŒ ç¼ºå°‘è®­ç»ƒçŠ¶æ€æ–‡ä»¶: trainer_state.json"
            checkpoint_validation_passed=false
        fi
        

        

        
        # éªŒè¯JSONæ–‡ä»¶æ ¼å¼
        if [ -f "${RESUME_FROM_CHECKPOINT_DIR}/trainer_state.json" ]; then
            if ! ${PYTHON_EXECUTABLE} -c "import json; json.load(open('${RESUME_FROM_CHECKPOINT_DIR}/trainer_state.json'))" 2>/dev/null; then
                log_error "âŒ trainer_state.jsonæ–‡ä»¶æ ¼å¼æŸå"
                checkpoint_validation_passed=false
            fi
        fi
        
        if [ -f "${RESUME_FROM_CHECKPOINT_DIR}/config.json" ]; then
            if ! ${PYTHON_EXECUTABLE} -c "import json; json.load(open('${RESUME_FROM_CHECKPOINT_DIR}/config.json'))" 2>/dev/null; then
                log_error "âŒ config.jsonæ–‡ä»¶æ ¼å¼æŸå"
                checkpoint_validation_passed=false
            fi
        fi
        
        if [ "$checkpoint_validation_passed" = false ]; then
            log_error "ğŸ’¥ CheckpointéªŒè¯å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æ–­ç»­è®­ç»ƒ"
            log_info "ğŸ’¡ å»ºè®®ï¼š"
            log_info "   1. æ£€æŸ¥checkpointç›®å½•æ˜¯å¦å®Œæ•´"
            log_info "   2. å°è¯•ä½¿ç”¨æ›´æ—©çš„checkpoint"
            log_info "   3. æˆ–è€…å¼€å§‹æ–°çš„è®­ç»ƒï¼ˆæ¸…ç©ºRESUME_FROM_CHECKPOINT_DIRï¼‰"
            exit 1
        else
            log_info "âœ… CheckpointéªŒè¯é€šè¿‡"
        fi
        
        echo "ğŸ”„ é…ç½®WandBæ¢å¤..."
        
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨å…¨å±€WandBç›®å½•è€Œä¸æ˜¯checkpointç›®å½•ä¸‹çš„wandb
        GLOBAL_WANDB_DIR="${SCRIPT_DIR}/wandb"
        PARENT_DIR=$(dirname "${RESUME_FROM_CHECKPOINT_DIR}")
        
        # ä»checkpointç›®å½•è·¯å¾„æå–æ—¶é—´æˆ³
        # ä¾‹å¦‚: v3-LR6e-6-R64-20250609-100431 -> 20250609-100431
        CHECKPOINT_DIR_NAME=$(basename "${PARENT_DIR}")
        echo "ğŸ“ æ£€æŸ¥ç‚¹ç›®å½•å: ${CHECKPOINT_DIR_NAME}"
        
        # æå–æ—¶é—´æˆ³ (æ ¼å¼: YYYYMMDD-HHMMSS)
        TIMESTAMP_PATTERN=$(echo "${CHECKPOINT_DIR_NAME}" | grep -o '[0-9]\{8\}-[0-9]\{6\}')
        
        if [ -n "${TIMESTAMP_PATTERN}" ] && [ -d "${GLOBAL_WANDB_DIR}" ]; then
            echo "ğŸ• ä»ç›®å½•åæå–æ—¶é—´æˆ³: ${TIMESTAMP_PATTERN}"
            
            # å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºWandBæ ¼å¼ (YYYYMMDD_HHMMSS)
            WANDB_TIMESTAMP_FORMAT=$(echo "${TIMESTAMP_PATTERN}" | sed 's/-/_/')
            echo "ğŸ” æŸ¥æ‰¾WandB runæ—¶é—´æˆ³: ${WANDB_TIMESTAMP_FORMAT}"
            
            # é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
            MATCHING_RUN_DIR=$(find "${GLOBAL_WANDB_DIR}" -name "run-${WANDB_TIMESTAMP_FORMAT}*" -type d | head -1)
            
            # å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•åŒ¹é…æ—¥æœŸå’Œå°æ—¶åˆ†é’Ÿï¼ˆå…è®¸ç§’æ•°å·®å¼‚ï¼‰
            if [ -z "${MATCHING_RUN_DIR}" ]; then
                # æå–æ—¥æœŸå’Œæ—¶åˆ† (YYYYMMDD_HHMM)
                DATE_HOUR_MIN=$(echo "${WANDB_TIMESTAMP_FORMAT}" | cut -c1-13)
                echo "ğŸ” æ‰©å±•æœç´¢ï¼Œä½¿ç”¨æ—¥æœŸ+æ—¶åˆ†: ${DATE_HOUR_MIN}*"
                MATCHING_RUN_DIR=$(find "${GLOBAL_WANDB_DIR}" -name "run-${DATE_HOUR_MIN}*" -type d | head -1)
            fi
            
            if [ -n "${MATCHING_RUN_DIR}" ]; then
                # æå–run ID (æ ¼å¼: run-20250609_100438-xpnrguty -> xpnrguty)
                RUN_ID=$(basename "${MATCHING_RUN_DIR}" | sed 's/run-[0-9]*_[0-9]*-//')
                if [ -n "${RUN_ID}" ]; then
                    export WANDB_RUN_ID="${RUN_ID}"
                    export WANDB_RESUME="must"
                    echo "âœ… æ‰¾åˆ°åŒ¹é…çš„WandB run: $(basename "${MATCHING_RUN_DIR}")"
                    echo "âœ… æå–åˆ°WandB run ID: ${RUN_ID}"
                    echo "âœ… è®¾ç½®WandBæ¢å¤æ¨¡å¼: must"
                else
                    echo "âš ï¸ æ— æ³•ä»runç›®å½•åæå–run IDï¼Œå°†å°è¯•è‡ªåŠ¨æ¢å¤"
                    export WANDB_RESUME="allow"
                fi
            else
                echo "âš ï¸ æœªæ‰¾åˆ°åŒ¹é…æ—¶é—´æˆ³çš„WandB runç›®å½•ï¼ŒæŸ¥æ‰¾æœ€è¿‘çš„run..."
                # å¤‡é€‰æ–¹æ¡ˆï¼šæŸ¥æ‰¾æœ€æ–°çš„runç›®å½•
                LATEST_RUN_DIR=$(find "${GLOBAL_WANDB_DIR}" -name "run-*" -type d | sort | tail -1)
                if [ -n "${LATEST_RUN_DIR}" ]; then
                    RUN_ID=$(basename "${LATEST_RUN_DIR}" | sed 's/run-[0-9]*_[0-9]*-//')
                    if [ -n "${RUN_ID}" ]; then
                        export WANDB_RUN_ID="${RUN_ID}"
                        export WANDB_RESUME="allow"  # ä½¿ç”¨allowå› ä¸ºæ—¶é—´æˆ³ä¸åŒ¹é…
                        echo "âœ… ä½¿ç”¨æœ€æ–°çš„WandB run: $(basename "${LATEST_RUN_DIR}")"
                        echo "âœ… æå–åˆ°WandB run ID: ${RUN_ID}"
                        echo "âœ… è®¾ç½®WandBæ¢å¤æ¨¡å¼: allow"
                    else
                        echo "âš ï¸ æ— æ³•ä»æœ€æ–°runç›®å½•æå–run IDï¼Œå°†å°è¯•è‡ªåŠ¨æ¢å¤"
                        export WANDB_RESUME="allow"
                    fi
                else
                    echo "âš ï¸ æœªæ‰¾åˆ°ä»»ä½•WandB runç›®å½•ï¼Œå°†å°è¯•è‡ªåŠ¨æ¢å¤"
                    export WANDB_RESUME="allow"
                fi
            fi
        else
            echo "âš ï¸ æ— æ³•ä»ç›®å½•åæå–æ—¶é—´æˆ³æˆ–WandBç›®å½•ä¸å­˜åœ¨ï¼Œå°†å°è¯•è‡ªåŠ¨æ¢å¤"
            export WANDB_RESUME="allow"
        fi
        
        # ä¿®æ”¹è¿è¡Œåç§°ä»¥è¡¨ç¤ºè¿™æ˜¯æ¢å¤çš„è®­ç»ƒ
        if [ -z "${WANDB_RUN_ID}" ]; then
            export WANDB_RUN_NAME="resumed-${WANDB_RUN_NAME}"
        fi
        
        echo "ğŸ”„ WandBæ¢å¤é…ç½®å®Œæˆ:"
        echo "  - WANDB_RUN_ID: ${WANDB_RUN_ID:-'(è‡ªåŠ¨æ£€æµ‹)'}"
        echo "  - WANDB_RESUME: ${WANDB_RESUME}"
        echo "  - WANDB_RUN_NAME: ${WANDB_RUN_NAME}"
        echo "  - å…¨å±€WandBç›®å½•: ${GLOBAL_WANDB_DIR}"
        
    else
        echo "âš ï¸ è­¦å‘Š: RESUME_FROM_CHECKPOINT_DIR ('${RESUME_FROM_CHECKPOINT_DIR}') æŒ‡å®šçš„ç›®å½•ä¸å­˜åœ¨ã€‚å°†å¼€å§‹æ–°çš„è®­ç»ƒï¼Œå¹¶å¿½ç•¥æ­¤è®¾ç½®ã€‚"
        # RESUME_FROM_CHECKPOINT_DIR="/home/qhy/Research/LLM/GRPO-Clean-2/enhanced_grpo_8B_runs/v3-LR6e-6-R16-20250612-175657/checkpoint-540" # æ¸…ç©ºä»¥é¿å…ä¼ é€’æ— æ•ˆè·¯å¾„ç»™Pythonè„šæœ¬
        # ç¡®ä¿ä¸è®¾ç½®æ¢å¤ç›¸å…³çš„ç¯å¢ƒå˜é‡
        unset WANDB_RUN_ID
        unset WANDB_RESUME
    fi
else
    echo "ğŸš€ å¼€å§‹æ–°çš„è®­ç»ƒè¿è¡Œ"
    # ç¡®ä¿ä¸è®¾ç½®æ¢å¤ç›¸å…³çš„ç¯å¢ƒå˜é‡
    unset WANDB_RUN_ID
    unset WANDB_RESUME
fi

# æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶æ˜¯å¦å­˜åœ¨
if [ ! -f "${DATASET_PATH}" ]; then
    echo "âŒ ERROR: Dataset file not found: ${DATASET_PATH}"
    echo ""
    echo "Please process your dataset first using one of these methods:"
    echo ""
    echo "Method 1 - If you have category folders:"
    echo "  python process_category_folders.py --output enhanced_dataset.jsonl"
    echo ""
    echo "Method 2 - If you have 4 separate files:"
    echo "  python merge_four_files.py"
    echo ""
    echo "Method 3 - Auto-detect and process:"
    echo "  python category_merger.py --auto --output enhanced_dataset.jsonl"
    echo ""
    echo "Then update DATASET_PATH in this script to point to the generated file."
    exit 1
fi

OUTPUT_DIR_BASE="./enhanced_grpo_8B_runs"

# --- Enhanced LoRA Configuration ---
LORA_RANK=16         # Increased capacity
LORA_ALPHA=32        # Scaled with rank  
LORA_DROPOUT=0.05
LORA_TARGET_MODULES="q_proj k_proj v_proj o_proj gate_proj up_proj down_proj"

# ğŸ”§ æ–°å¢ï¼šç‹¬ç«‹çš„é•¿åº¦é…ç½®å‚æ•°
# æ€»åºåˆ—é•¿åº¦ï¼ˆä¸å˜ï¼‰
MAX_SEQ_LENGTH=5120

# ğŸ”§ ç‹¬ç«‹é…ç½®promptå’Œcompletioné•¿åº¦
# å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´è¿™äº›å€¼
MAX_PROMPT_LENGTH=1024    # æç¤ºçš„æœ€å¤§é•¿åº¦ (é»˜è®¤: ~37.5%çš„æ€»é•¿åº¦)
MAX_COMPLETION_LENGTH=4096 # è¾“å‡ºçš„æœ€å¤§é•¿åº¦ (é»˜è®¤: ~62.5%çš„æ€»é•¿åº¦)

# ğŸ”§ é•¿åº¦åˆ†é…ç­–ç•¥é€‰æ‹©
# é€‰é¡¹: "balanced" (50/50), "prompt_heavy" (60/40), "completion_heavy" (40/60), "custom" (ä½¿ç”¨ä¸Šé¢çš„è‡ªå®šä¹‰å€¼)
LENGTH_ALLOCATION_STRATEGY="custom"

# ğŸ”§ é•¿åº¦é…ç½®é¢„è®¾æ¨¡æ¿ - å¯ä»¥ç›´æ¥å¤åˆ¶ä½¿ç”¨
# å¦‚æœè¦ä½¿ç”¨é¢„è®¾ï¼Œè¯·å–æ¶ˆæ³¨é‡Šç›¸åº”çš„é…ç½®å¹¶æ³¨é‡Šæ‰ä¸Šé¢çš„è‡ªå®šä¹‰é…ç½®

# é¢„è®¾1: å¹³è¡¡åˆ†é… (é€‚åˆä¸€èˆ¬ç”¨é€”)
# LENGTH_ALLOCATION_STRATEGY="balanced"
# MAX_PROMPT_LENGTH=2048
# MAX_COMPLETION_LENGTH=2048

# é¢„è®¾2: é•¿è¾“å‡ºæ¨¡å¼ (é€‚åˆéœ€è¦ç”Ÿæˆé•¿ä»£ç çš„æƒ…å†µ)
# LENGTH_ALLOCATION_STRATEGY="completion_heavy"
# MAX_PROMPT_LENGTH=1280   # ~31%
# MAX_COMPLETION_LENGTH=2816 # ~69%

# é¢„è®¾3: è¶…é•¿è¾“å‡ºæ¨¡å¼ (é€‚åˆç”Ÿæˆéå¸¸é•¿çš„ä»£ç )
# LENGTH_ALLOCATION_STRATEGY="custom"
# MAX_PROMPT_LENGTH=1024   # ~25%
# MAX_COMPLETION_LENGTH=3072 # ~75%

# é¢„è®¾4: å·¨å‹è¾“å‡ºæ¨¡å¼ (éœ€è¦æ›´å¤§çš„æ€»åºåˆ—é•¿åº¦)
# MAX_SEQ_LENGTH=6144
# LENGTH_ALLOCATION_STRATEGY="custom"
# MAX_PROMPT_LENGTH=1536   # ~25%
# MAX_COMPLETION_LENGTH=4608 # ~75%

# ğŸ”§ éªŒè¯é•¿åº¦é…ç½®
validate_length_config() {
    local total_length=$((MAX_PROMPT_LENGTH + MAX_COMPLETION_LENGTH))
    echo "ğŸ“ é•¿åº¦é…ç½®éªŒè¯:"
    echo "  - æ€»åºåˆ—é•¿åº¦: ${MAX_SEQ_LENGTH}"
    echo "  - æœ€å¤§æç¤ºé•¿åº¦: ${MAX_PROMPT_LENGTH} ($(( MAX_PROMPT_LENGTH * 100 / MAX_SEQ_LENGTH ))%)"
    echo "  - æœ€å¤§è¾“å‡ºé•¿åº¦: ${MAX_COMPLETION_LENGTH} ($(( MAX_COMPLETION_LENGTH * 100 / MAX_SEQ_LENGTH ))%)"
    echo "  - æ€»ä½¿ç”¨é•¿åº¦: ${total_length}"
    echo "  - åˆ†é…ç­–ç•¥: ${LENGTH_ALLOCATION_STRATEGY}"
    
    if [ ${total_length} -gt ${MAX_SEQ_LENGTH} ]; then
        echo "âš ï¸ è­¦å‘Š: æç¤ºé•¿åº¦ + è¾“å‡ºé•¿åº¦ (${total_length}) > æ€»åºåˆ—é•¿åº¦ (${MAX_SEQ_LENGTH})"
        echo "   å»ºè®®è°ƒæ•´é…ç½®æˆ–å¢åŠ MAX_SEQ_LENGTH"
        return 1
    else
        echo "âœ… é•¿åº¦é…ç½®æœ‰æ•ˆ"
        return 0
    fi
}

# --- Enhanced Callback Configuration ---
CALLBACK_NUM_SAMPLES=2
CALLBACK_EVAL_EVERY_N_STEPS=5

# --- Enhanced Generation Parameters ---
GEN_TEMPERATURE=0.8
GEN_TOP_K=50
GEN_TOP_P=0.95
GEN_REPETITION_PENALTY=1.1
GEN_LENGTH_PENALTY=1.0

# --- Enhanced Multi-Objective Reward Configuration ---
# Basic compilation rewards (enhanced)
REWARD_COMPILATION_SUCCESS=2.0
REWARD_COMPILATION_FAILURE=-4.0
REWARD_SIMULATION_CRASH=-4.0
REWARD_OUTPUT_PARSE_ERROR=-2.0
REWARD_MISSING_CODE_BLOCK_PENALTY=-3.0
REWARD_TIMEOUT_PENALTY=-3.0

# Enhanced functional rewards (non-linear)
REWARD_TEST_PASS_BASE=1.5
REWARD_TEST_PASS_BONUS_MULTIPLIER=1.2
REWARD_MAX_FUNCTIONAL=15.0
REWARD_ALL_TESTS_PASSED_BONUS=5.0

# Code quality rewards (new)
REWARD_CODE_EFFICIENCY_BONUS=2.0
REWARD_CODE_READABILITY_BONUS=1.0
REWARD_CODE_COMPLEXITY_PENALTY=-1.0
REWARD_EDGE_CASE_HANDLING_BONUS=1.5
REWARD_SYNTHESIS_FRIENDLY_BONUS=1.0
REWARD_RESOURCE_USAGE_PENALTY=-0.5

# Multi-objective weights
REWARD_FUNCTIONAL_WEIGHT=0.7
REWARD_EFFICIENCY_WEIGHT=0.15
REWARD_READABILITY_WEIGHT=0.1
REWARD_ROBUSTNESS_WEIGHT=0.05

# Adaptive reward scaling
REWARD_ENABLE_ADAPTIVE_SCALING=false
REWARD_SCALE_FACTOR=1.0
REWARD_CLIPPING_RANGE=20.0

# --- Dual-Layer Curriculum Learning Configuration ---
ENABLE_CURRICULUM=true
CURRICULUM_TYPE="dual_layer"

# ğŸ¯ æ ¹æ®ä½ çš„æ•°æ®é›†åˆ†å¸ƒè°ƒæ•´è¿™äº›è®¾ç½®
# è¿è¡Œæ•°æ®é›†å¤„ç†è„šæœ¬åï¼Œå®ƒä¼šç»™å‡ºæ¨èé…ç½®ï¼Œå¤åˆ¶åˆ°è¿™é‡Œ

CURRICULUM_FOCUS_LEVELS="advanced basic intermediate"
CURRICULUM_COMPLEXITY_EMPHASIS="simple"   # é€‰é¡¹: "simple", "balanced", "complex"

# ğŸ”§ æ–°å¢ï¼šè¯¾ç¨‹å­¦ä¹ æ€§èƒ½æ£€æŸ¥é—´éš”é…ç½®
# æ§åˆ¶å¤šå°‘æ­¥æ£€æŸ¥ä¸€æ¬¡æ€§èƒ½å¹¶åˆ¤æ–­æ˜¯å¦å¯ä»¥è¿›é˜¶åˆ°ä¸‹ä¸€é˜¶æ®µ
# è¾ƒå°çš„å€¼(å¦‚5)ï¼šæ›´é¢‘ç¹æ£€æŸ¥ï¼Œå“åº”æ›´å¿«ï¼Œä½†è®¡ç®—å¼€é”€ç¨å¤§
# è¾ƒå¤§çš„å€¼(å¦‚25)ï¼šæ£€æŸ¥è¾ƒå°‘ï¼ŒèŠ‚çœè®¡ç®—ï¼Œä½†å“åº”ç¨æ…¢
CURRICULUM_PERFORMANCE_CHECK_INTERVAL=5   # æ¯5æ­¥æ£€æŸ¥ä¸€æ¬¡ï¼Œæ›´é¢‘ç¹ç›‘æ§

# å¦‚æœä½ çš„æ•°æ®é›†ä¸»è¦æ˜¯åŸºç¡€çº§åˆ«ï¼Œä½¿ç”¨:
# CURRICULUM_FOCUS_LEVELS="basic intermediate"
# CURRICULUM_COMPLEXITY_EMPHASIS="simple"

# å¦‚æœä½ çš„æ•°æ®é›†ä¸»è¦æ˜¯é«˜çº§ï¼Œä½¿ç”¨:
# CURRICULUM_FOCUS_LEVELS="intermediate advanced expert"
# CURRICULUM_COMPLEXITY_EMPHASIS="complex"

# --- Experience Replay Configuration ---
ENABLE_EXPERIENCE_REPLAY=true
EXPERIENCE_BUFFER_SIZE=1000
REPLAY_SAMPLE_RATIO=0.2

# --- Enhanced Training Configuration ---
PER_DEVICE_TRAIN_BATCH_SIZE=1
GRADIENT_ACCUMULATION_STEPS=16
LEARNING_RATE=6e-6               # More conservative
NUM_TRAIN_EPOCHS=4
MAX_STEPS=-1
WARMUP_RATIO=0.15                # Increased warmup
LR_SCHEDULER_TYPE="cosine"
WEIGHT_DECAY=0.01
LOGGING_STRATEGY="steps"
LOGGING_STEPS=2
SAVE_STRATEGY="steps"
SAVE_STEPS=10
SAVE_TOTAL_LIMIT=5
SEED=42

# --- Enhanced GRPO Configuration ---
NUM_GENERATIONS_GRPO=2
# æ³¨æ„: MAX_COMPLETION_LENGTHç°åœ¨ä½¿ç”¨ä¸Šé¢é…ç½®çš„ç‹¬ç«‹å‚æ•°

# --- Enhanced Performance Settings ---
BF16_ENABLED=true
FP16_ENABLED=false
GRADIENT_CHECKPOINTING_ENABLED=false
OPTIMIZER_TYPE="adamw_torch"

# --- Enhanced DataLoader Configuration ---
NUM_CPU_CORES=$(nproc --all 2>/dev/null || echo 4)
DATALOADER_NUM_WORKERS=0
# $((NUM_CPU_CORES / NUM_GPUS_PER_NODE / 2))
# if [ "${DATALOADER_NUM_WORKERS}" -lt "1" ]; then DATALOADER_NUM_WORKERS=1; fi
DATALOADER_PIN_MEMORY=true

# --- FSDP Configuration (disabled by default) ---
FSDP_ENABLED=false

# --- Dynamic W&B Run Name ---
TIMESTAMP=$(date +%Y%m%d-%H%M%S)
MODEL_NAME_SLUG=${BASE_MODEL_NAME_OR_PATH//\//_}
export WANDB_RUN_NAME="v3-LR${LEARNING_RATE}-R${LORA_RANK}-${TIMESTAMP}"

# --- Script Backup Configuration ---
# ä¸ºäº†è®°å½•å®éªŒï¼Œæˆ‘ä»¬ä¼šå°†è®­ç»ƒè„šæœ¬å¤åˆ¶åˆ°è¾“å‡ºç›®å½•
SCRIPT_BACKUP_ENABLED=true
# é™ä½è¯¾ç¨‹å­¦ä¹ çš„æ€§èƒ½é˜ˆå€¼ï¼Œæ›´å®¹æ˜“è¿›é˜¶
CURRICULUM_PERFORMANCE_THRESHOLD_1=0.65  # åŸºç¡€é˜¶æ®µ
CURRICULUM_PERFORMANCE_THRESHOLD_2=0.60  # åˆçº§é˜¶æ®µ
CURRICULUM_PERFORMANCE_THRESHOLD_3=0.55  # ä¸­çº§é˜¶æ®µ
CURRICULUM_PERFORMANCE_THRESHOLD_4=0.50  # é«˜çº§é˜¶æ®µ
CURRICULUM_PERFORMANCE_THRESHOLD_5=0.45  # ä¸“å®¶é˜¶æ®µ

# å‡å°‘æœ€å°è¯„ä¼°æ¬¡æ•°
CURRICULUM_MIN_EVALUATIONS=3

# Qwen3ä¼˜åŒ–çš„ç”Ÿæˆå‚æ•°
GEN_TEMPERATURE=0.7
GEN_TOP_P=0.8
GEN_TOP_K=40
GEN_REPETITION_PENALTY=1.05
# --- Pre-flight Checks ---
echo "========================================================================"
echo "                    ENHANCED GRPO v3 TRAINING"
echo "========================================================================"
echo "Script Directory: ${SCRIPT_DIR}"
echo "Dataset: ${DATASET_PATH}"
echo "Base Model: ${BASE_MODEL_NAME_OR_PATH}"
echo "Stage 1 Adapters: ${STAGE1_ADAPTER_PATH}"
if [ -n "${RESUME_FROM_CHECKPOINT_DIR}" ]; then # å†æ¬¡æ£€æŸ¥ï¼Œå› ä¸ºå¯èƒ½åœ¨ä¸Šé¢è¢«æ¸…ç©º
    echo "ğŸ”„ å°†ä» Checkpoint æ¢å¤è®­ç»ƒ: ${RESUME_FROM_CHECKPOINT_DIR}"
else
    echo "ğŸš€ å¼€å§‹æ–°çš„è®­ç»ƒè¿è¡Œã€‚"
fi
echo "Output Directory Base: ${OUTPUT_DIR_BASE}" # æ³¨æ„: è„šæœ¬ä¸­ OUTPUT_DIR_BASE çš„å£°æ˜åœ¨å…¶é¦–æ¬¡ä½¿ç”¨ä¹‹åï¼Œå»ºè®®ç§»åˆ°å‰é¢

# ğŸ”§ éªŒè¯é•¿åº¦é…ç½®
echo ""
validate_length_config
if [ $? -ne 0 ]; then
    echo "âŒ é•¿åº¦é…ç½®éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®åé‡æ–°è¿è¡Œ"
    exit 1
fi
echo ""

# Check dataset format
echo "ğŸ” Checking dataset format..."
FIRST_LINE=$(head -1 "${DATASET_PATH}")
if echo "${FIRST_LINE}" | grep -q '"level"' && echo "${FIRST_LINE}" | grep -q '"complexity_score"'; then
    echo "âœ… Detected enhanced dataset format with level and complexity_score"
    
    # Extract some statistics
    echo "ğŸ“Š Dataset preview:"
    python3 -c "
import json
data = []
with open('${DATASET_PATH}') as f:
    for i, line in enumerate(f):
        if i >= 100: break  # Sample first 100 lines
        data.append(json.loads(line))

if data:
    levels = [d.get('level', 'unknown') for d in data]
    complexities = [d.get('complexity_score', 0) for d in data]
    categories = [d.get('category', 'Unknown') for d in data]
    
    print(f'  ğŸ“ˆ Sample size: {len(data)} (from first 100 lines)')
    
    print(f'  ğŸ“Š Level distribution:')
    for level in sorted(set(levels)):
        count = levels.count(level)
        print(f'    {level}: {count} ({count/len(levels)*100:.1f}%)')
    
    if complexities:
        print(f'  ğŸ§® Complexity range: {min(complexities):.1f} - {max(complexities):.1f}')
    
    print(f'  ğŸ“‚ Categories: {len(set(categories))} unique')
    for cat in sorted(set(categories))[:5]:  # Show top 5
        count = categories.count(cat)
        print(f'    {cat}: {count}')
    if len(set(categories)) > 5:
        print(f'    ... and {len(set(categories)) - 5} more categories')
"
else
    echo "âš ï¸  WARNING: Dataset may be in legacy format (missing level/complexity_score)"
    echo "   The training script will attempt to auto-upgrade the dataset"
    echo "   For best results, consider processing your dataset first:"
    echo "   python process_category_folders.py ${DATASET_PATH} --output enhanced_${DATASET_PATH}"
fi

echo ""

# Check file paths in dataset
echo "ğŸ” Checking file paths in dataset..."
python3 -c "
import json
import os

with open('${DATASET_PATH}') as f:
    sample = json.loads(f.readline())

ref_path = sample.get('reference_verilog_path', '')
tb_path = sample.get('testbench_path', '')

print(f'  ğŸ“„ Sample reference path: {ref_path}')
print(f'  ğŸ“„ Sample testbench path: {tb_path}')

ref_exists = os.path.exists(ref_path)
tb_exists = os.path.exists(tb_path)

print(f'  âœ… Reference file exists: {ref_exists}')
print(f'  âœ… Testbench file exists: {tb_exists}')

if not ref_exists and not tb_exists:
    print('  âš ï¸  WARNING: Sample files not found. Check your working directory.')
    print('     Make sure you are running from the correct directory where the')
    print('     category folders or reference files are located.')
"

echo ""
echo "Enhanced Features Summary:"
echo "  âœ… Multi-objective reward system with detailed component tracking"
echo "  âœ… Dual-layer curriculum learning: ${CURRICULUM_TYPE} (${CURRICULUM_FOCUS_LEVELS})"
echo "  âœ… Complexity emphasis: ${CURRICULUM_COMPLEXITY_EMPHASIS}"
echo "  âœ… Curriculum performance check interval: every ${CURRICULUM_PERFORMANCE_CHECK_INTERVAL} steps"
echo "  âœ… Experience replay: ${ENABLE_EXPERIENCE_REPLAY} (buffer size: ${EXPERIENCE_BUFFER_SIZE})"
echo "  âœ… Adaptive reward scaling: ${REWARD_ENABLE_ADAPTIVE_SCALING}"
echo "  âœ… Enhanced LoRA: rank=${LORA_RANK}, alpha=${LORA_ALPHA}"
echo "  âœ… Conservative learning: lr=${LEARNING_RATE}, warmup=${WARMUP_RATIO}"
echo "  âœ… Generation diversity: temp=${GEN_TEMPERATURE}, rep_penalty=${GEN_REPETITION_PENALTY}"
echo "  âœ… Enhanced monitoring: ${CALLBACK_NUM_SAMPLES} samples every ${CALLBACK_EVAL_EVERY_N_STEPS} steps"

echo ""
mkdir -p ${OUTPUT_DIR_BASE}

# --- Script Backup Function ---
backup_training_scripts() {
    local output_dir="$1"
    if [ "$SCRIPT_BACKUP_ENABLED" = true ] && [ -n "$output_dir" ]; then
        echo "ğŸ“„ æ­£åœ¨å¤‡ä»½è®­ç»ƒè„šæœ¬åˆ°å®éªŒæ–‡ä»¶å¤¹..."
        
        # åˆ›å»ºè„šæœ¬å¤‡ä»½ç›®å½•
        local script_backup_dir="${output_dir}/training_scripts_backup"
        mkdir -p "$script_backup_dir"
        
        # å¤åˆ¶ä¸»è¦è®­ç»ƒè„šæœ¬
        if [ -f "${BASH_SOURCE[0]}" ]; then
            cp "${BASH_SOURCE[0]}" "$script_backup_dir/"
            echo "  âœ… å¤åˆ¶è®­ç»ƒè„šæœ¬: $(basename ${BASH_SOURCE[0]})"
        fi
        
        # å¤åˆ¶Pythonä¸»è„šæœ¬
        if [ -f "$PYTHON_SCRIPT_TO_RUN" ]; then
            cp "$PYTHON_SCRIPT_TO_RUN" "$script_backup_dir/"
            echo "  âœ… å¤åˆ¶Pythonè„šæœ¬: $(basename $PYTHON_SCRIPT_TO_RUN)"
        fi
        
        # å¤åˆ¶å…¶ä»–ç›¸å…³Pythonæ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        for py_file in "${SCRIPT_DIR}"/*.py; do
            if [ -f "$py_file" ] && [ "$py_file" != "$PYTHON_SCRIPT_TO_RUN" ]; then
                cp "$py_file" "$script_backup_dir/"
                echo "  âœ… å¤åˆ¶ç›¸å…³è„šæœ¬: $(basename $py_file)"
            fi
        done
        
        # ç‰¹åˆ«å¤åˆ¶WandBæ­¥æ•°ä¿®å¤æ¨¡å—
        if [ -f "${SCRIPT_DIR}/wandb_step_fix.py" ]; then
            cp "${SCRIPT_DIR}/wandb_step_fix.py" "$script_backup_dir/"
            echo "  âœ… å¤åˆ¶WandBä¿®å¤æ¨¡å—: wandb_step_fix.py"
        fi
        
        # ä¿å­˜è®­ç»ƒå‚æ•°åˆ°é…ç½®æ–‡ä»¶
        local config_file="${script_backup_dir}/training_config_${TIMESTAMP}.txt"
        cat > "$config_file" << EOF
# Enhanced GRPO v3 Training Configuration
# Generated at: $(date)
# Script: ${BASH_SOURCE[0]}

## Model Configuration
BASE_MODEL_NAME_OR_PATH="${BASE_MODEL_NAME_OR_PATH}"
STAGE1_ADAPTER_PATH="${STAGE1_ADAPTER_PATH}"
DATASET_PATH="${DATASET_PATH}"

## LoRA Configuration
LORA_RANK=${LORA_RANK}
LORA_ALPHA=${LORA_ALPHA}
LORA_DROPOUT=${LORA_DROPOUT}
LORA_TARGET_MODULES="${LORA_TARGET_MODULES}"

## Training Configuration
PER_DEVICE_TRAIN_BATCH_SIZE=${PER_DEVICE_TRAIN_BATCH_SIZE}
GRADIENT_ACCUMULATION_STEPS=${GRADIENT_ACCUMULATION_STEPS}
LEARNING_RATE=${LEARNING_RATE}
NUM_TRAIN_EPOCHS=${NUM_TRAIN_EPOCHS}
MAX_STEPS=${MAX_STEPS}
WARMUP_RATIO=${WARMUP_RATIO}
LR_SCHEDULER_TYPE="${LR_SCHEDULER_TYPE}"
WEIGHT_DECAY=${WEIGHT_DECAY}

## GRPO Configuration
NUM_GENERATIONS_GRPO=${NUM_GENERATIONS_GRPO}
MAX_COMPLETION_LENGTH_GRPO=${MAX_COMPLETION_LENGTH_GRPO}

## Curriculum Learning
ENABLE_CURRICULUM=${ENABLE_CURRICULUM}
CURRICULUM_TYPE="${CURRICULUM_TYPE}"
CURRICULUM_FOCUS_LEVELS="${CURRICULUM_FOCUS_LEVELS}"
CURRICULUM_COMPLEXITY_EMPHASIS="${CURRICULUM_COMPLEXITY_EMPHASIS}"
CURRICULUM_PERFORMANCE_CHECK_INTERVAL=${CURRICULUM_PERFORMANCE_CHECK_INTERVAL}

## Experience Replay
ENABLE_EXPERIENCE_REPLAY=${ENABLE_EXPERIENCE_REPLAY}
EXPERIENCE_BUFFER_SIZE=${EXPERIENCE_BUFFER_SIZE}
REPLAY_SAMPLE_RATIO=${REPLAY_SAMPLE_RATIO}

## Generation Parameters
GEN_TEMPERATURE=${GEN_TEMPERATURE}
GEN_TOP_K=${GEN_TOP_K}
GEN_TOP_P=${GEN_TOP_P}
GEN_REPETITION_PENALTY=${GEN_REPETITION_PENALTY}

## WandB Configuration
WANDB_PROJECT="${WANDB_PROJECT}"
WANDB_ENTITY="${WANDB_ENTITY}"
WANDB_RUN_NAME="${WANDB_RUN_NAME}"
WANDB_STEP_FIX_ENABLED="${WANDB_STEP_FIX_ENABLED}"

## Resume Configuration
RESUME_FROM_CHECKPOINT_DIR="/home/qhy/Research/LLM/GRPO-Clean-2/enhanced_grpo_8B_runs/v3-LR6e-6-R16-20250612-175657/checkpoint-540"

## Runtime Information
TIMESTAMP=${TIMESTAMP}
NUM_GPUS_PER_NODE=${NUM_GPUS_PER_NODE}
DATALOADER_NUM_WORKERS=${DATALOADER_NUM_WORKERS}
EOF
        
        echo "  âœ… ä¿å­˜è®­ç»ƒé…ç½®: $(basename $config_file)"
        
        # ä¿å­˜å®Œæ•´çš„å‘½ä»¤è¡Œ
        echo "$FULL_CMD" > "${script_backup_dir}/full_command_${TIMESTAMP}.txt"
        echo "  âœ… ä¿å­˜å®Œæ•´å‘½ä»¤: full_command_${TIMESTAMP}.txt"
        
        # åˆ›å»ºå®éªŒè¯´æ˜æ–‡ä»¶
        local experiment_info="${script_backup_dir}/experiment_info_${TIMESTAMP}.md"
        cat > "$experiment_info" << EOF
# Enhanced GRPO v3 Training Experiment

## å®éªŒä¿¡æ¯
- **å¼€å§‹æ—¶é—´**: $(date)
- **å®éªŒåç§°**: ${WANDB_RUN_NAME}
- **è„šæœ¬ç‰ˆæœ¬**: Enhanced GRPO v3
- **æ•°æ®é›†**: ${DATASET_PATH}
- **åŸºç¡€æ¨¡å‹**: ${BASE_MODEL_NAME_OR_PATH}

## å…³é”®é…ç½®
- **å­¦ä¹ ç‡**: ${LEARNING_RATE}
- **LoRA Rank**: ${LORA_RANK}
- **æ‰¹æ¬¡å¤§å°**: ${PER_DEVICE_TRAIN_BATCH_SIZE}
- **æ¢¯åº¦ç´¯ç§¯æ­¥æ•°**: ${GRADIENT_ACCUMULATION_STEPS}
- **è¯¾ç¨‹å­¦ä¹ **: ${ENABLE_CURRICULUM}
- **ç»éªŒå›æ”¾**: ${ENABLE_EXPERIENCE_REPLAY}
- **WandBæ­¥æ•°ä¿®å¤**: ${WANDB_STEP_FIX_ENABLED}

## æ–‡ä»¶è¯´æ˜
- \`$(basename ${BASH_SOURCE[0]})\`: è®­ç»ƒå¯åŠ¨è„šæœ¬
- \`$(basename $PYTHON_SCRIPT_TO_RUN)\`: Pythonä¸»è®­ç»ƒè„šæœ¬  
- \`training_config_${TIMESTAMP}.txt\`: è¯¦ç»†è®­ç»ƒå‚æ•°
- \`full_command_${TIMESTAMP}.txt\`: å®Œæ•´æ‰§è¡Œå‘½ä»¤
- \`experiment_info_${TIMESTAMP}.md\`: æœ¬å®éªŒè¯´æ˜æ–‡ä»¶

**æ³¨æ„**: æ‰€æœ‰è®­ç»ƒè„šæœ¬å·²è‡ªåŠ¨å¤‡ä»½åˆ°ä¸æ¨¡å‹æƒé‡ç›¸åŒçš„ç›®å½•ä¸­ï¼Œä¾¿äºè¿½æº¯å’Œå¤ç°å®éªŒã€‚

## ç›‘æ§é“¾æ¥
- **WandBé¡¹ç›®**: https://wandb.ai/${WANDB_ENTITY}/${WANDB_PROJECT}
- **è¿è¡Œé¡µé¢**: https://wandb.ai/${WANDB_ENTITY}/${WANDB_PROJECT}/runs/${WANDB_RUN_NAME}

## æŠ€æœ¯è¯´æ˜

### WandBæ­¥æ•°åŒæ­¥ä¿®å¤
æœ¬å®éªŒä½¿ç”¨äº†WandBæ­¥æ•°åŒæ­¥ä¿®å¤æ¨¡å— (\`wandb_step_fix.py\`)ï¼Œè§£å†³äº†ä»¥ä¸‹é—®é¢˜ï¼š
1. **å¤šå›è°ƒæ—¥å¿—å†²çª**: ä¸åŒçš„è®­ç»ƒå›è°ƒå¯èƒ½åœ¨ä¸åŒæ—¶é—´è®°å½•åŒä¸€æ­¥æ•°çš„æŒ‡æ ‡
2. **æ­¥æ•°ä¸ä¸€è‡´**: è®­ç»ƒå™¨çš„global_stepä¸å›è°ƒå†…éƒ¨è®¡æ•°å™¨ä¸åŒæ­¥
3. **å¼‚æ­¥æ—¥å¿—ä¹±åº**: æŸäº›æŒ‡æ ‡å¯èƒ½å¼‚æ­¥è®°å½•ï¼Œå¯¼è‡´æ­¥æ•°å€’é€€è­¦å‘Š

ä¿®å¤ç­–ç•¥ï¼š
- ç»Ÿä¸€æ­¥æ•°ç®¡ç†å™¨ç¡®ä¿æ‰€æœ‰æ—¥å¿—ä½¿ç”¨ä¸€è‡´çš„æ­¥æ•°
- æŒ‰ä¼˜å…ˆçº§ç¼“å†²æ—¥å¿—ï¼Œé¿å…å†²çª
- æ‰¹é‡æäº¤å‡å°‘WandBæœåŠ¡å™¨å‹åŠ›

å¦‚æœä»ç„¶çœ‹åˆ°æ­¥æ•°è­¦å‘Šï¼Œè¿™æ˜¯æ­£å¸¸çš„ - ä¿®å¤æ¨¡å—ä¼šè‡ªåŠ¨å¤„ç†è¿™äº›å†²çªã€‚

## å¤‡æ³¨
è¯·åœ¨è®­ç»ƒå®Œæˆåæ›´æ–°æ­¤æ–‡ä»¶ï¼Œè®°å½•å®éªŒç»“æœå’Œå‘ç°ã€‚
EOF
        
        echo "  âœ… åˆ›å»ºå®éªŒè¯´æ˜: $(basename $experiment_info)"
        echo "ğŸ“„ è„šæœ¬å¤‡ä»½å®Œæˆï¼Œä¿å­˜åˆ°: $script_backup_dir"
        
        return 0
    else
        echo "â­ï¸  è·³è¿‡è„šæœ¬å¤‡ä»½ (SCRIPT_BACKUP_ENABLED=${SCRIPT_BACKUP_ENABLED})"
        return 1
    fi
}

# --- Build Command Arguments ---
CMD_ARGS=""

# EnvConfig
CMD_ARGS="${CMD_ARGS} --hf_endpoint \"https://hf-mirror.com\""
CMD_ARGS="${CMD_ARGS} --http_proxy \"http://10.130.148.206:7890\""
CMD_ARGS="${CMD_ARGS} --https_proxy \"http://10.130.148.206:7890\""
CMD_ARGS="${CMD_ARGS} --wandb_project \"${WANDB_PROJECT}\""
CMD_ARGS="${CMD_ARGS} --wandb_entity \"${WANDB_ENTITY}\""
CMD_ARGS="${CMD_ARGS} --wandb_run_name_prefix \"enhanced-v3-${MODEL_NAME_SLUG}\""

# Enhanced ScriptConfig
CMD_ARGS="${CMD_ARGS} --model_name_or_path \"${BASE_MODEL_NAME_OR_PATH}\""
CMD_ARGS="${CMD_ARGS} --stage1_adapter_path \"${STAGE1_ADAPTER_PATH}\""
CMD_ARGS="${CMD_ARGS} --dataset_path \"${DATASET_PATH}\""
CMD_ARGS="${CMD_ARGS} --dataset_base_path "${DATASET_BASE_PATH}""
CMD_ARGS="${CMD_ARGS} --output_dir_base \"${OUTPUT_DIR_BASE}\""
CMD_ARGS="${CMD_ARGS} --lora_rank ${LORA_RANK}"
CMD_ARGS="${CMD_ARGS} --lora_alpha ${LORA_ALPHA}"
CMD_ARGS="${CMD_ARGS} --lora_dropout ${LORA_DROPOUT}"
if [ -n "${LORA_TARGET_MODULES}" ]; then
    CMD_ARGS="${CMD_ARGS} --lora_target_modules ${LORA_TARGET_MODULES}"
fi
CMD_ARGS="${CMD_ARGS} --max_seq_length ${MAX_SEQ_LENGTH}"
CMD_ARGS="${CMD_ARGS} --script_max_prompt_length ${MAX_PROMPT_LENGTH}"
CMD_ARGS="${CMD_ARGS} --script_max_completion_length ${MAX_COMPLETION_LENGTH}"
CMD_ARGS="${CMD_ARGS} --length_allocation_strategy ${LENGTH_ALLOCATION_STRATEGY}"
CMD_ARGS="${CMD_ARGS} --callback_num_samples ${CALLBACK_NUM_SAMPLES}"
CMD_ARGS="${CMD_ARGS} --callback_eval_every_n_steps ${CALLBACK_EVAL_EVERY_N_STEPS}"
CMD_ARGS="${CMD_ARGS} --gen_temperature ${GEN_TEMPERATURE}"
CMD_ARGS="${CMD_ARGS} --gen_top_k ${GEN_TOP_K}"
CMD_ARGS="${CMD_ARGS} --gen_top_p ${GEN_TOP_P}"
CMD_ARGS="${CMD_ARGS} --gen_repetition_penalty ${GEN_REPETITION_PENALTY}"
CMD_ARGS="${CMD_ARGS} --gen_length_penalty ${GEN_LENGTH_PENALTY}"

# Enhanced Curriculum Learning
CMD_ARGS="${CMD_ARGS} --enable_curriculum ${ENABLE_CURRICULUM}"
CMD_ARGS="${CMD_ARGS} --curriculum_type ${CURRICULUM_TYPE}"
CMD_ARGS="${CMD_ARGS} --curriculum_focus_levels ${CURRICULUM_FOCUS_LEVELS}"
CMD_ARGS="${CMD_ARGS} --curriculum_complexity_emphasis ${CURRICULUM_COMPLEXITY_EMPHASIS}"
CMD_ARGS="${CMD_ARGS} --curriculum_performance_check_interval ${CURRICULUM_PERFORMANCE_CHECK_INTERVAL}"

# Experience Replay
CMD_ARGS="${CMD_ARGS} --enable_experience_replay ${ENABLE_EXPERIENCE_REPLAY}"
CMD_ARGS="${CMD_ARGS} --experience_buffer_size ${EXPERIENCE_BUFFER_SIZE}"
CMD_ARGS="${CMD_ARGS} --replay_sample_ratio ${REPLAY_SAMPLE_RATIO}"

# Enhanced RewardConfig
CMD_ARGS="${CMD_ARGS} --compilation_success ${REWARD_COMPILATION_SUCCESS}"
CMD_ARGS="${CMD_ARGS} --compilation_failure ${REWARD_COMPILATION_FAILURE}"
CMD_ARGS="${CMD_ARGS} --simulation_crash ${REWARD_SIMULATION_CRASH}"
CMD_ARGS="${CMD_ARGS} --output_parse_error ${REWARD_OUTPUT_PARSE_ERROR}"
CMD_ARGS="${CMD_ARGS} --missing_code_block_penalty ${REWARD_MISSING_CODE_BLOCK_PENALTY}"
CMD_ARGS="${CMD_ARGS} --timeout_penalty ${REWARD_TIMEOUT_PENALTY}"

# Enhanced functional rewards
CMD_ARGS="${CMD_ARGS} --test_pass_base_reward ${REWARD_TEST_PASS_BASE}"
CMD_ARGS="${CMD_ARGS} --test_pass_bonus_multiplier ${REWARD_TEST_PASS_BONUS_MULTIPLIER}"
CMD_ARGS="${CMD_ARGS} --max_functional_reward ${REWARD_MAX_FUNCTIONAL}"
CMD_ARGS="${CMD_ARGS} --all_tests_passed_bonus ${REWARD_ALL_TESTS_PASSED_BONUS}"

# Code quality rewards
CMD_ARGS="${CMD_ARGS} --code_efficiency_bonus ${REWARD_CODE_EFFICIENCY_BONUS}"
CMD_ARGS="${CMD_ARGS} --code_readability_bonus ${REWARD_CODE_READABILITY_BONUS}"
CMD_ARGS="${CMD_ARGS} --code_complexity_penalty ${REWARD_CODE_COMPLEXITY_PENALTY}"
CMD_ARGS="${CMD_ARGS} --edge_case_handling_bonus ${REWARD_EDGE_CASE_HANDLING_BONUS}"
CMD_ARGS="${CMD_ARGS} --synthesis_friendly_bonus ${REWARD_SYNTHESIS_FRIENDLY_BONUS}"
CMD_ARGS="${CMD_ARGS} --resource_usage_penalty ${REWARD_RESOURCE_USAGE_PENALTY}"

# Multi-objective weights
CMD_ARGS="${CMD_ARGS} --functional_weight ${REWARD_FUNCTIONAL_WEIGHT}"
CMD_ARGS="${CMD_ARGS} --efficiency_weight ${REWARD_EFFICIENCY_WEIGHT}"
CMD_ARGS="${CMD_ARGS} --readability_weight ${REWARD_READABILITY_WEIGHT}"
CMD_ARGS="${CMD_ARGS} --robustness_weight ${REWARD_ROBUSTNESS_WEIGHT}"

# Adaptive scaling
CMD_ARGS="${CMD_ARGS} --enable_adaptive_scaling ${REWARD_ENABLE_ADAPTIVE_SCALING}"
CMD_ARGS="${CMD_ARGS} --reward_scale_factor ${REWARD_SCALE_FACTOR}"
CMD_ARGS="${CMD_ARGS} --reward_clipping_range ${REWARD_CLIPPING_RANGE}"

# Enhanced GRPOConfig (TrainingArguments)

CMD_ARGS="${CMD_ARGS} --per_device_train_batch_size ${PER_DEVICE_TRAIN_BATCH_SIZE}"
CMD_ARGS="${CMD_ARGS} --gradient_accumulation_steps ${GRADIENT_ACCUMULATION_STEPS}"
CMD_ARGS="${CMD_ARGS} --learning_rate ${LEARNING_RATE}"
CMD_ARGS="${CMD_ARGS} --num_train_epochs ${NUM_TRAIN_EPOCHS}"
if [ ${MAX_STEPS} -gt 0 ]; then
    CMD_ARGS="${CMD_ARGS} --max_steps ${MAX_STEPS}"
fi
CMD_ARGS="${CMD_ARGS} --warmup_ratio ${WARMUP_RATIO}"
CMD_ARGS="${CMD_ARGS} --lr_scheduler_type \"${LR_SCHEDULER_TYPE}\""
CMD_ARGS="${CMD_ARGS} --weight_decay ${WEIGHT_DECAY}"
CMD_ARGS="${CMD_ARGS} --logging_strategy \"${LOGGING_STRATEGY}\""
CMD_ARGS="${CMD_ARGS} --logging_steps ${LOGGING_STEPS}"
CMD_ARGS="${CMD_ARGS} --save_strategy \"${SAVE_STRATEGY}\""
CMD_ARGS="${CMD_ARGS} --save_steps ${SAVE_STEPS}"
CMD_ARGS="${CMD_ARGS} --save_total_limit ${SAVE_TOTAL_LIMIT}"
CMD_ARGS="${CMD_ARGS} --report_to \"wandb\""
CMD_ARGS="${CMD_ARGS} --remove_unused_columns False"
CMD_ARGS="${CMD_ARGS} --num_generations ${NUM_GENERATIONS_GRPO}"
CMD_ARGS="${CMD_ARGS} --max_prompt_length ${MAX_PROMPT_LENGTH}"  # GRPOå‚æ•°ï¼šæœ€å¤§æç¤ºé•¿åº¦
CMD_ARGS="${CMD_ARGS} --max_completion_length ${MAX_COMPLETION_LENGTH}"  # GRPOå‚æ•°ï¼šæœ€å¤§å®Œæˆé•¿åº¦
CMD_ARGS="${CMD_ARGS} --optim \"${OPTIMIZER_TYPE}\""
CMD_ARGS="${CMD_ARGS} --ddp_find_unused_parameters False"
CMD_ARGS="${CMD_ARGS} --seed ${SEED}"
CMD_ARGS="${CMD_ARGS} --dataloader_num_workers ${DATALOADER_NUM_WORKERS}"
if [ "$DATALOADER_PIN_MEMORY" = true ]; then
    CMD_ARGS="${CMD_ARGS} --dataloader_pin_memory"
fi
# åœ¨æ„å»ºå…¶ä»–å‚æ•°ä¹‹åï¼Œæ·»åŠ  resume_from_checkpoint å‚æ•°
if [ -n "${RESUME_FROM_CHECKPOINT_DIR}" ] && [ -d "${RESUME_FROM_CHECKPOINT_DIR}" ]; then
    CMD_ARGS="${CMD_ARGS} --resume_from_checkpoint \"${RESUME_FROM_CHECKPOINT_DIR}\""
fi

# Enhanced performance settings
if [ "$BF16_ENABLED" = true ]; then CMD_ARGS="${CMD_ARGS} --bf16"; fi
if [ "$FP16_ENABLED" = true ] && [ "$BF16_ENABLED" = false ]; then CMD_ARGS="${CMD_ARGS} --fp16"; fi
if [ "$GRADIENT_CHECKPOINTING_ENABLED" = true ]; then
    CMD_ARGS="${CMD_ARGS} --gradient_checkpointing"
fi

# Enhanced cache directory setup
CACHE_DIR_BASE="${SCRIPT_DIR}/.enhanced_cache_v2"
mkdir -p "${CACHE_DIR_BASE}/datasets"
mkdir -p "${CACHE_DIR_BASE}/models"
CMD_ARGS="${CMD_ARGS} --cache_dir \"${CACHE_DIR_BASE}/models\""

# --- Enhanced Training Execution ---
LAUNCHER=""
PYTHON_SCRIPT_TO_RUN="${SCRIPT_DIR}/main.py" # Updated to use main.py

if [ ${NUM_GPUS_PER_NODE} -gt 1 ] || [ "$FSDP_ENABLED" = true ]; then
  LAUNCHER="torchrun \
    --nproc_per_node ${NUM_GPUS_PER_NODE} \
    --master_addr ${MASTER_ADDR} \
    --master_port ${MASTER_PORT}"
  
  if [ "$FSDP_ENABLED" = true ]; then
    CMD_ARGS="${CMD_ARGS} --fsdp \"full_shard\""
  fi
  FULL_CMD="${LAUNCHER} ${PYTHON_SCRIPT_TO_RUN} ${CMD_ARGS}"
else
  FULL_CMD="${PYTHON_EXECUTABLE} ${PYTHON_SCRIPT_TO_RUN} ${CMD_ARGS}"
fi

echo "========================================================================"
echo "                        STARTING ENHANCED TRAINING"
echo "========================================================================"

# é›†æˆçš„è®­ç»ƒçŠ¶æ€æ‘˜è¦
log_info "ğŸ¯ è®­ç»ƒé…ç½®æ‘˜è¦ï¼š"
log_info "   æ¨¡å‹: $(basename "${BASE_MODEL_NAME_OR_PATH}")"
log_info "   æ•°æ®é›†: $(basename "${DATASET_PATH}")"
log_info "   æœ€å¤§åºåˆ—é•¿åº¦: ${MAX_SEQ_LENGTH}"
log_info "   Prompté•¿åº¦: ${MAX_PROMPT_LENGTH} ($(( MAX_PROMPT_LENGTH * 100 / MAX_SEQ_LENGTH ))%)"
log_info "   Completioné•¿åº¦: ${MAX_COMPLETION_LENGTH} ($(( MAX_COMPLETION_LENGTH * 100 / MAX_SEQ_LENGTH ))%)"
log_info "   LoRAé…ç½®: rank=${LORA_RANK}, alpha=${LORA_ALPHA}, dropout=${LORA_DROPOUT}"

if [ -n "${RESUME_FROM_CHECKPOINT_DIR}" ] && [ -d "${RESUME_FROM_CHECKPOINT_DIR}" ]; then
    log_info "   è®­ç»ƒæ¨¡å¼: ğŸ”„ æ–­ç»­è®­ç»ƒ (ä» $(basename "${RESUME_FROM_CHECKPOINT_DIR}") æ¢å¤)"
    if [ -n "${WANDB_RUN_ID}" ]; then
        log_info "   WandBæ¢å¤: âœ… Run ID: ${WANDB_RUN_ID}"
    else
        log_info "   WandBæ¢å¤: âš ï¸ è‡ªåŠ¨æ£€æµ‹æ¨¡å¼"
    fi
else
    log_info "   è®­ç»ƒæ¨¡å¼: ğŸ†• æ–°è®­ç»ƒ"
fi

log_info "   è¾“å‡ºç›®å½•: ${OUTPUT_DIR_BASE}"
log_info "   WandBé¡¹ç›®: ${WANDB_PROJECT}"

echo ""
echo "Training will begin with curriculum stage 0..."
echo "Monitor progress at: https://wandb.ai/${WANDB_ENTITY}/${WANDB_PROJECT}"
echo "========================================================================"

# Save full command to file for debugging
echo "${FULL_CMD}" > "${OUTPUT_DIR_BASE}/full_training_command.txt"
echo "Full command saved to: ${OUTPUT_DIR_BASE}/full_training_command.txt"

# --- Backup Training Scripts (Dynamic) ---
# åˆ›å»ºä¸€ä¸ªå»¶è¿Ÿå¤‡ä»½å‡½æ•°ï¼Œåœ¨è®­ç»ƒå¼€å§‹åæŸ¥æ‰¾å®é™…è¾“å‡ºç›®å½•
backup_scripts_to_model_dir() {
    echo "ğŸ” æ­£åœ¨æŸ¥æ‰¾å®é™…çš„æ¨¡å‹ä¿å­˜ç›®å½•..."
    
    # ç­‰å¾…ä¸€å°æ®µæ—¶é—´è®©Pythonè„šæœ¬åˆ›å»ºç›®å½•
    sleep 5
    
    # æŸ¥æ‰¾æœ€æ–°åˆ›å»ºçš„è®­ç»ƒç›®å½•
    ACTUAL_OUTPUT_DIR=""
    if [ -d "${OUTPUT_DIR_BASE}" ]; then
        # æŸ¥æ‰¾åŒ…å«å½“å‰æ—¶é—´æˆ³çš„ç›®å½•ï¼ˆæœ€è¿‘5åˆ†é’Ÿå†…åˆ›å»ºçš„ï¼‰
        ACTUAL_OUTPUT_DIR=$(find "${OUTPUT_DIR_BASE}" -maxdepth 1 -type d -name "*${TIMESTAMP}*" -newermt "5 minutes ago" | head -1)
        
        # å¦‚æœæ²¡æ‰¾åˆ°å¸¦æ—¶é—´æˆ³çš„ï¼ŒæŸ¥æ‰¾æœ€æ–°çš„ç›®å½•
        if [ -z "${ACTUAL_OUTPUT_DIR}" ]; then
            ACTUAL_OUTPUT_DIR=$(find "${OUTPUT_DIR_BASE}" -maxdepth 1 -type d -newer "${OUTPUT_DIR_BASE}/full_training_command.txt" 2>/dev/null | head -1)
        fi
        
        # æœ€åå¤‡é€‰ï¼šæŸ¥æ‰¾æœ€æ–°ä¿®æ”¹çš„ç›®å½•
        if [ -z "${ACTUAL_OUTPUT_DIR}" ]; then
            ACTUAL_OUTPUT_DIR=$(ls -dt "${OUTPUT_DIR_BASE}"/*/ 2>/dev/null | head -1 | sed 's/\/$//')
        fi
    fi
    
    if [ -n "${ACTUAL_OUTPUT_DIR}" ] && [ -d "${ACTUAL_OUTPUT_DIR}" ]; then
        echo "âœ… æ‰¾åˆ°å®é™…è¾“å‡ºç›®å½•: ${ACTUAL_OUTPUT_DIR}"
        backup_training_scripts "${ACTUAL_OUTPUT_DIR}"
        
        # åˆ›å»ºä¸€ä¸ªç¬¦å·é“¾æ¥åˆ°åŸºç¡€ç›®å½•ï¼Œæ–¹ä¾¿æŸ¥æ‰¾
        LINK_NAME="${OUTPUT_DIR_BASE}/latest_run"
        if [ -L "${LINK_NAME}" ]; then
            rm "${LINK_NAME}"
        fi
        ln -s "$(basename "${ACTUAL_OUTPUT_DIR}")" "${LINK_NAME}"
        echo "âœ… åˆ›å»ºç¬¦å·é“¾æ¥: ${LINK_NAME} -> $(basename "${ACTUAL_OUTPUT_DIR}")"
    else
        echo "âš ï¸ æœªæ‰¾åˆ°å®é™…è¾“å‡ºç›®å½•ï¼Œä½¿ç”¨åŸºç¡€ç›®å½•å¤‡ä»½"
        backup_training_scripts "${OUTPUT_DIR_BASE}"
    fi
}

# å¯åŠ¨åå°å¤‡ä»½ä»»åŠ¡
backup_scripts_to_model_dir &
BACKUP_PID=$!

# --- Execute Training ---
echo "Starting enhanced GRPO v2 training at $(date)..."

# Trap to handle interruption
trap 'echo "Training interrupted at $(date). Cleaning up..."; exit 130' INT TERM

eval "${FULL_CMD}"

status=$?

# ç­‰å¾…å¤‡ä»½ä»»åŠ¡å®Œæˆ
if [ -n "${BACKUP_PID}" ]; then
    echo "â³ ç­‰å¾…è„šæœ¬å¤‡ä»½ä»»åŠ¡å®Œæˆ..."
    wait ${BACKUP_PID} 2>/dev/null || true
    echo "âœ… è„šæœ¬å¤‡ä»½ä»»åŠ¡å·²å®Œæˆ"
fi

# --- Post-training Summary ---
echo ""
echo "========================================================================"
echo "                      TRAINING COMPLETION SUMMARY"
echo "========================================================================"
echo "Training finished at: $(date)"
echo "Exit status: ${status}"

if [ $status -eq 0 ]; then
    echo "âœ… Enhanced GRPO v2 training completed successfully!"
    
    # Check for outputs
    if [ -d "${OUTPUT_DIR_BASE}" ]; then
        FINAL_MODEL_DIR=$(find "${OUTPUT_DIR_BASE}" -name "*final*model*" -type d 2>/dev/null | head -1)
        if [ -n "${FINAL_MODEL_DIR}" ]; then
            echo "âœ… Final enhanced model saved to: ${FINAL_MODEL_DIR}"
            MODEL_SIZE=$(du -sh "${FINAL_MODEL_DIR}" 2>/dev/null | cut -f1 || echo "Unknown")
            echo "  Model size: ${MODEL_SIZE}"
        fi
        
        # Check for enhanced artifacts
        ARTIFACTS_DIR=$(find "${OUTPUT_DIR_BASE}" -name "enhanced_artifacts" -type d 2>/dev/null | head -1)
        if [ -d "${ARTIFACTS_DIR}" ]; then
            echo "âœ… Enhanced training artifacts saved:"
            find "${ARTIFACTS_DIR}" -name "*.json" -exec basename {} \; 2>/dev/null | sed 's/^/    - /' || echo "    - (artifacts found but couldn't list)"
        fi
        
        # Check logs
        LOG_FILE=$(find "${OUTPUT_DIR_BASE}" -name "*training_log.txt" 2>/dev/null | head -1)
        if [ -f "${LOG_FILE}" ]; then
            LOG_SIZE=$(wc -l < "${LOG_FILE}" 2>/dev/null || echo "0")
            echo "âœ… Training log: ${LOG_SIZE} lines"
            
            # Extract final metrics
            if command -v grep &> /dev/null && [ -f "${LOG_FILE}" ]; then
                FINAL_LOSS=$(grep -o "train_loss[^,]*" "${LOG_FILE}" | tail -1 | cut -d':' -f2 | tr -d ' ' 2>/dev/null || echo "N/A")
                echo "  Final training loss: ${FINAL_LOSS}"
            fi
        fi
    fi
    
    echo ""
    echo "ğŸ‰ Next steps:"
    echo "1. Check W&B dashboard for detailed metrics and curriculum progression"
    echo "2. Evaluate the model on your test set"
    echo "3. Compare with baseline model performance"
    echo "4. Consider fine-tuning hyperparameters based on the results"
    
else
    echo "âŒ Enhanced training failed with exit code ${status}"
    echo ""
    echo "ğŸ”§ Troubleshooting:"
    echo "1. Check the training log: ${OUTPUT_DIR_BASE}/*/enhanced_training_log.txt"
    echo "2. Verify dataset format: head -1 ${DATASET_PATH} | python -m json.tool"
    echo "3. Check file paths: ensure verilog and testbench files are accessible"
    echo "4. Review GPU memory usage (consider reducing batch size)"
    echo "5. Check W&B logs for detailed error information"
fi

echo "========================================================================"

# Clean up
if command -v nvidia-smi &> /dev/null; then
    echo "Final GPU status:"
    nvidia-smi --query-gpu=index,name,memory.used,memory.total --format=csv,noheader,nounits | head -n ${NUM_GPUS_PER_NODE}
fi

exit ${status}

# ====================================================================
# é›†æˆå®‰å…¨æ£€æŸ¥åŠŸèƒ½è¯´æ˜
# ====================================================================
#
# æœ¬è„šæœ¬å·²é›†æˆæ–­ç»­è®­ç»ƒå®‰å…¨æ£€æŸ¥åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
#
# ğŸ›¡ï¸ è‡ªåŠ¨å®‰å…¨æ£€æŸ¥:
#   - é¡¹ç›®æ–‡ä»¶ç»“æ„éªŒè¯
#   - Pythonç¯å¢ƒå’Œä¾èµ–æ£€æŸ¥  
#   - é…ç½®ä¸€è‡´æ€§éªŒè¯
#   - Checkpointå®Œæ•´æ€§æ£€æŸ¥
#   - é¡¹ç›®æ¨¡å—å¯¼å…¥æµ‹è¯•
#
# ğŸ”§ è‡ªåŠ¨ä¿®å¤åŠŸèƒ½:
#   - æ¸…ç†æ®‹ç•™ç¯å¢ƒå˜é‡
#   - æ¸…ç†Pythonç¼“å­˜
#   - åˆ›å»ºå¿…è¦ç›®å½•
#
# ğŸ“Š æ™ºèƒ½æ‘˜è¦æ˜¾ç¤º:
#   - è®­ç»ƒé…ç½®æ¦‚è§ˆ
#   - é•¿åº¦é…ç½®æ¯”ä¾‹
#   - æ¢å¤çŠ¶æ€ä¿¡æ¯
#
# ğŸ›ï¸ æ§åˆ¶é€‰é¡¹:
#   export SKIP_SAFETY_CHECK=true     # è·³è¿‡å®‰å…¨æ£€æŸ¥ï¼ˆä¸æ¨èï¼‰
#   export AUTO_FIX_ISSUES=false      # ç¦ç”¨è‡ªåŠ¨ä¿®å¤
#
# ğŸš€ ä½¿ç”¨æ–¹æ³•:
#   1. è®¾ç½®RESUME_FROM_CHECKPOINT_DIRï¼ˆå¦‚éœ€æ–­ç»­è®­ç»ƒï¼‰
#   2. ç›´æ¥è¿è¡Œ: ./run_enhanced_grpo_training.sh
#   3. è„šæœ¬ä¼šè‡ªåŠ¨æ£€æŸ¥å¹¶ä¿®å¤å¸¸è§é—®é¢˜
#   4. æ£€æŸ¥é€šè¿‡åè‡ªåŠ¨å¼€å§‹è®­ç»ƒ
#
# ğŸ’¡ å¦‚æœé‡åˆ°é—®é¢˜:
#   1. æŸ¥çœ‹æ£€æŸ¥è¾“å‡ºçš„é”™è¯¯ä¿¡æ¯
#   2. ä½¿ç”¨ç‹¬ç«‹å·¥å…·è¯¦ç»†è¯Šæ–­: python3 quick_resume_check.py
#   3. è¿è¡Œå®Œæ•´ä¿®å¤: ./cleanup_before_training.sh
#   4. é‡ç½®è¯¾ç¨‹çŠ¶æ€: python3 fix_curriculum_sync.py --create-fresh
#
# ====================================================================

RESUME_FROM_CHECKPOINT_DIR="/home/qhy/Research/LLM/GRPO-Clean-2/enhanced_grpo_8B_runs/v3-LR6e-6-R16-20250612-175657/checkpoint-540"

RESUME_FROM_CHECKPOINT_DIR="/home/qhy/Research/LLM/GRPO-Clean-2/enhanced_grpo_8B_runs/v3-LR6e-6-R16-20250612-175657/checkpoint-540"
