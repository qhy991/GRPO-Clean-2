#!/bin/bash
# run_model_parallel_only.sh - çº¯æ¨¡å‹å¹¶è¡Œè®­ç»ƒè„šæœ¬ï¼ˆä¸ä½¿ç”¨FSDPåˆ†å¸ƒå¼ï¼‰

set -e

# ğŸ”§ æ¿€æ´»condaç¯å¢ƒ
echo "ğŸ”§ æ¿€æ´»ReasoningVç¯å¢ƒ..."
source /home/qhy/anaconda3/bin/activate ReasoningV

# æ¸…é™¤åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡
echo "ğŸ§¹ æ¸…é™¤åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡..."
unset RANK 2>/dev/null || true
unset LOCAL_RANK 2>/dev/null || true  
unset WORLD_SIZE 2>/dev/null || true
unset MASTER_ADDR 2>/dev/null || true
unset MASTER_PORT 2>/dev/null || true

# ğŸ”§ è®¾ç½®GPUè®¾å¤‡
export CUDA_VISIBLE_DEVICES=0,1

# ğŸ”§ å¢å¼ºæ¨¡å‹å¹¶è¡Œä¼˜åŒ–è®¾ç½®
export PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True,max_split_size_mb:512,roundup_power2_divisions:16"
export CUDA_LAUNCH_BLOCKING=0
export TORCH_NCCL_BLOCKING_WAIT=1
export NCCL_TIMEOUT=7200
export NCCL_DEBUG=INFO
export NCCL_P2P_DISABLE=0
export CUDA_DEVICE_ORDER="PCI_BUS_ID"        # ç¡®ä¿è®¾å¤‡é¡ºåºä¸€è‡´
export NCCL_IB_DISABLE=0                     # å¯ç”¨InfiniBandï¼ˆå¦‚æœå¯ç”¨ï¼‰
export NCCL_SOCKET_IFNAME=^docker0,lo        # ä¼˜åŒ–ç½‘ç»œæ¥å£

# Flash Attention
export FLASH_ATTENTION_V2=1

# ç›®å½•è®¾ç½®
SCRIPT_DIR=$(cd "$(dirname "${BASH_SOURCE[0]}")" &> /dev/null && pwd)
PYTHON_EXECUTABLE="python"

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'  
BLUE='\033[0;34m'
NC='\033[0m'

log_info() { echo -e "${GREEN}[INFO]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARN]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

# é…ç½®å‚æ•°
export WANDB_PROJECT="VerilogGRPO_ModelParallel_Only"
export WANDB_ENTITY="qhy0227-tsinghua-university"

BASE_MODEL_NAME_OR_PATH="/home/share/Qwen3-8B/models--Qwen--Qwen3-8B/snapshots/a80f5e57cce20e57b65145f4213844dec1a80834"
STAGE1_ADAPTER_PATH="/home/share/ReasoningV/Qwen3/ckpts/sft-qwen3-lora-5epoch-origen200k-S1-20250429_211831/checkpoint-34695/"
DATASET_PATH="/home/qhy/Research/LLM/GRPO-RV/dataset/all-with-module-2.jsonl"
DATASET_BASE_PATH="/home/qhy/Research/LLM/GRPO-RV/dataset"
OUTPUT_DIR_BASE="./model_parallel_only_outputs"

# ğŸ› è¯¦ç»†DEBUGè¾“å‡ºç›®å½•é…ç½®
DEBUG_OUTPUT_BASE="${OUTPUT_DIR_BASE}/debug_data"
GENERATIONS_OUTPUT_DIR="${DEBUG_OUTPUT_BASE}/generations"
FAILED_GENERATIONS_DIR="${DEBUG_OUTPUT_BASE}/failed_generations"
SUCCESSFUL_GENERATIONS_DIR="${DEBUG_OUTPUT_BASE}/successful_generations"
DETAILED_METRICS_DIR="${DEBUG_OUTPUT_BASE}/detailed_metrics"
MODEL_OUTPUTS_DIR="${DEBUG_OUTPUT_BASE}/model_outputs"
REWARD_DETAILS_DIR="${DEBUG_OUTPUT_BASE}/reward_details"
TRAINING_LOGS_DIR="${DEBUG_OUTPUT_BASE}/training_logs"

# éªŒè¯è·¯å¾„
if [ ! -f "${DATASET_PATH}" ]; then
    log_error "âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: ${DATASET_PATH}"
    exit 1
fi

# ğŸ”§ å¢å¼ºæ¨¡å‹å¹¶è¡Œé…ç½®ï¼ˆå…³é”®ï¼šä¸ä½¿ç”¨FSDPï¼‰
USE_MODEL_PARALLEL=true
USE_FSDP=false  # æ˜ç¡®ç¦ç”¨FSDP
MAX_MEMORY_PER_GPU="78GiB"                    # æé«˜å†…å­˜é™åˆ¶ï¼ˆ75â†’78GBï¼‰
LOW_CPU_MEM_USAGE=true
# æ³¨æ„ï¼šç§»é™¤äº†ä¸æ”¯æŒçš„å‚æ•° MODEL_PARALLEL_STRATEGY, DEVICE_MAP_STRATEGY, LOAD_IN_8BIT, LOAD_IN_4BIT

# ğŸ”§ é«˜æ•ˆLoRAé…ç½®
LORA_RANK=32               # å¢åŠ LoRA rankä»¥æé«˜è¡¨è¾¾èƒ½åŠ›
LORA_ALPHA=64              # ç›¸åº”å¢åŠ alpha
LORA_DROPOUT=0.1            # é€‚å½“å¢åŠ dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
LORA_TARGET_MODULES="q_proj k_proj v_proj o_proj gate_proj up_proj down_proj"
# æ³¨æ„ï¼šç§»é™¤äº†ä¸æ”¯æŒçš„å‚æ•° LORA_FAN_IN_FAN_OUT, LORA_BIAS

# é•¿åº¦é…ç½®
MAX_SEQ_LENGTH=9216
MAX_PROMPT_LENGTH=1024
MAX_COMPLETION_LENGTH=8192
LENGTH_ALLOCATION_STRATEGY="custom"

# ğŸ”§ é«˜æ•ˆLoRAè®­ç»ƒé…ç½®
PER_DEVICE_TRAIN_BATCH_SIZE=1   # ä¿æŒç”¨æˆ·è®¾ç½®çš„æ‰¹æ¬¡å¤§å°
GRADIENT_ACCUMULATION_STEPS=8   # ä¿æŒç”¨æˆ·è®¾ç½®çš„æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
LEARNING_RATE=2e-5              # é€‚å½“æé«˜å­¦ä¹ ç‡
NUM_TRAIN_EPOCHS=3
WARMUP_RATIO=0.1
LR_SCHEDULER_TYPE="cosine"
WEIGHT_DECAY=0.01

# æ€§èƒ½è®¾ç½®
BF16_ENABLED=true
FP16_ENABLED=false
GRADIENT_CHECKPOINTING_ENABLED=false  # æ¨¡å‹å¹¶è¡Œæ—¶ç¦ç”¨
OPTIMIZER_TYPE="adamw_torch"

# ğŸ”§ ä¼˜åŒ–æ•°æ®åŠ è½½é…ç½®
DATALOADER_NUM_WORKERS=4    # å¢åŠ æ•°æ®åŠ è½½å¹¶è¡Œåº¦
DATALOADER_PIN_MEMORY=true  # å¯ç”¨pin memoryåŠ é€ŸGPUä¼ è¾“
DATALOADER_PREFETCH_FACTOR=8 # å¢åŠ é¢„å–å› å­

# ğŸ”§ ä¼˜åŒ–GRPOå’Œè®­ç»ƒé…ç½®
NUM_GENERATIONS_GRPO=2          # å¢åŠ ç”Ÿæˆæ•°é‡ä»¥æé«˜æ ·æœ¬æ•ˆç‡
CALLBACK_NUM_SAMPLES=2          # å¢åŠ å›è°ƒæ ·æœ¬æ•°
CALLBACK_EVAL_EVERY_N_STEPS=25  # å‡å°‘è¯„ä¼°é¢‘ç‡ä»¥æé«˜è®­ç»ƒæ•ˆç‡
SAVE_STRATEGY="steps"
SAVE_STEPS=20                  # å‡å°‘ä¿å­˜é¢‘ç‡
SAVE_TOTAL_LIMIT=5              # å¢åŠ ä¿å­˜æ£€æŸ¥ç‚¹æ•°é‡ (3â†’5)
LOGGING_STEPS=1                 # æ¯æ­¥éƒ½è®°å½•æ—¥å¿— (5â†’1)

# ğŸ› è¯¦ç»†DEBUGé…ç½®
DEBUG_MODE=true                 # å¯ç”¨è¯¦ç»†debugæ¨¡å¼
SAVE_ALL_GENERATIONS=true       # ä¿å­˜æ‰€æœ‰ç”Ÿæˆçš„æ ·æœ¬
SAVE_FAILED_GENERATIONS=true    # ä¿å­˜å¤±è´¥çš„ç”Ÿæˆæ ·æœ¬
SAVE_SUCCESSFUL_GENERATIONS=true # ä¿å­˜æˆåŠŸçš„ç”Ÿæˆæ ·æœ¬
SAVE_DETAILED_METRICS=true      # ä¿å­˜è¯¦ç»†çš„è®­ç»ƒæŒ‡æ ‡
SAVE_MODEL_OUTPUTS=true         # ä¿å­˜æ¨¡å‹çš„åŸå§‹è¾“å‡º
SAVE_REWARD_DETAILS=true        # ä¿å­˜å¥–åŠ±è®¡ç®—çš„è¯¦ç»†ä¿¡æ¯
DEBUG_SAMPLE_FREQUENCY=5        # æ¯5æ­¥ä¿å­˜ä¸€æ¬¡è¯¦ç»†æ ·æœ¬

# è¯¾ç¨‹å­¦ä¹ é…ç½®
ENABLE_CURRICULUM=true
CURRICULUM_TYPE="multi_stage"
CURRICULUM_FOCUS_LEVELS="basic intermediate advanced expert master"
CURRICULUM_COMPLEXITY_EMPHASIS="progressive"
CURRICULUM_PERFORMANCE_CHECK_INTERVAL=30   # å»¶é•¿æ£€æŸ¥é—´éš”ï¼ˆ10â†’50ï¼‰
# æ³¨æ„ï¼šç§»é™¤äº†ä¸æ”¯æŒçš„å‚æ•° CURRICULUM_MIN_STAGE_STEPS, CURRICULUM_ADAPTIVE_THRESHOLDS, CURRICULUM_STAGE_PATIENCE, CURRICULUM_PERFORMANCE_WINDOW

# ç»éªŒå›æ”¾é…ç½®
ENABLE_EXPERIENCE_REPLAY=true
EXPERIENCE_BUFFER_SIZE=1500
REPLAY_SAMPLE_RATIO=0.2

# å¥–åŠ±é…ç½®
REWARD_COMPILATION_SUCCESS=2.0
REWARD_COMPILATION_FAILURE=-4.0
REWARD_TEST_PASS_BASE=1.5
REWARD_TEST_PASS_BONUS_MULTIPLIER=1.2
REWARD_MAX_FUNCTIONAL=15.0
REWARD_ALL_TESTS_PASSED_BONUS=5.0

# ç”Ÿæˆå‚æ•°
GEN_TEMPERATURE=0.7
GEN_TOP_K=40
GEN_TOP_P=0.8
GEN_REPETITION_PENALTY=1.05

# åŠ¨æ€WandBè¿è¡Œåç§°
TIMESTAMP=$(date +%Y%m%d-%H%M%S)
export WANDB_RUN_NAME="DEBUG-model-parallel-LR${LEARNING_RATE}-R${LORA_RANK}-BS${PER_DEVICE_TRAIN_BATCH_SIZE}x${GRADIENT_ACCUMULATION_STEPS}-${TIMESTAMP}"

# ğŸ› è¯¦ç»†çš„WandBé…ç½®
export WANDB_LOG_MODEL="true"           # ä¿å­˜æ¨¡å‹åˆ°WandB
export WANDB_WATCH="all"                # ç›‘æ§æ‰€æœ‰å‚æ•°
export WANDB_SAVE_CODE="true"           # ä¿å­˜ä»£ç 
export WANDB_NOTES="DEBUGæ¨¡å¼è®­ç»ƒ - ä¿å­˜æ‰€æœ‰ç”Ÿæˆæ•°æ®ç”¨äºè¯¦ç»†åˆ†æ"
export WANDB_TAGS="debug,model_parallel,lora,grpo,verilog"

# ğŸ› é€šè¿‡ç¯å¢ƒå˜é‡ä¼ é€’DEBUGé…ç½® (é¿å…å‚æ•°è§£æé”™è¯¯)
export DEBUG_MODE="${DEBUG_MODE}"
export SAVE_ALL_GENERATIONS="${SAVE_ALL_GENERATIONS}"
export SAVE_FAILED_GENERATIONS="${SAVE_FAILED_GENERATIONS}"
export SAVE_SUCCESSFUL_GENERATIONS="${SAVE_SUCCESSFUL_GENERATIONS}"
export SAVE_DETAILED_METRICS="${SAVE_DETAILED_METRICS}"
export SAVE_MODEL_OUTPUTS="${SAVE_MODEL_OUTPUTS}"
export SAVE_REWARD_DETAILS="${SAVE_REWARD_DETAILS}"
export DEBUG_SAMPLE_FREQUENCY="${DEBUG_SAMPLE_FREQUENCY}"
export DEBUG_OUTPUT_BASE="${DEBUG_OUTPUT_BASE}"
export GENERATIONS_OUTPUT_DIR="${GENERATIONS_OUTPUT_DIR}"
export FAILED_GENERATIONS_DIR="${FAILED_GENERATIONS_DIR}"
export SUCCESSFUL_GENERATIONS_DIR="${SUCCESSFUL_GENERATIONS_DIR}"
export DETAILED_METRICS_DIR="${DETAILED_METRICS_DIR}"
export MODEL_OUTPUTS_DIR="${MODEL_OUTPUTS_DIR}"
export REWARD_DETAILS_DIR="${REWARD_DETAILS_DIR}"
export TRAINING_LOGS_DIR="${TRAINING_LOGS_DIR}"

# æ„å»ºå‘½ä»¤å‚æ•°
CMD_ARGS=""

# åŸºç¡€é…ç½®
CMD_ARGS="${CMD_ARGS} --wandb_project \"${WANDB_PROJECT}\""
CMD_ARGS="${CMD_ARGS} --wandb_entity \"${WANDB_ENTITY}\""
CMD_ARGS="${CMD_ARGS} --model_name_or_path \"${BASE_MODEL_NAME_OR_PATH}\""
CMD_ARGS="${CMD_ARGS} --stage1_adapter_path \"${STAGE1_ADAPTER_PATH}\""
CMD_ARGS="${CMD_ARGS} --dataset_path \"${DATASET_PATH}\""
CMD_ARGS="${CMD_ARGS} --dataset_base_path \"${DATASET_BASE_PATH}\""
CMD_ARGS="${CMD_ARGS} --output_dir_base \"${OUTPUT_DIR_BASE}\""

# ğŸ› DEBUGé…ç½®å‚æ•° (æ³¨æ„ï¼šç§»é™¤äº†ä¸æ”¯æŒçš„å‚æ•°ï¼Œä½†ä¿ç•™ç›®å½•å’Œç¯å¢ƒé…ç½®)
# è¿™äº›DEBUGå‚æ•°åœ¨main.pyçš„HfArgumentParserä¸­æœªå®šä¹‰ï¼Œæ‰€ä»¥æš‚æ—¶ç§»é™¤
# DEBUGåŠŸèƒ½å°†é€šè¿‡ç¯å¢ƒå˜é‡å’Œç›®å½•ç»“æ„æ¥å®ç°

# ğŸ”§ å…³é”®ï¼šæ˜ç¡®å¯ç”¨æ¨¡å‹å¹¶è¡Œ
CMD_ARGS="${CMD_ARGS} --use_model_parallel true"
CMD_ARGS="${CMD_ARGS} --max_memory_per_gpu \"${MAX_MEMORY_PER_GPU}\""
CMD_ARGS="${CMD_ARGS} --low_cpu_mem_usage ${LOW_CPU_MEM_USAGE}"
# ç§»é™¤äº†ä¸æ”¯æŒçš„å‚æ•°ï¼šmodel_parallel_strategy, device_map_strategy, load_in_8bit, load_in_4bit

# LoRAé…ç½®
CMD_ARGS="${CMD_ARGS} --lora_rank ${LORA_RANK}"
CMD_ARGS="${CMD_ARGS} --lora_alpha ${LORA_ALPHA}"
CMD_ARGS="${CMD_ARGS} --lora_dropout ${LORA_DROPOUT}"
CMD_ARGS="${CMD_ARGS} --lora_target_modules ${LORA_TARGET_MODULES}"
# ç§»é™¤äº†ä¸æ”¯æŒçš„å‚æ•°ï¼šlora_fan_in_fan_out, lora_bias

# é•¿åº¦é…ç½®
CMD_ARGS="${CMD_ARGS} --max_seq_length ${MAX_SEQ_LENGTH}"
CMD_ARGS="${CMD_ARGS} --script_max_prompt_length ${MAX_PROMPT_LENGTH}"
CMD_ARGS="${CMD_ARGS} --script_max_completion_length ${MAX_COMPLETION_LENGTH}"
CMD_ARGS="${CMD_ARGS} --length_allocation_strategy ${LENGTH_ALLOCATION_STRATEGY}"

# è®­ç»ƒé…ç½®
CMD_ARGS="${CMD_ARGS} --per_device_train_batch_size ${PER_DEVICE_TRAIN_BATCH_SIZE}"
CMD_ARGS="${CMD_ARGS} --gradient_accumulation_steps ${GRADIENT_ACCUMULATION_STEPS}"
CMD_ARGS="${CMD_ARGS} --learning_rate ${LEARNING_RATE}"
CMD_ARGS="${CMD_ARGS} --num_train_epochs ${NUM_TRAIN_EPOCHS}"
CMD_ARGS="${CMD_ARGS} --warmup_ratio ${WARMUP_RATIO}"
CMD_ARGS="${CMD_ARGS} --lr_scheduler_type \"${LR_SCHEDULER_TYPE}\""
CMD_ARGS="${CMD_ARGS} --weight_decay ${WEIGHT_DECAY}"

# GRPOé…ç½®
CMD_ARGS="${CMD_ARGS} --num_generations ${NUM_GENERATIONS_GRPO}"
CMD_ARGS="${CMD_ARGS} --max_prompt_length ${MAX_PROMPT_LENGTH}"
CMD_ARGS="${CMD_ARGS} --max_completion_length ${MAX_COMPLETION_LENGTH}"

# å›è°ƒé…ç½®
CMD_ARGS="${CMD_ARGS} --callback_num_samples ${CALLBACK_NUM_SAMPLES}"
CMD_ARGS="${CMD_ARGS} --callback_eval_every_n_steps ${CALLBACK_EVAL_EVERY_N_STEPS}"

# è¯¾ç¨‹å­¦ä¹ é…ç½®
CMD_ARGS="${CMD_ARGS} --enable_curriculum ${ENABLE_CURRICULUM}"
CMD_ARGS="${CMD_ARGS} --curriculum_type ${CURRICULUM_TYPE}"
CMD_ARGS="${CMD_ARGS} --curriculum_focus_levels ${CURRICULUM_FOCUS_LEVELS}"
CMD_ARGS="${CMD_ARGS} --curriculum_complexity_emphasis ${CURRICULUM_COMPLEXITY_EMPHASIS}"
CMD_ARGS="${CMD_ARGS} --curriculum_performance_check_interval ${CURRICULUM_PERFORMANCE_CHECK_INTERVAL}"
# ç§»é™¤äº†ä¸æ”¯æŒçš„å‚æ•°ï¼šcurriculum_min_stage_steps, curriculum_adaptive_thresholds, curriculum_stage_patience, curriculum_performance_window

# ç»éªŒå›æ”¾é…ç½®
CMD_ARGS="${CMD_ARGS} --enable_experience_replay ${ENABLE_EXPERIENCE_REPLAY}"
CMD_ARGS="${CMD_ARGS} --experience_buffer_size ${EXPERIENCE_BUFFER_SIZE}"
CMD_ARGS="${CMD_ARGS} --replay_sample_ratio ${REPLAY_SAMPLE_RATIO}"

# å¥–åŠ±é…ç½®
CMD_ARGS="${CMD_ARGS} --compilation_success ${REWARD_COMPILATION_SUCCESS}"
CMD_ARGS="${CMD_ARGS} --compilation_failure ${REWARD_COMPILATION_FAILURE}"
CMD_ARGS="${CMD_ARGS} --test_pass_base_reward ${REWARD_TEST_PASS_BASE}"
CMD_ARGS="${CMD_ARGS} --test_pass_bonus_multiplier ${REWARD_TEST_PASS_BONUS_MULTIPLIER}"
CMD_ARGS="${CMD_ARGS} --max_functional_reward ${REWARD_MAX_FUNCTIONAL}"
CMD_ARGS="${CMD_ARGS} --all_tests_passed_bonus ${REWARD_ALL_TESTS_PASSED_BONUS}"

# ç”Ÿæˆå‚æ•°
CMD_ARGS="${CMD_ARGS} --gen_temperature ${GEN_TEMPERATURE}"
CMD_ARGS="${CMD_ARGS} --gen_top_k ${GEN_TOP_K}"
CMD_ARGS="${CMD_ARGS} --gen_top_p ${GEN_TOP_P}"
CMD_ARGS="${CMD_ARGS} --gen_repetition_penalty ${GEN_REPETITION_PENALTY}"

# æ€§èƒ½è®¾ç½®
if [ "$BF16_ENABLED" = true ]; then CMD_ARGS="${CMD_ARGS} --bf16"; fi
if [ "$FP16_ENABLED" = true ] && [ "$BF16_ENABLED" = false ]; then CMD_ARGS="${CMD_ARGS} --fp16"; fi
if [ "$GRADIENT_CHECKPOINTING_ENABLED" = true ]; then CMD_ARGS="${CMD_ARGS} --gradient_checkpointing"; fi

# ğŸ”§ ä¼˜åŒ–æ•°æ®åŠ è½½é…ç½®
CMD_ARGS="${CMD_ARGS} --dataloader_num_workers ${DATALOADER_NUM_WORKERS}"
CMD_ARGS="${CMD_ARGS} --dataloader_prefetch_factor ${DATALOADER_PREFETCH_FACTOR}"
if [ "$DATALOADER_PIN_MEMORY" = true ]; then CMD_ARGS="${CMD_ARGS} --dataloader_pin_memory"; fi

# ä¿å­˜å’Œæ—¥å¿—é…ç½®
CMD_ARGS="${CMD_ARGS} --logging_strategy \"steps\""
CMD_ARGS="${CMD_ARGS} --logging_steps ${LOGGING_STEPS}"
CMD_ARGS="${CMD_ARGS} --save_strategy \"${SAVE_STRATEGY}\""
CMD_ARGS="${CMD_ARGS} --save_steps ${SAVE_STEPS}"
CMD_ARGS="${CMD_ARGS} --save_total_limit ${SAVE_TOTAL_LIMIT}"
CMD_ARGS="${CMD_ARGS} --report_to \"wandb\""
CMD_ARGS="${CMD_ARGS} --remove_unused_columns False"
CMD_ARGS="${CMD_ARGS} --optim \"${OPTIMIZER_TYPE}\""
CMD_ARGS="${CMD_ARGS} --ddp_find_unused_parameters False"

# ç¼“å­˜ç›®å½•
CACHE_DIR_BASE="${SCRIPT_DIR}/.model_parallel_cache"
mkdir -p "${CACHE_DIR_BASE}/datasets"
mkdir -p "${CACHE_DIR_BASE}/models"
CMD_ARGS="${CMD_ARGS} --cache_dir \"${CACHE_DIR_BASE}/models\""

# Pythonè„šæœ¬è·¯å¾„
PYTHON_SCRIPT_TO_RUN="${SCRIPT_DIR}/main.py"

# ğŸ”§ å…³é”®ï¼šä½¿ç”¨ç›´æ¥pythonæ‰§è¡Œï¼Œä¸ä½¿ç”¨torchrun
FULL_CMD="${PYTHON_EXECUTABLE} ${PYTHON_SCRIPT_TO_RUN} ${CMD_ARGS}"

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p ${OUTPUT_DIR_BASE}

# ğŸ› åˆ›å»ºè¯¦ç»†çš„DEBUGç›®å½•ç»“æ„
log_info "ğŸ“ åˆ›å»ºè¯¦ç»†çš„DEBUGç›®å½•ç»“æ„..."
mkdir -p "${DEBUG_OUTPUT_BASE}"
mkdir -p "${GENERATIONS_OUTPUT_DIR}"
mkdir -p "${FAILED_GENERATIONS_DIR}"
mkdir -p "${SUCCESSFUL_GENERATIONS_DIR}"
mkdir -p "${DETAILED_METRICS_DIR}"
mkdir -p "${MODEL_OUTPUTS_DIR}"
mkdir -p "${REWARD_DETAILS_DIR}"
mkdir -p "${TRAINING_LOGS_DIR}"

# åˆ›å»ºæŒ‰æ—¶é—´æˆ³çš„å­ç›®å½•
TIMESTAMP_DIR="${TIMESTAMP}"
mkdir -p "${GENERATIONS_OUTPUT_DIR}/${TIMESTAMP_DIR}"
mkdir -p "${FAILED_GENERATIONS_DIR}/${TIMESTAMP_DIR}"
mkdir -p "${SUCCESSFUL_GENERATIONS_DIR}/${TIMESTAMP_DIR}"
mkdir -p "${DETAILED_METRICS_DIR}/${TIMESTAMP_DIR}"
mkdir -p "${MODEL_OUTPUTS_DIR}/${TIMESTAMP_DIR}"
mkdir -p "${REWARD_DETAILS_DIR}/${TIMESTAMP_DIR}"
mkdir -p "${TRAINING_LOGS_DIR}/${TIMESTAMP_DIR}"

log_info "âœ… DEBUGç›®å½•åˆ›å»ºå®Œæˆï¼š${DEBUG_OUTPUT_BASE}"

# è®­ç»ƒå‰æ‘˜è¦
echo ""
echo "========================================================================"
echo "                    çº¯æ¨¡å‹å¹¶è¡ŒGRPOè®­ç»ƒå¯åŠ¨"
echo "========================================================================"
log_info "ğŸ¯ ä¼˜åŒ–åçš„LoRAè®­ç»ƒé…ç½®æ‘˜è¦:"
log_info "  - æ¨¡å¼: çº¯æ¨¡å‹å¹¶è¡Œï¼ˆæ— FSDPï¼Œæ— DDPï¼‰"
log_info "  - å¯åŠ¨æ–¹å¼: ç›´æ¥pythonï¼ˆå•è¿›ç¨‹ï¼‰"
log_info "  - GPUé…ç½®: ${CUDA_VISIBLE_DEVICES}"
log_info "  - æ¨¡å‹: $(basename "${BASE_MODEL_NAME_OR_PATH}")"
log_info "  - æ•°æ®é›†: $(basename "${DATASET_PATH}")"
log_info "  - åºåˆ—é•¿åº¦: ${MAX_SEQ_LENGTH} (ç”¨æˆ·ä¼˜åŒ–: 9216)"
log_info "  - LoRA: rank=${LORA_RANK}, alpha=${LORA_ALPHA}"
log_info "  - æ‰¹æ¬¡å¤§å°: ${PER_DEVICE_TRAIN_BATCH_SIZE}"
log_info "  - æ¢¯åº¦ç´¯ç§¯: ${GRADIENT_ACCUMULATION_STEPS} (ç”¨æˆ·ä¼˜åŒ–: 8)"
log_info "  - æœ‰æ•ˆæ‰¹æ¬¡: $((PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS * 2)) (åŒGPU)"
log_info "  - å­¦ä¹ ç‡: ${LEARNING_RATE}"
log_info "  - GRPOç”Ÿæˆæ•°: ${NUM_GENERATIONS_GRPO}"
log_info "  - è¯¾ç¨‹æ£€æŸ¥é—´éš”: ${CURRICULUM_PERFORMANCE_CHECK_INTERVAL}æ­¥"
log_info "  - æ•°æ®åŠ è½½å™¨: ${DATALOADER_NUM_WORKERS} workers, prefetch=${DATALOADER_PREFETCH_FACTOR}"
log_info "  - è¾“å‡ºç›®å½•: ${OUTPUT_DIR_BASE}"

echo ""
log_info "ğŸ“Š ä¼˜åŒ–åçš„å†…å­˜é…ç½®:"
log_info "  - æ¯GPUå†…å­˜é™åˆ¶: ${MAX_MEMORY_PER_GPU} (ä¼˜åŒ–: 78GB)"
log_info "  - ä½CPUå†…å­˜ä½¿ç”¨: ${LOW_CPU_MEM_USAGE}"
log_info "  - æ¢¯åº¦æ£€æŸ¥ç‚¹: ${GRADIENT_CHECKPOINTING_ENABLED}"
log_info "  - è¯¾ç¨‹å­¦ä¹ ç±»å‹: ${CURRICULUM_TYPE} (å¤šé˜¶æ®µ)"
log_info "  - è¯¾ç¨‹å­¦ä¹ ç­‰çº§: ${CURRICULUM_FOCUS_LEVELS}"
log_info "  - æ•°æ®åŠ è½½ä¼˜åŒ–: Pin Memory + ${DATALOADER_NUM_WORKERS} Workers"

echo ""
log_info "ğŸ› è¯¦ç»†DEBUGé…ç½® (é€šè¿‡ç¯å¢ƒå˜é‡ä¼ é€’):"
log_info "  - DEBUGæ¨¡å¼: ${DEBUG_MODE} âœ…"
log_info "  - ä¿å­˜æ‰€æœ‰ç”Ÿæˆ: ${SAVE_ALL_GENERATIONS}"
log_info "  - ä¿å­˜å¤±è´¥æ ·æœ¬: ${SAVE_FAILED_GENERATIONS}"
log_info "  - ä¿å­˜æˆåŠŸæ ·æœ¬: ${SAVE_SUCCESSFUL_GENERATIONS}"
log_info "  - ä¿å­˜è¯¦ç»†æŒ‡æ ‡: ${SAVE_DETAILED_METRICS}"
log_info "  - ä¿å­˜æ¨¡å‹è¾“å‡º: ${SAVE_MODEL_OUTPUTS}"
log_info "  - ä¿å­˜å¥–åŠ±è¯¦æƒ…: ${SAVE_REWARD_DETAILS}"
log_info "  - DEBUGé‡‡æ ·é¢‘ç‡: æ¯${DEBUG_SAMPLE_FREQUENCY}æ­¥"
log_info "  - æ—¥å¿—è®°å½•é¢‘ç‡: æ¯${LOGGING_STEPS}æ­¥"
log_info "  - DEBUGè¾“å‡ºç›®å½•: ${DEBUG_OUTPUT_BASE}"
log_info "  - é…ç½®æ–¹å¼: ç¯å¢ƒå˜é‡ (é¿å…å‚æ•°è§£æå†²çª)"

echo ""
echo "ğŸš€ é«˜æ•ˆè®­ç»ƒä¼˜åŒ–è¯´æ˜:"
echo "  - ğŸ“ˆ æ‰¹æ¬¡å¤§å°ä¼˜åŒ–: 2â†’8 (4å€æå‡), å……åˆ†åˆ©ç”¨GPUè®¡ç®—èƒ½åŠ›"
echo "  - ğŸ§  LoRAé…ç½®å¢å¼º: rank 64â†’128, alpha 128â†’256, æå‡æ¨¡å‹è¡¨è¾¾èƒ½åŠ›"
echo "  - âš¡ æ•°æ®åŠ è½½åŠ é€Ÿ: 0â†’4 workers + 8å€é¢„å–, å‡å°‘I/Oç­‰å¾…"
echo "  - ğŸ”§ GRPOä¼˜åŒ–: ç”Ÿæˆæ•° 2â†’4, å‡å°‘è®¾å¤‡åŒæ­¥è­¦å‘Šé¢‘ç‡"
echo "  - ğŸ¯ é¢„æœŸæ•ˆæœ: GPUåˆ©ç”¨ç‡ >80%, è®­ç»ƒé€Ÿåº¦æå‡ 2-3å€"
echo "  - ğŸ“Š ç›‘æ§æŒ‡æ ‡: nvidia-smiæŸ¥çœ‹GPUåˆ©ç”¨ç‡, WandBç›‘æ§loss"
echo "========================================================================"

# ä¿å­˜å®Œæ•´å‘½ä»¤
echo "${FULL_CMD}" > "${OUTPUT_DIR_BASE}/full_training_command.txt"
log_info "ğŸ’¾ å®Œæ•´å‘½ä»¤å·²ä¿å­˜åˆ°: ${OUTPUT_DIR_BASE}/full_training_command.txt"

# è®­ç»ƒå‰GPUçŠ¶æ€
echo ""
log_info "ğŸ” è®­ç»ƒå‰GPUçŠ¶æ€:"
nvidia-smi --query-gpu=index,name,memory.used,memory.total --format=csv,noheader

# æ¸…ç†å‡½æ•°
cleanup_on_exit() {
    echo ""
    log_info "ğŸ›‘ è®­ç»ƒç»“æŸ/ä¸­æ–­ï¼Œæ‰§è¡Œæ¸…ç†..."
    
    # åœæ­¢DEBUGæ”¶é›†å™¨
    if [ ! -z "$DEBUG_COLLECTOR_PID" ]; then
        kill $DEBUG_COLLECTOR_PID 2>/dev/null
        log_info "ğŸ”´ DEBUGæ”¶é›†å™¨å·²åœæ­¢ (PID: ${DEBUG_COLLECTOR_PID})"
    fi
    
    # ä¿å­˜æœ€ç»ˆçŠ¶æ€
    log_info "ğŸ“Š ä¿å­˜æœ€ç»ˆGPUçŠ¶æ€..."
    nvidia-smi --query-gpu=index,name,memory.used,memory.total,utilization.gpu --format=csv,noheader > "${TRAINING_LOGS_DIR}/${TIMESTAMP_DIR}/final_gpu_status.csv"
    
    # ä¿å­˜è®­ç»ƒæ‘˜è¦
    TRAINING_SUMMARY="${TRAINING_LOGS_DIR}/${TIMESTAMP_DIR}/training_summary.txt"
    {
        echo "=== è®­ç»ƒæ‘˜è¦ ==="
        echo "å¼€å§‹æ—¶é—´: ${TIMESTAMP}"
        echo "ç»“æŸæ—¶é—´: $(date +%Y%m%d-%H%M%S)"
        echo "è®­ç»ƒçŠ¶æ€: $([[ $status -eq 0 ]] && echo 'æˆåŠŸ' || echo 'å¤±è´¥')"
        echo "é€€å‡ºç : ${status}"
        echo "GPUé…ç½®: ${CUDA_VISIBLE_DEVICES}"
        echo "åºåˆ—é•¿åº¦: ${MAX_SEQ_LENGTH}"
        echo "æ‰¹æ¬¡å¤§å°: ${PER_DEVICE_TRAIN_BATCH_SIZE}"
        echo "æ¢¯åº¦ç´¯ç§¯: ${GRADIENT_ACCUMULATION_STEPS}"
        echo "å­¦ä¹ ç‡: ${LEARNING_RATE}"
        echo "DEBUGæ¨¡å¼: ${DEBUG_MODE}"
        echo ""
        echo "=== è¾“å‡ºç›®å½• ==="
        echo "ä¸»è¾“å‡º: ${OUTPUT_DIR_BASE}"
        echo "DEBUGæ•°æ®: ${DEBUG_OUTPUT_BASE}"
        echo "è®­ç»ƒæ—¥å¿—: ${TRAINING_LOGS_DIR}/${TIMESTAMP_DIR}"
    } > "${TRAINING_SUMMARY}"
    
    log_info "ğŸ“„ è®­ç»ƒæ‘˜è¦å·²ä¿å­˜åˆ°: ${TRAINING_SUMMARY}"
    log_info "â° è®­ç»ƒç»“æŸæ—¶é—´: $(date)"
}
trap cleanup_on_exit INT TERM EXIT

# ğŸ”§ æ‰§è¡Œè®­ç»ƒå‘½ä»¤
echo ""
log_info "ğŸš€ å¼€å§‹çº¯æ¨¡å‹å¹¶è¡ŒGRPOè®­ç»ƒ ($(date))..."

# ğŸ› è®¾ç½®è¯¦ç»†çš„æ—¥å¿—è®°å½•
FULL_LOG_FILE="${TRAINING_LOGS_DIR}/${TIMESTAMP_DIR}/full_training_log.txt"
ERROR_LOG_FILE="${TRAINING_LOGS_DIR}/${TIMESTAMP_DIR}/error_log.txt"
GPU_MONITOR_FILE="${TRAINING_LOGS_DIR}/${TIMESTAMP_DIR}/gpu_monitor.log"

# å¯åŠ¨ç®€å•DEBUGæ”¶é›†å™¨ (åå°è¿è¡Œ)
log_info "ğŸš€ å¯åŠ¨ç®€å•DEBUGæ”¶é›†å™¨..."
python3 "${SCRIPT_DIR}/simple_debug_collector.py" &
DEBUG_COLLECTOR_PID=$!

log_info "ğŸ“Š DEBUGæ”¶é›†å™¨å·²å¯åŠ¨ï¼ŒPID: ${DEBUG_COLLECTOR_PID}"
log_info "ğŸ“ å®Œæ•´æ—¥å¿—å°†ä¿å­˜åˆ°: ${FULL_LOG_FILE}"
log_info "âŒ é”™è¯¯æ—¥å¿—å°†ä¿å­˜åˆ°: ${ERROR_LOG_FILE}"

# ğŸ› éªŒè¯ç¯å¢ƒå˜é‡ä¼ é€’
log_info "ğŸ” éªŒè¯DEBUGç¯å¢ƒå˜é‡ä¼ é€’:"
log_info "  DEBUG_MODE=${DEBUG_MODE}"
log_info "  SAVE_ALL_GENERATIONS=${SAVE_ALL_GENERATIONS}"
log_info "  DEBUG_OUTPUT_BASE=${DEBUG_OUTPUT_BASE}"

# ğŸ› æ˜¾å¼å¯¼å‡ºæ‰€æœ‰DEBUGç¯å¢ƒå˜é‡
export DEBUG_MODE
export SAVE_ALL_GENERATIONS
export SAVE_FAILED_GENERATIONS
export SAVE_SUCCESSFUL_GENERATIONS
export SAVE_DETAILED_METRICS
export SAVE_MODEL_OUTPUTS
export SAVE_REWARD_DETAILS
export DEBUG_SAMPLE_FREQUENCY
export DEBUG_OUTPUT_BASE
export GENERATIONS_OUTPUT_DIR
export FAILED_GENERATIONS_DIR
export SUCCESSFUL_GENERATIONS_DIR
export DETAILED_METRICS_DIR
export MODEL_OUTPUTS_DIR
export REWARD_DETAILS_DIR
export TRAINING_LOGS_DIR

# æ‰§è¡Œè®­ç»ƒå‘½ä»¤å¹¶è®°å½•æ‰€æœ‰è¾“å‡º
eval ${FULL_CMD} 2>&1 | tee "${FULL_LOG_FILE}"

status=$?

# è®­ç»ƒåæ‘˜è¦
echo ""
echo "========================================================================"
echo "                      çº¯æ¨¡å‹å¹¶è¡Œè®­ç»ƒå®Œæˆæ‘˜è¦"
echo "========================================================================"
log_info "â° è®­ç»ƒç»“æŸæ—¶é—´: $(date)"
log_info "ğŸ¯ é€€å‡ºçŠ¶æ€: ${status}"

if [ $status -eq 0 ]; then
    log_info "âœ… çº¯æ¨¡å‹å¹¶è¡ŒGRPOè®­ç»ƒæˆåŠŸå®Œæˆ!"
else
    log_error "âŒ çº¯æ¨¡å‹å¹¶è¡Œè®­ç»ƒå¤±è´¥ï¼Œé€€å‡ºç : ${status}"
fi

echo "========================================================================"

exit ${status} 