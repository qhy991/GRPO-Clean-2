#!/bin/bash
# run_swift_grpo_training.sh - ä½¿ç”¨Swiftè¿›è¡ŒGRPOå¤šå¡è®­ç»ƒ

# --- Exit on error ---
set -e
export CUDA_VISIBLE_DEVICES=0,1

# --- æ¿€æ´»ReasoningVç¯å¢ƒ ---
source ~/anaconda3/etc/profile.d/conda.sh
conda activate ReasoningV
echo "âœ… å·²æ¿€æ´»ç¯å¢ƒ: $CONDA_DEFAULT_ENV"

# --- è·å–è„šæœ¬ç›®å½• ---
SCRIPT_DIR=$(cd "$(dirname "${BASH_SOURCE[0]}")" &> /dev/null && pwd)

# --- Swiftå¤šå¡è®­ç»ƒä¼˜åŒ–è®¾ç½® ---
export NCCL_TIMEOUT=7200
export NCCL_IB_DISABLE=1
export NCCL_P2P_DISABLE=1
export NCCL_DEBUG=INFO
export NCCL_BLOCKING_WAIT=1
export PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True,max_split_size_mb:128"

# --- é¢œè‰²å®šä¹‰ ---
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() { echo -e "${GREEN}[INFO]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARN]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

# --- Swiftå’ŒWandBé…ç½® ---
export WANDB_PROJECT="VerilogGRPO_Swift_Enhanced"
export WANDB_ENTITY="qhy0227-tsinghua-university"

# --- Modelå’Œæ•°æ®é…ç½® ---
BASE_MODEL_NAME_OR_PATH="/home/share/Qwen3-8B/models--Qwen--Qwen3-8B/snapshots/a80f5e57cce20e57b65145f4213844dec1a80834"
STAGE1_ADAPTER_PATH="/home/share/ReasoningV/Qwen3/ckpts/sft-qwen3-lora-5epoch-origen200k-S1-20250429_211831/checkpoint-34695/"
DATASET_PATH="/home/qhy/Research/LLM/GRPO-RV/dataset/swift_all-with-module-2.jsonl"

# --- Swiftä¸“ç”¨ç›®å½•è®¾ç½® ---
OUTPUT_DIR_BASE="./swift_grpo_runs"
mkdir -p ${OUTPUT_DIR_BASE}

# --- è®­ç»ƒå‚æ•° ---
PER_DEVICE_TRAIN_BATCH_SIZE=1
GRADIENT_ACCUMULATION_STEPS=8
LEARNING_RATE=6e-6
NUM_TRAIN_EPOCHS=4
MAX_SEQ_LENGTH=4096
MAX_PROMPT_LENGTH=1024
MAX_COMPLETION_LENGTH=3072

# --- LoRAé…ç½® ---
LORA_RANK=16
LORA_ALPHA=32
LORA_DROPOUT=0.05

# --- è¯¾ç¨‹å­¦ä¹ é…ç½® ---
CURRICULUM_PERFORMANCE_THRESHOLD_1=0.75
CURRICULUM_PERFORMANCE_THRESHOLD_2=0.70
CURRICULUM_PERFORMANCE_THRESHOLD_3=0.65
CURRICULUM_PERFORMANCE_THRESHOLD_4=0.60
CURRICULUM_PERFORMANCE_THRESHOLD_5=0.55

# --- æ£€æŸ¥æ•°æ®é›† ---
if [ ! -f "${DATASET_PATH}" ]; then
    log_error "æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: ${DATASET_PATH}"
    exit 1
fi

log_info "ğŸš€ å¯åŠ¨Swiftå¤šå¡GRPOè®­ç»ƒ"
log_info "æ¨¡å‹: $(basename "${BASE_MODEL_NAME_OR_PATH}")"
log_info "æ•°æ®é›†: $(basename "${DATASET_PATH}")"
log_info "è¾“å‡ºç›®å½•: ${OUTPUT_DIR_BASE}"

# --- é¢„è®­ç»ƒæ£€æŸ¥ ---
echo ""
echo "ğŸ” è®­ç»ƒå‰ç¯å¢ƒæ£€æŸ¥..."
echo "Pythonç¯å¢ƒ: $(which python)"
echo "Swiftç‰ˆæœ¬: $(python -c 'import swift; print(swift.__version__)' 2>/dev/null || echo 'N/A')"
echo "PyTorchç‰ˆæœ¬: $(python -c 'import torch; print(torch.__version__)' 2>/dev/null || echo 'N/A')"
echo ""
echo "GPUçŠ¶æ€:"
nvidia-smi --query-gpu=index,name,memory.used,memory.total --format=csv,noheader,nounits
echo ""

# æ£€æŸ¥æ•°æ®é›†æ ·æœ¬
echo "ğŸ“Š æ•°æ®é›†ä¿¡æ¯:"
DATASET_LINES=$(wc -l < "${DATASET_PATH}" 2>/dev/null || echo "0")
echo "æ•°æ®é›†è¡Œæ•°: ${DATASET_LINES}"
if [ -f "${DATASET_PATH}" ]; then
    echo "æ•°æ®é›†å¤§å°: $(du -sh "${DATASET_PATH}" | cut -f1)"
    echo "æ ·æœ¬é¢„è§ˆ:"
    head -1 "${DATASET_PATH}" | python -c "
import sys, json
try:
    data = json.loads(sys.stdin.read())
    print(f'  conversationsæ•°é‡: {len(data.get(\"conversations\", []))}')
    if 'conversations' in data and len(data['conversations']) > 0:
        print(f'  ç¬¬ä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯é•¿åº¦: {len(data[\"conversations\"][0].get(\"value\", \"\"))}')
        if len(data['conversations']) > 1:
            print(f'  ç¬¬ä¸€ä¸ªåŠ©æ‰‹å›å¤é•¿åº¦: {len(data[\"conversations\"][1].get(\"value\", \"\"))}')
except Exception as e:
    print(f'  æ•°æ®æ ¼å¼æ£€æŸ¥å¤±è´¥: {e}')
" 2>/dev/null || echo "  æ•°æ®æ ¼å¼æ£€æŸ¥å¤±è´¥"
else
    log_error "æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: ${DATASET_PATH}"
    exit 1
fi

# --- ç”Ÿæˆæ—¶é—´æˆ³ ---
TIMESTAMP=$(date +%Y%m%d-%H%M%S)
EXPERIMENT_NAME="swift-grpo-${TIMESTAMP}"

echo "========================================================================"
echo "                    SWIFT GRPOå¤šå¡è®­ç»ƒå¼€å§‹"
echo "========================================================================"

# --- Swiftå¤šå¡è®­ç»ƒå‘½ä»¤ ---
# ä½¿ç”¨Swiftçš„åˆ†å¸ƒå¼è®­ç»ƒæ¥å£è¿›è¡ŒGRPOé£æ ¼è®­ç»ƒ
CUDA_VISIBLE_DEVICES=0,1 swift sft \
    --model_type qwen3-8b \
    --model_id_or_path "${BASE_MODEL_NAME_OR_PATH}" \
    --sft_type lora \
    --tuner_backend peft \
    --dataset_path "${DATASET_PATH}" \
    --train_dataset_sample -1 \
    --num_train_epochs ${NUM_TRAIN_EPOCHS} \
    --max_length ${MAX_SEQ_LENGTH} \
    --truncation_strategy delete \
    --check_dataset_strategy warning \
    --lora_rank ${LORA_RANK} \
    --lora_alpha ${LORA_ALPHA} \
    --lora_dropout_p ${LORA_DROPOUT} \
    --lora_target_modules ALL \
    --gradient_checkpointing true \
    --batch_size ${PER_DEVICE_TRAIN_BATCH_SIZE} \
    --weight_decay 0.01 \
    --learning_rate ${LEARNING_RATE} \
    --gradient_accumulation_steps ${GRADIENT_ACCUMULATION_STEPS} \
    --max_grad_norm 0.5 \
    --warmup_ratio 0.15 \
    --eval_steps 50 \
    --save_steps 100 \
    --save_total_limit 3 \
    --logging_steps 10 \
    --output_dir "${OUTPUT_DIR_BASE}/${EXPERIMENT_NAME}" \
    --ddp_backend nccl \
    --ddp_find_unused_parameters false \
    --dataloader_num_workers 0 \
    --push_to_hub false \
    --hub_model_id "${EXPERIMENT_NAME}" \
    --hub_private_repo true \
    --hub_token none \
    --use_flash_attn true \

    --report_to 'wandb' \
    --run_name "${EXPERIMENT_NAME}" \
    --seed 42 \
    --deepspeed default-zero2

training_status=$?

echo ""
echo "========================================================================"
echo "                      SWIFTè®­ç»ƒå®Œæˆ"
echo "========================================================================"
echo "è®­ç»ƒç»“æŸæ—¶é—´: $(date)"
echo "é€€å‡ºçŠ¶æ€: ${training_status}"

if [ $training_status -eq 0 ]; then
    log_info "âœ… Swift GRPOè®­ç»ƒæˆåŠŸå®Œæˆï¼"
    log_info "æ¨¡å‹ä¿å­˜ä½ç½®: ${OUTPUT_DIR_BASE}/${EXPERIMENT_NAME}"
    
    # æ˜¾ç¤ºæœ€ç»ˆæ¨¡å‹ä¿¡æ¯
    if [ -d "${OUTPUT_DIR_BASE}/${EXPERIMENT_NAME}" ]; then
        MODEL_SIZE=$(du -sh "${OUTPUT_DIR_BASE}/${EXPERIMENT_NAME}" 2>/dev/null | cut -f1 || echo "Unknown")
        log_info "æ¨¡å‹å¤§å°: ${MODEL_SIZE}"
    fi
else
    log_error "âŒ Swiftè®­ç»ƒå¤±è´¥ï¼Œé€€å‡ºç : ${training_status}"
    log_error "è¯·æ£€æŸ¥è®­ç»ƒæ—¥å¿—å’Œé”™è¯¯ä¿¡æ¯"
fi

# --- æœ€ç»ˆGPUçŠ¶æ€ ---
echo ""
echo "æœ€ç»ˆGPUçŠ¶æ€:"
nvidia-smi --query-gpu=index,name,memory.used,memory.total --format=csv,noheader,nounits

exit ${training_status} 