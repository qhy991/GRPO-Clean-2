#!/usr/bin/env python3
"""è®¾å¤‡ä¸€è‡´æ€§ä¿®å¤è°ƒè¯•è„šæœ¬"""

import os
import sys
import torch
import logging
from transformers import HfArgumentParser

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from grpo_project.configs import EnvConfig, ScriptConfig, EnhancedRewardConfig
from trl import GRPOConfig

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_device_consistency_fix():
    """æµ‹è¯•è®¾å¤‡ä¸€è‡´æ€§ä¿®å¤é€»è¾‘"""
    print("ğŸ”§ æµ‹è¯•è®¾å¤‡ä¸€è‡´æ€§ä¿®å¤...")
    
    # 1. è§£æé…ç½®
    parser = HfArgumentParser((EnvConfig, ScriptConfig, EnhancedRewardConfig, GRPOConfig))
    env_cfg, script_cfg, reward_cfg, grpo_cfg = parser.parse_args_into_dataclasses()
    
    print(f"\nğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"  - use_model_parallel: {getattr(script_cfg, 'use_model_parallel', 'MISSING')}")
    print(f"  - ç±»å‹: {type(getattr(script_cfg, 'use_model_parallel', None))}")
    
    # 2. æ£€æŸ¥GPUç¯å¢ƒ
    print(f"\nğŸš€ GPUç¯å¢ƒ:")
    if not torch.cuda.is_available():
        print("  âŒ CUDAä¸å¯ç”¨")
        return False
    
    gpu_count = torch.cuda.device_count()
    print(f"  - GPUæ•°é‡: {gpu_count}")
    
    for i in range(gpu_count):
        props = torch.cuda.get_device_properties(i)
        memory_gb = props.total_memory / 1024**3
        allocated = torch.cuda.memory_allocated(i) / 1024**3
        print(f"    GPU {i}: {props.name}, {memory_gb:.1f}GBæ€»å®¹é‡, {allocated:.2f}GBå·²ç”¨")
    
    # 3. æ£€æŸ¥åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡
    print(f"\nğŸ” åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡:")
    dist_vars = ['RANK', 'LOCAL_RANK', 'WORLD_SIZE', 'MASTER_ADDR', 'MASTER_PORT']
    has_dist_vars = False
    for var in dist_vars:
        value = os.environ.get(var)
        if value:
            print(f"  - {var}: {value}")
            has_dist_vars = True
    
    if not has_dist_vars:
        print("  âœ… æ— åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡")
    
    # 4. æ¨¡æ‹Ÿå¤šGPUç¯å¢ƒæ£€æµ‹é€»è¾‘
    print(f"\nğŸ”§ æ¨¡æ‹Ÿå¤šGPUç¯å¢ƒæ£€æµ‹:")
    
    user_specified_model_parallel = getattr(script_cfg, 'use_model_parallel', None)
    print(f"  - ç”¨æˆ·è®¾ç½®çš„æ¨¡å‹å¹¶è¡Œ: {user_specified_model_parallel}")
    
    # æ¨¡æ‹Ÿæ£€æµ‹é€»è¾‘
    if user_specified_model_parallel is False:
        detected_strategy = "single_gpu (ç”¨æˆ·æ˜ç¡®ç¦ç”¨)"
        use_model_parallel = False
    elif user_specified_model_parallel is True:
        if gpu_count >= 2:
            detected_strategy = "model_parallel (ç”¨æˆ·æ˜ç¡®å¯ç”¨)"
            use_model_parallel = True
        else:
            print(f"  âŒ é”™è¯¯ï¼šè¦æ±‚æ¨¡å‹å¹¶è¡Œä½†GPUä¸è¶³({gpu_count}<2)")
            return False
    elif user_specified_model_parallel is None and gpu_count >= 2:
        detected_strategy = "model_parallel (è‡ªåŠ¨å¯ç”¨)"
        use_model_parallel = True
    else:
        detected_strategy = "single_gpu (é»˜è®¤)"
        use_model_parallel = False
    
    print(f"  - æ£€æµ‹åˆ°çš„ç­–ç•¥: {detected_strategy}")
    print(f"  - ä½¿ç”¨æ¨¡å‹å¹¶è¡Œ: {use_model_parallel}")
    
    # 5. æ¨¡æ‹Ÿè®­ç»ƒç­–ç•¥é…ç½®
    print(f"\nâš™ï¸ æ¨¡æ‹Ÿè®­ç»ƒç­–ç•¥é…ç½®:")
    
    is_distributed = any(var in os.environ for var in ['RANK', 'LOCAL_RANK', 'WORLD_SIZE'])
    print(f"  - æ£€æµ‹åˆ°åˆ†å¸ƒå¼ç¯å¢ƒ: {is_distributed}")
    
    if use_model_parallel and gpu_count >= 2:
        if is_distributed:
            final_strategy = "model_parallel_only (æ¸…ç†åˆ†å¸ƒå¼å˜é‡)"
            print("  ğŸ§¹ éœ€è¦æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡")
        else:
            final_strategy = "model_parallel_single_process"
    elif is_distributed and not use_model_parallel:
        final_strategy = "distributed_data_parallel"
    else:
        final_strategy = "single_gpu"
    
    print(f"  - æœ€ç»ˆç­–ç•¥: {final_strategy}")
    
    # 6. æµ‹è¯•å¼ é‡æ“ä½œ
    if use_model_parallel and gpu_count >= 2:
        print(f"\nğŸ§ª æµ‹è¯•è·¨è®¾å¤‡å¼ é‡æ“ä½œ:")
        try:
            # åˆ›å»ºåœ¨ä¸åŒè®¾å¤‡ä¸Šçš„å¼ é‡
            tensor_0 = torch.randn(10, 10).cuda(0)
            tensor_1 = torch.randn(10, 10).cuda(1)
            
            print(f"  - tensor_0 è®¾å¤‡: {tensor_0.device}")
            print(f"  - tensor_1 è®¾å¤‡: {tensor_1.device}")
            
            # æµ‹è¯•å¯èƒ½å¯¼è‡´é”™è¯¯çš„æ“ä½œ
            try:
                # è¿™ç§æ“ä½œåœ¨æ¨¡å‹å¹¶è¡Œä¸­å¯èƒ½ä¼šå¯¼è‡´è®¾å¤‡ä¸ä¸€è‡´é”™è¯¯
                result = tensor_0 + tensor_1  # è¿™ä¼šå¤±è´¥
                print("  âŒ æ„å¤–ï¼šè·¨è®¾å¤‡æ“ä½œæˆåŠŸäº†")
            except RuntimeError as e:
                if "Expected all tensors to be on the same device" in str(e):
                    print("  âœ… ç¡®è®¤ï¼šæ£€æµ‹åˆ°è·¨è®¾å¤‡å¼ é‡é”™è¯¯")
                    print(f"    é”™è¯¯ä¿¡æ¯: {e}")
                else:
                    print(f"  âš ï¸ å…¶ä»–é”™è¯¯: {e}")
            
            # æµ‹è¯•ä¿®å¤æ–¹æ³•
            print("  ğŸ”§ æµ‹è¯•ä¿®å¤æ–¹æ³•:")
            tensor_1_fixed = tensor_1.to(tensor_0.device)
            result = tensor_0 + tensor_1_fixed
            print(f"  âœ… ä¿®å¤åæ“ä½œæˆåŠŸï¼Œç»“æœè®¾å¤‡: {result.device}")
            
        except Exception as e:
            print(f"  âŒ å¼ é‡æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    print(f"\nâœ… è®¾å¤‡ä¸€è‡´æ€§ä¿®å¤æµ‹è¯•å®Œæˆ")
    return True

def main():
    """ä¸»å‡½æ•°"""
    try:
        print("ğŸš€ å¼€å§‹è®¾å¤‡ä¸€è‡´æ€§ä¿®å¤è°ƒè¯•...")
        success = test_device_consistency_fix()
        
        if success:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
            return 0
        else:
            print("\nâŒ æµ‹è¯•å¤±è´¥ï¼")
            return 1
            
    except Exception as e:
        print(f"\nğŸ’¥ è°ƒè¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code) 