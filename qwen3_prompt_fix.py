# qwen3_prompt_fix.py - å®Œæ•´ç‰ˆæœ¬ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„å¯¼å…¥

import re
import os
import logging
import json
import time
import random
import numpy as np
import torch
from typing import List, Tuple, Dict, Any, Optional
from datasets import Dataset
from transformers import (
    TrainerCallback, 
    TrainingArguments, 
    TrainerState, 
    TrainerControl,
    GenerationConfig
)
from datetime import datetime

# è·å–logger
logger = logging.getLogger(__name__)

# å¯¼å…¥å¿…è¦çš„å·¥å…·å‡½æ•°
try:
    from grpo_project.utils.file_ops import validate_and_update_dataset_paths
    from grpo_project.utils.prompt_utils import enhance_prompt_func
    from grpo_project.utils.verilog_utils import assess_code_quality
except ImportError:
    logger.warning("æŸäº›grpo_project.utilså‡½æ•°å¯¼å…¥å¤±è´¥ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")
    
    # ç®€åŒ–ç‰ˆæœ¬çš„å‡½æ•°
    def validate_and_update_dataset_paths(dataset, dataset_base_path=None):
        """ç®€åŒ–ç‰ˆæœ¬çš„è·¯å¾„éªŒè¯å‡½æ•°"""
        if dataset is None:
            return []
        
        processed_examples = []
        for example in dataset:
            # åŸºæœ¬éªŒè¯
            required_keys = ['prompt', 'testbench_path', 'expected_total_tests', 'reference_verilog_path']
            if all(key in example and example[key] is not None for key in required_keys):
                processed_examples.append(example)
        
        return processed_examples
    
    def enhance_prompt_func(example):
        """ç®€åŒ–ç‰ˆæœ¬çš„æç¤ºå¢å¼ºå‡½æ•°"""
        if isinstance(example, dict) and 'prompt' in example:
            # ä¿æŒåŸæ ·æˆ–åšåŸºæœ¬å¤„ç†
            return {
                "prompt": example['prompt'],
                "original_prompt_for_debug": example.get('prompt', '')
            }
        return example
    
    def assess_code_quality(code):
        """ç®€åŒ–ç‰ˆæœ¬çš„ä»£ç è´¨é‡è¯„ä¼°"""
        if not code:
            return {"complexity": 0, "readability": 0, "efficiency": 0, "structure": 0}
        
        lines = code.split('\n')
        non_empty_lines = [line.strip() for line in lines if line.strip()]
        
        # ç®€å•çš„è´¨é‡è¯„ä¼°
        has_module = 'module' in code.lower()
        has_endmodule = 'endmodule' in code.lower()
        has_comments = any('//' in line or '/*' in line for line in lines)
        
        return {
            "complexity": 0.8 if has_module and has_endmodule else 0.3,
            "readability": 0.7 if has_comments else 0.5,
            "efficiency": 0.6,
            "structure": 0.8 if has_module and has_endmodule else 0.3
        }

# ä¿®å¤1: wrap_prompt_for_qwen3å‡½æ•°
def wrap_prompt_for_qwen3(example: Dict[str, Any]) -> Dict[str, Any]:
    """ä¸ºQwen3æ¨¡å‹åŒ…è£…promptçš„å‡½æ•°"""
    enhanced_content = example.get("prompt")
    
    # Qwen3çš„ç³»ç»Ÿæç¤º
    system_message = """You are a Verilog expert. Please provide your solution in the following strict format:

<think>
Your detailed analysis and thinking process here
</think>

```verilog
Your complete Verilog module code here
```

Requirements:
- The <think> section must contain your reasoning
- The ```verilog section must contain complete, compilable Verilog code
- Follow the exact format shown above"""
    
    if enhanced_content and isinstance(enhanced_content, str):
        # ä¿ç•™åŸå§‹çš„å¢å¼ºåæç¤º
        example["original_enhanced_prompt"] = enhanced_content
        
        # æ„å»ºQwen3æ ¼å¼çš„å¯¹è¯æç¤º
        example["prompt"] = f"<|im_start|>system\n{system_message}<|im_end|>\n<|im_start|>user\n{enhanced_content.strip()}<|im_end|>\n<|im_start|>assistant\n"
    else:
        logger.warning(f"é‡åˆ°éå­—ç¬¦ä¸²æˆ–ç©ºprompt: {enhanced_content}")
        example["prompt"] = f"<|im_start|>system\n{system_message}<|im_end|>\n<|im_start|>user\nPlease design a Verilog module.<|im_end|>\n<|im_start|>assistant\n"
        example["original_enhanced_prompt"] = "Default prompt due to invalid input"
    
    return example

# ä¿®å¤2: å¢å¼ºçš„parse_llm_completionå‡½æ•° - æ›´å¥½åœ°å¤„ç†Qwen3è¾“å‡º
def parse_llm_completion_qwen3(completion_text: str, debug_prompt: Optional[str] = None, 
                              debug_context: Optional[Dict[str, Any]] = None) -> Tuple[Optional[str], Optional[str]]:
    """
    ä¸“é—¨ä¸ºQwen3ä¼˜åŒ–çš„è§£æå‡½æ•°
    """
    debug_enabled = logger.isEnabledFor(logging.DEBUG)
    
    if not completion_text or not isinstance(completion_text, str):
        if debug_enabled:
            logger.debug("âŒ Qwen3è§£æ: è¾“å…¥ä¸ºç©ºæˆ–æ— æ•ˆ")
        return None, None
    
    completion_text = completion_text.strip()
    
    if debug_enabled:
        logger.debug("="*60)
        logger.debug("ğŸ” Qwen3è¾“å‡ºè§£æå¼€å§‹")
        logger.debug(f"è¾“å…¥é•¿åº¦: {len(completion_text)} å­—ç¬¦")
        logger.debug(f"è¾“å…¥é¢„è§ˆ: {completion_text[:200]}...")
        logger.debug("="*60)
    
    reasoning_part = None
    code_part = None
    
    try:
        # æ­¥éª¤1: æ¸…ç†Qwen3ç‰¹æœ‰çš„æ ‡è®°
        cleaned_text = completion_text
        
        # ç§»é™¤å¯èƒ½çš„å¯¹è¯æ ‡è®°æ®‹ç•™
        qwen_markers = [
            r'<\|im_start\|>.*?<\|im_end\|>',
            r'<\|im_start\|>assistant\n?',
            r'<\|im_end\|>',
        ]
        
        for marker in qwen_markers:
            cleaned_text = re.sub(marker, '', cleaned_text, flags=re.DOTALL)
        
        cleaned_text = cleaned_text.strip()
        
        if debug_enabled:
            logger.debug(f"æ¸…ç†åæ–‡æœ¬é•¿åº¦: {len(cleaned_text)} å­—ç¬¦")
            logger.debug(f"æ¸…ç†åé¢„è§ˆ: {cleaned_text[:200]}...")
        
        # æ­¥éª¤2: æå–<think>éƒ¨åˆ†
        think_pattern = r'<think>(.*?)</think>'
        think_match = re.search(think_pattern, cleaned_text, re.DOTALL | re.IGNORECASE)
        
        if think_match:
            reasoning_part = think_match.group(1).strip()
            if debug_enabled:
                logger.debug(f"âœ… æ‰¾åˆ°<think>å—: {len(reasoning_part)} å­—ç¬¦")
                logger.debug(f"æ¨ç†é¢„è§ˆ: {reasoning_part[:150]}...")
        else:
            if debug_enabled:
                logger.debug("âŒ æœªæ‰¾åˆ°<think>å—")
                # æ£€æŸ¥æ˜¯å¦æœ‰ä¸å®Œæ•´çš„thinkæ ‡ç­¾
                if '<think>' in cleaned_text.lower():
                    logger.debug("âš ï¸ æ‰¾åˆ°å¼€å§‹æ ‡ç­¾ä½†ç¼ºå°‘ç»“æŸæ ‡ç­¾")
                    # å°è¯•æå–å¼€å§‹æ ‡ç­¾åçš„å†…å®¹ä½œä¸ºæ¨ç†
                    think_start = cleaned_text.lower().find('<think>')
                    if think_start >= 0:
                        potential_reasoning = cleaned_text[think_start + 7:].split('```')[0].strip()
                        if len(potential_reasoning) > 20:
                            reasoning_part = potential_reasoning
                            if debug_enabled:
                                logger.debug(f"ğŸ”„ æ¢å¤çš„æ¨ç†: {len(reasoning_part)} å­—ç¬¦")
        
        # æ­¥éª¤3: æå–Verilogä»£ç å—
        verilog_patterns = [
            r'```verilog\s*(.*?)\s*```',  # æ ‡å‡†Verilogå—
            r'```\s*(module\s+.*?endmodule)\s*```',  # é€šç”¨ä»£ç å—ä¸­çš„module
            r'```(?:systemverilog|sv)?\s*(.*?)\s*```',  # SystemVerilogæˆ–å…¶ä»–å˜ä½“
        ]
        
        for i, pattern in enumerate(verilog_patterns):
            code_match = re.search(pattern, cleaned_text, re.DOTALL | re.IGNORECASE)
            if code_match:
                code_part = code_match.group(1).strip()
                if debug_enabled:
                    logger.debug(f"âœ… æ‰¾åˆ°ä»£ç å—(æ¨¡å¼{i+1}): {len(code_part)} å­—ç¬¦")
                    logger.debug(f"ä»£ç é¢„è§ˆ: {code_part[:150]}...")
                break
        
        if not code_part:
            if debug_enabled:
                logger.debug("âŒ æœªæ‰¾åˆ°æ ‡å‡†ä»£ç å—ï¼Œå°è¯•ç›´æ¥æå–module")
            
            # æ­¥éª¤4: ç›´æ¥æå–module...endmodule
            module_pattern = r'(module\s+\w+.*?endmodule)'
            module_match = re.search(module_pattern, cleaned_text, re.DOTALL | re.IGNORECASE)
            
            if module_match:
                code_part = module_match.group(1).strip()
                if debug_enabled:
                    logger.debug(f"âœ… ç›´æ¥æå–module: {len(code_part)} å­—ç¬¦")
            else:
                if debug_enabled:
                    logger.debug("âŒ æœªæ‰¾åˆ°module...endmoduleæ¨¡å¼")
        
        # æ­¥éª¤5: æœ€ç»ˆéªŒè¯å’Œæ¸…ç†
        if reasoning_part:
            # æ¸…ç†æ¨ç†éƒ¨åˆ†
            reasoning_part = re.sub(r'^[\s\n]*', '', reasoning_part)
            reasoning_part = re.sub(r'[\s\n]*$', '', reasoning_part)
            
            # ç§»é™¤å¯èƒ½çš„æç¤ºæ€§æ–‡å­—
            reasoning_part = re.sub(r'^(.*thinking.*?[:ï¼š]\s*)', '', reasoning_part, flags=re.IGNORECASE)
            
            if len(reasoning_part) < 10:
                if debug_enabled:
                    logger.debug(f"âš ï¸ æ¨ç†éƒ¨åˆ†å¤ªçŸ­({len(reasoning_part)}å­—ç¬¦)ï¼Œè®¾ä¸ºNone")
                reasoning_part = None
        
        if code_part:
            # æ¸…ç†ä»£ç éƒ¨åˆ†
            code_part = re.sub(r'^[\s\n]*', '', code_part)
            code_part = re.sub(r'[\s\n]*$', '', code_part)
            
            # éªŒè¯ä»£ç è´¨é‡
            if 'module' not in code_part.lower():
                if debug_enabled:
                    logger.debug("âš ï¸ ä»£ç ä¸­æ²¡æœ‰'module'å…³é”®å­—")
            
            if len(code_part) < 20:
                if debug_enabled:
                    logger.debug(f"âš ï¸ ä»£ç å¤ªçŸ­({len(code_part)}å­—ç¬¦)ï¼Œè®¾ä¸ºNone")
                code_part = None
        
        # æœ€ç»ˆç»“æœè®°å½•
        if debug_enabled:
            logger.debug("="*40)
            logger.debug("ğŸ¯ è§£æç»“æœ:")
            logger.debug(f"  æ¨ç†éƒ¨åˆ†: {'âœ…' if reasoning_part else 'âŒ'} ({len(reasoning_part) if reasoning_part else 0} å­—ç¬¦)")
            logger.debug(f"  ä»£ç éƒ¨åˆ†: {'âœ…' if code_part else 'âŒ'} ({len(code_part) if code_part else 0} å­—ç¬¦)")
            
            if not reasoning_part and not code_part:
                logger.debug("âŒ å®Œå…¨è§£æå¤±è´¥")
                logger.debug("ğŸ” è¯Šæ–­ä¿¡æ¯:")
                logger.debug(f"  åŒ…å«'<think>': {'<think>' in cleaned_text.lower()}")
                logger.debug(f"  åŒ…å«'</think>': {'</think>' in cleaned_text.lower()}")
                logger.debug(f"  åŒ…å«'```verilog': {'```verilog' in cleaned_text.lower()}")
                logger.debug(f"  åŒ…å«'```': {'```' in cleaned_text}")
                logger.debug(f"  åŒ…å«'module': {'module' in cleaned_text.lower()}")
                logger.debug(f"  åŒ…å«'endmodule': {'endmodule' in cleaned_text.lower()}")
            elif reasoning_part and code_part:
                logger.debug("âœ… å®Œå…¨è§£ææˆåŠŸ")
            else:
                logger.debug("âš ï¸ éƒ¨åˆ†è§£ææˆåŠŸ")
            
            logger.debug("="*60)
    
    except Exception as e:
        logger.error(f"âŒ Qwen3è§£æå¼‚å¸¸: {e}", exc_info=True)
        if debug_enabled:
            logger.debug("å°è¯•åº”æ€¥æ¢å¤...")
        
        # åº”æ€¥æ¢å¤
        if 'module' in completion_text.lower() and 'endmodule' in completion_text.lower():
            try:
                emergency_match = re.search(r'(module\s+.*?endmodule)', completion_text, re.DOTALL | re.IGNORECASE)
                if emergency_match:
                    code_part = emergency_match.group(1).strip()
                    if debug_enabled:
                        logger.debug(f"ğŸ†˜ åº”æ€¥æ¢å¤æˆåŠŸ: {len(code_part)} å­—ç¬¦")
            except Exception as e_emergency:
                if debug_enabled:
                    logger.debug(f"âŒ åº”æ€¥æ¢å¤å¤±è´¥: {e_emergency}")
    
    return reasoning_part, code_part

# ä¿®å¤3: æ›´æ–°parse_llm_completion_with_contextå‡½æ•°
def parse_llm_completion_with_context_qwen3(completion_text: str, prompt: str = None, 
                                           step: int = None, sample_idx: int = None) -> Tuple[Optional[str], Optional[str]]:
    """
    ä¸“é—¨ä¸ºQwen3ä¼˜åŒ–çš„å¸¦ä¸Šä¸‹æ–‡çš„è§£æå‡½æ•°
    """
    debug_context = {
        "model": "qwen3",
        "step": step,
        "sample_idx": sample_idx,
        "prompt_preview": prompt[:100] if prompt else None
    }
    
    return parse_llm_completion_qwen3(completion_text, debug_prompt=prompt, debug_context=debug_context)

# ä¿®å¤4: Qwen3æ¨ç†å›è°ƒçš„ç”Ÿæˆé…ç½®
class Qwen3InferenceCallback(TrainerCallback):
    """ä¸“é—¨ä¸ºQwen3ä¼˜åŒ–çš„æ¨ç†å›è°ƒ"""
    
    def __init__(self, tokenizer, eval_dataset=None, num_samples=2, eval_every_n_steps=25, 
                 max_new_tokens=512, max_seq_length=4096, output_dir=None):
        self.tokenizer = tokenizer
        self.eval_dataset = eval_dataset
        self.num_samples = num_samples
        self.eval_every_n_steps = eval_every_n_steps
        self.max_new_tokens = max_new_tokens
        self.max_seq_length = max_seq_length
        self.output_dir = output_dir
        
        if output_dir:
            self.samples_dir = os.path.join(output_dir, "qwen3_generated_samples")
            os.makedirs(self.samples_dir, exist_ok=True)
    
    def on_step_end(self, args, state, control, model=None, **kwargs):
        if state.global_step > 0 and state.global_step % self.eval_every_n_steps == 0:
            if model is None or args.local_rank > 0:
                return
            
            logger.info(f"\nğŸ¤– === Qwen3æ¨ç†å›è°ƒ - æ­¥æ•° {state.global_step} ===")
            
            if self.eval_dataset and len(self.eval_dataset) > 0:
                sample_indices = random.sample(range(len(self.eval_dataset)), 
                                             min(self.num_samples, len(self.eval_dataset)))
                
                for i, idx in enumerate(sample_indices):
                    sample = self.eval_dataset[idx]
                    prompt = sample['prompt']
                    
                    logger.info(f"\nğŸ“ Qwen3æ ·æœ¬ {i+1}/{len(sample_indices)} (æ•°æ®é›†ç´¢å¼•: {idx})")
                    logger.info(f"ç­‰çº§: {sample.get('level', 'unknown')}")
                    logger.info(f"å¤æ‚åº¦: {sample.get('complexity_score', 'unknown')}")
                    
                    try:
                        result = self._generate_qwen3_sample(model, prompt, state.global_step)
                        
                        logger.info(f"ç”Ÿæˆæ—¶é—´: {result.get('generation_time', 0):.2f}ç§’")
                        
                        # éªŒè¯ç”Ÿæˆè´¨é‡
                        reasoning = result.get('reasoning')
                        code = result.get('code')
                        
                        logger.info(f"æ¨ç†éƒ¨åˆ†: {'âœ…' if reasoning else 'âŒ'} ({len(reasoning) if reasoning else 0} å­—ç¬¦)")
                        logger.info(f"ä»£ç éƒ¨åˆ†: {'âœ…' if code else 'âŒ'} ({len(code) if code else 0} å­—ç¬¦)")
                        
                        if code:
                            # ç®€å•éªŒè¯ä»£ç è´¨é‡
                            has_module = 'module' in code.lower()
                            has_endmodule = 'endmodule' in code.lower()
                            logger.info(f"ä»£ç è´¨é‡: module={'âœ…' if has_module else 'âŒ'}, endmodule={'âœ…' if has_endmodule else 'âŒ'}")
                        
                        # ä¿å­˜æ ·æœ¬
                        if self.output_dir:
                            self._save_qwen3_sample(state.global_step, i, sample, result)
                    
                    except Exception as e:
                        logger.error(f"Qwen3ç”Ÿæˆæ ·æœ¬å¤±è´¥: {e}", exc_info=True)
            
            logger.info(f"ğŸ¤– === Qwen3æ¨ç†å›è°ƒç»“æŸ ===\n")
    
    def _generate_qwen3_sample(self, model, prompt, step):
        """ç”ŸæˆQwen3æ ·æœ¬"""
        model.eval()
        
        # Qwen3ç‰¹å®šçš„ç”Ÿæˆé…ç½®
        generation_config = {
            "max_new_tokens": self.max_new_tokens,
            "do_sample": True,
            "temperature": 0.7,  # Qwen3æ¨èçš„æ¸©åº¦
            "top_p": 0.8,        # Qwen3æ¨èçš„top_p
            "top_k": 40,         # Qwen3æ¨èçš„top_k
            "repetition_penalty": 1.05,  # è½»å¾®çš„é‡å¤æƒ©ç½š
            "pad_token_id": self.tokenizer.pad_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
        }
        
        # å‡†å¤‡è¾“å…¥
        max_prompt_len = self.max_seq_length - self.max_new_tokens
        inputs = self.tokenizer(
            prompt, 
            return_tensors="pt", 
            truncation=True, 
            max_length=max_prompt_len,
            padding=False
        ).to(model.device)
        
        start_time = time.time()
        
        try:
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    **generation_config
                )
            
            generation_time = time.time() - start_time
            
            # è§£ç ç”Ÿæˆçš„éƒ¨åˆ†
            generated_tokens = outputs[0][inputs.input_ids.shape[1]:]
            generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
            
            # ä½¿ç”¨Qwen3ä¼˜åŒ–çš„è§£æ
            reasoning, code = parse_llm_completion_qwen3(
                generated_text,
                debug_prompt=prompt,
                debug_context={"step": step, "model": "qwen3"}
            )
            
            model.train()  # æ¢å¤è®­ç»ƒæ¨¡å¼
            
            return {
                'reasoning': reasoning,
                'code': code,
                'raw_output': generated_text,
                'generation_time': generation_time,
                'step': step,
                'generation_config': generation_config
            }
        
        except Exception as e:
            logger.error(f"Qwen3ç”Ÿæˆå¼‚å¸¸: {e}", exc_info=True)
            model.train()
            return {
                'reasoning': None,
                'code': None,
                'raw_output': f"Generation failed: {e}",
                'generation_time': time.time() - start_time,
                'step': step,
                'error': str(e)
            }
    
    def _save_qwen3_sample(self, step, sample_idx, original_sample, generated_result):
        """ä¿å­˜Qwen3ç”Ÿæˆæ ·æœ¬"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        task_id = str(original_sample.get('task_id', f"sample_{sample_idx}")).replace('/', '_')
        
        filename = f"qwen3_step_{step}_{task_id}_{timestamp}.json"
        filepath = os.path.join(self.samples_dir, filename)
        
        # è·å–åŸå§‹é—®é¢˜æè¿°
        orig_prompt = original_sample.get('original_prompt_for_debug', 
                                        original_sample.get('original_enhanced_prompt', 'N/A'))
        
        sample_data = {
            "metadata": {
                "step": step,
                "timestamp": datetime.now().isoformat(),
                "model": "qwen3",
                "sample_idx": sample_idx,
                "task_id": original_sample.get('task_id')
            },
            "original_sample": {
                "level": original_sample.get('level'),
                "complexity_score": original_sample.get('complexity_score'),
                "category": original_sample.get('category'),
                "original_problem": orig_prompt[:500] + "..." if len(orig_prompt) > 500 else orig_prompt,
                "testbench_path": original_sample.get('testbench_path', ''),
            },
            "generation": {
                "reasoning": generated_result.get('reasoning'),
                "code": generated_result.get('code'),
                "raw_output_preview": generated_result.get('raw_output', '')[:300] + "...",
                "generation_time_seconds": generated_result.get('generation_time'),
                "generation_config": generated_result.get('generation_config', {}),
                "error": generated_result.get('error')
            },
            "quality_assessment": {
                "has_reasoning": generated_result.get('reasoning') is not None,
                "has_code": generated_result.get('code') is not None,
                "reasoning_length": len(generated_result.get('reasoning', '')),
                "code_length": len(generated_result.get('code', '')),
            }
        }
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(sample_data, f, indent=2, ensure_ascii=False)
            logger.debug(f"Qwen3æ ·æœ¬å·²ä¿å­˜: {filepath}")
        except Exception as e:
            logger.error(f"ä¿å­˜Qwen3æ ·æœ¬å¤±è´¥: {e}")

# ä¿®å¤5: åœ¨ä¸»æ•°æ®å¤„ç†æµç¨‹ä¸­ä½¿ç”¨Qwen3æ ¼å¼
def qwen3_dataset_processing_pipeline(raw_ds, ds_dir, script_cfg_val):
    """ä¸“é—¨ä¸ºQwen3ä¼˜åŒ–çš„æ•°æ®é›†å¤„ç†æµç¨‹"""
    logger.info("ğŸ¤– å¯åŠ¨Qwen3æ•°æ®é›†å¤„ç†æµç¨‹...")
    
    # ç¬¬ä¸€æ­¥ï¼šåŸºç¡€æ•°æ®éªŒè¯å’Œç±»å‹å¤„ç†
    def preprocess_for_qwen3(example):
        if not isinstance(example, dict):
            return None
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        required_fields = ["prompt", "testbench_path", "expected_total_tests", "reference_verilog_path"]
        for field in required_fields:
            if field not in example or example[field] is None:
                return None
        
        # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
        processed = {}
        for key, value in example.items():
            if key in ["prompt", "testbench_path", "reference_verilog_path"]:
                processed[key] = str(value)
            elif key == "expected_total_tests":
                try:
                    processed[key] = int(value) if isinstance(value, str) and value.isdigit() else int(value)
                except (ValueError, TypeError):
                    return None
            else:
                processed[key] = value
        
        return processed
    
    # åº”ç”¨é¢„å¤„ç†
    processed_ds = raw_ds.map(preprocess_for_qwen3, num_proc=1).filter(lambda x: x is not None)
    logger.info(f"Qwen3é¢„å¤„ç†å: {len(processed_ds)} è¡Œ")
    
    if len(processed_ds) == 0:
        return processed_ds
    
    # ç¬¬äºŒæ­¥ï¼šè·¯å¾„éªŒè¯
    validated_examples = validate_and_update_dataset_paths(processed_ds, ds_dir)
    if not validated_examples:
        return Dataset.from_list([])
    
    validated_ds = Dataset.from_list(validated_examples)
    logger.info(f"Qwen3è·¯å¾„éªŒè¯å: {len(validated_ds)} è¡Œ")
    
    # ç¬¬ä¸‰æ­¥ï¼šæç¤ºå¢å¼º
    enhanced_ds = validated_ds.map(enhance_prompt_func, num_proc=1)
    logger.info(f"Qwen3æç¤ºå¢å¼ºå: {len(enhanced_ds)} è¡Œ")
    
    # ç¬¬å››æ­¥ï¼šQwen3æ ¼å¼åŒ…è£…
    qwen3_ds = enhanced_ds.map(wrap_prompt_for_qwen3, num_proc=1)
    logger.info(f"Qwen3æ ¼å¼åŒ…è£…å: {len(qwen3_ds)} è¡Œ")
    
    # ç¬¬äº”æ­¥ï¼šæ·»åŠ ç¼ºå¤±çš„åˆ—
    final_cols = [
        "prompt",                       # Qwen3æ ¼å¼åŒ–åçš„prompt
        "original_enhanced_prompt",     # enhance_prompt_funcçš„ç›´æ¥è¾“å‡º
        "testbench_path",
        "expected_total_tests",
        "reference_verilog_path", 
        "original_prompt_for_debug",    # æœ€åˆå§‹çš„ç”¨æˆ·è¾“å…¥
        "level",
        "complexity_score",
        "category",
        "difficulty",
        "task_id"
    ]
    
    def add_missing_columns(example):
        """æ·»åŠ ç¼ºå¤±çš„åˆ—"""
        result = example.copy()
        
        # æ·»åŠ é»˜è®¤å€¼
        if 'level' not in result or result['level'] is None:
            result['level'] = 'intermediate'
        if 'complexity_score' not in result or result['complexity_score'] is None:
            result['complexity_score'] = 5.0
        if 'category' not in result or result['category'] is None:
            result['category'] = 'Unknown'
        if 'difficulty' not in result or result['difficulty'] is None:
            result['difficulty'] = 'C'
        if 'task_id' not in result or result['task_id'] is None:
            result['task_id'] = f"task_{hash(str(result.get('prompt', random.random())))}"
        if 'original_prompt_for_debug' not in result:
            result['original_prompt_for_debug'] = result.get('original_enhanced_prompt', result.get('prompt', ''))
        
        return result
    
    final_ds = qwen3_ds.map(add_missing_columns, num_proc=1)
    logger.info(f"Qwen3åˆ—è¡¥å…¨å: {len(final_ds)} è¡Œ")
    
    # éªŒè¯Qwen3æ ¼å¼
    if len(final_ds) > 0:
        sample_prompt = final_ds[0]['prompt']
        logger.debug(f"æ ·æœ¬prompté¢„è§ˆ: {sample_prompt[:200]}...")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«Qwen3çš„å…³é”®æ ‡è®°
        has_system = "<|im_start|>system" in sample_prompt
        has_user = "<|im_start|>user" in sample_prompt
        has_assistant = "<|im_start|>assistant" in sample_prompt
        
        if has_system and has_user and has_assistant:
            logger.info("âœ… Qwen3æ ¼å¼éªŒè¯é€šè¿‡")
        else:
            logger.warning("âš ï¸ Qwen3æ ¼å¼å¯èƒ½ä¸å®Œæ•´")
            logger.info(f"æ ¼å¼æ£€æŸ¥: system={has_system}, user={has_user}, assistant={has_assistant}")
    
    return final_ds

# ä¿®å¤6: æ›´æ–°ç”Ÿæˆé…ç½®çš„è¾…åŠ©å‡½æ•°
def setup_qwen3_generation_config(model, tokenizer, script_cfg):
    """è®¾ç½®Qwen3çš„ç”Ÿæˆé…ç½®"""
    from transformers import GenerationConfig
    
    logger.info("ğŸ¤– è®¾ç½®Qwen3ç”Ÿæˆé…ç½®...")
    
    # ç¡®ä¿tokenizeré…ç½®æ­£ç¡®
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
        logger.info("è®¾ç½®pad_tokenä¸ºeos_token")
    
    # æ›´æ–°æ¨¡å‹é…ç½®
    if hasattr(model, 'config'):
        model.config.pad_token_id = tokenizer.pad_token_id
        model.config.eos_token_id = tokenizer.eos_token_id
    
    # è®¾ç½®ç”Ÿæˆé…ç½®
    generation_config = GenerationConfig(
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
        max_new_tokens=script_cfg.max_seq_length // 2,
        do_sample=True,
        temperature=getattr(script_cfg, 'gen_temperature', 0.7),
        top_p=getattr(script_cfg, 'gen_top_p', 0.8),
        top_k=getattr(script_cfg, 'gen_top_k', 40),
        repetition_penalty=getattr(script_cfg, 'gen_repetition_penalty', 1.05),
        length_penalty=getattr(script_cfg, 'gen_length_penalty', 1.0),
    )
    
    model.generation_config = generation_config
    logger.info("âœ… Qwen3ç”Ÿæˆé…ç½®è®¾ç½®å®Œæˆ")
    
    return model, tokenizer

# åœ¨ä¸»è®­ç»ƒè„šæœ¬ä¸­çš„é›†æˆç¤ºä¾‹
def integrate_qwen3_fixes():
    """åœ¨train.pyä¸­é›†æˆQwen3ä¿®å¤çš„ç¤ºä¾‹ä»£ç """
    return """
# åœ¨train.pyçš„mainå‡½æ•°ä¸­æ·»åŠ ä»¥ä¸‹ä¿®å¤:

# 1. æ›¿æ¢æ•°æ®é›†å¤„ç†å‡½æ•°
dataset = qwen3_dataset_processing_pipeline(dataset_raw, dataset_dir, script_cfg)

# 2. åº”ç”¨Qwen3å…¼å®¹æ€§ä¿®å¤
model, tokenizer = setup_qwen3_generation_config(model, tokenizer, script_cfg)

# 3. ä½¿ç”¨Qwen3ä¼˜åŒ–çš„è§£æå‡½æ•°
# åœ¨parse_llm_completion_with_contextçš„è°ƒç”¨å¤„ï¼Œæ›¿æ¢ä¸º:
reasoning, code = parse_llm_completion_with_context_qwen3(
    generated_text, prompt=prompt, step=step, sample_idx=sample_idx
)

# 4. ä½¿ç”¨Qwen3æ¨ç†å›è°ƒ
qwen3_inference_cb = Qwen3InferenceCallback(
    tokenizer=tokenizer,
    eval_dataset=sample_dataset_for_inf_cb,
    num_samples=script_cfg.callback_num_samples,
    eval_every_n_steps=script_cfg.callback_eval_every_n_steps,
    output_dir=script_cfg.output_dir
)
callbacks_list.append(qwen3_inference_cb)

# 5. åœ¨å¥–åŠ±è®¡ç®—ä¸­ä½¿ç”¨Qwen3è§£æ
# åœ¨calculate_enhanced_rewards_for_single_promptå‡½æ•°ä¸­:
_, code = parse_llm_completion_qwen3(full_output, debug_prompt=prompt_str)
"""