#!/usr/bin/env python3

import os
import torch
import sys
from dataclasses import asdict
from transformers import HfArgumentParser
from trl import GRPOConfig

# Add the project path
sys.path.append('/home/qhy/Research/LLM/GRPO-Clean-2')

from grpo_project.configs import EnvConfig, ScriptConfig, EnhancedRewardConfig

class TestPipeline:
    def __init__(self):
        # æ¨¡æ‹Ÿå‘½ä»¤è¡Œå‚æ•°
        test_args = [
            '--use_model_parallel', 'true',
            '--model_name_or_path', '/home/share/Qwen3-8B/models--Qwen--Qwen3-8B/snapshots/a80f5e57cce20e57b65145f4213844dec1a80834',
            '--stage1_adapter_path', '/home/share/ReasoningV/Qwen3/ckpts/sft-qwen3-lora-5epoch-origen200k-S1-20250429_211831/checkpoint-34695/',
            '--dataset_path', '/home/qhy/Research/LLM/GRPO-RV/dataset/all-with-module-2.jsonl',
            '--output_dir_base', './test_outputs',
            '--max_seq_length', '1024',
            '--script_max_prompt_length', '512',
            '--script_max_completion_length', '512',
            '--per_device_train_batch_size', '1',
            '--gradient_accumulation_steps', '2',
            '--learning_rate', '1e-5',
            '--num_train_epochs', '1',
            '--lora_rank', '32',
            '--lora_alpha', '64'
        ]
        
        # è§£æé…ç½®
        sys.argv = ['test_config.py'] + test_args
        parser = HfArgumentParser((EnvConfig, ScriptConfig, EnhancedRewardConfig, GRPOConfig))
        self.env_cfg, self.script_cfg, self.reward_cfg, self.grpo_cfg = parser.parse_args_into_dataclasses()
        
        print("ğŸ”§ æµ‹è¯•è®­ç»ƒç­–ç•¥é…ç½®")
        print("=" * 50)
        
        # æµ‹è¯•GPUç¯å¢ƒæ£€æµ‹
        self._detect_multi_gpu_environment()
        print(f"å¤šGPUä¿¡æ¯: {self.multi_gpu_info}")
        
        # æµ‹è¯•è®­ç»ƒç­–ç•¥é…ç½®
        self._configure_training_strategy()
        print(f"è®­ç»ƒç­–ç•¥: {self.training_strategy}")
        
        print("=" * 50)

    def _detect_multi_gpu_environment(self):
        """ç®€åŒ–ç‰ˆGPUç¯å¢ƒæ£€æµ‹"""
        try:
            print("ğŸ” æ£€æµ‹å¤šGPUç¯å¢ƒ...")
            
            if not torch.cuda.is_available():
                print("âš ï¸ CUDAä¸å¯ç”¨")
                self.multi_gpu_info = {'gpu_count': 0, 'use_model_parallel': False}
                return
            
            user_specified_model_parallel = getattr(self.script_cfg, 'use_model_parallel', None)
            print(f"ğŸ¯ ç”¨æˆ·æŒ‡å®šçš„æ¨¡å‹å¹¶è¡Œè®¾ç½®: {user_specified_model_parallel}")
            
            gpu_count = torch.cuda.device_count()
            print(f"ğŸ“Š æ£€æµ‹åˆ° {gpu_count} å¼ GPU")
            
            if user_specified_model_parallel is False:
                print("ğŸ¯ ç”¨æˆ·æ˜ç¡®ç¦ç”¨æ¨¡å‹å¹¶è¡Œ")
                os.environ['CUDA_VISIBLE_DEVICES'] = '0'
                self.multi_gpu_info = {
                    'gpu_count': 1,
                    'total_memory_gb': 79.2,
                    'average_memory_gb': 79.2,
                    'use_model_parallel': False
                }
                return
            
            if gpu_count < 2:
                print(f"âš ï¸ GPUæ•°é‡({gpu_count})å°‘äº2å¼ ï¼Œç¦ç”¨å¤šGPUæ¨¡å¼")
                self.multi_gpu_info = {
                    'gpu_count': gpu_count,
                    'total_memory_gb': 79.2 if gpu_count > 0 else 0,
                    'average_memory_gb': 79.2 if gpu_count > 0 else 0,
                    'use_model_parallel': False
                }
                return
            
            # æ­£å¸¸å¤šGPUæƒ…å†µ
            use_model_parallel = (user_specified_model_parallel is True or 
                                (user_specified_model_parallel is None and gpu_count >= 2))
            
            self.multi_gpu_info = {
                'gpu_count': gpu_count,
                'total_memory_gb': 79.2 * gpu_count,
                'average_memory_gb': 79.2,
                'use_model_parallel': use_model_parallel
            }
            
            if self.multi_gpu_info['use_model_parallel']:
                print("âœ… å¤šGPUæ¨¡å‹å¹¶è¡Œæ¨¡å¼å·²å¯ç”¨")
            else:
                print("ğŸ“± å°†ä½¿ç”¨å•GPUæ¨¡å¼")
                
        except Exception as e:
            print(f"âš ï¸ å¤šGPUç¯å¢ƒæ£€æµ‹å¤±è´¥: {e}")
            self.multi_gpu_info = {'gpu_count': 0, 'use_model_parallel': False}

    def _configure_training_strategy(self):
        """ç®€åŒ–ç‰ˆè®­ç»ƒç­–ç•¥é…ç½®"""
        try:
            print("ğŸ”§ é…ç½®è®­ç»ƒç­–ç•¥...")
            
            # æ£€æµ‹å½“å‰ç¯å¢ƒ
            is_distributed = (
                'RANK' in os.environ or 
                'LOCAL_RANK' in os.environ or 
                'WORLD_SIZE' in os.environ or
                getattr(self.grpo_cfg, 'local_rank', -1) >= 0
            )
            
            use_model_parallel = self.multi_gpu_info.get('use_model_parallel', False)
            gpu_count = self.multi_gpu_info.get('gpu_count', 1)
            
            print(f"ğŸ” ç¯å¢ƒæ£€æµ‹ç»“æœ:")
            print(f"  - æ£€æµ‹åˆ°åˆ†å¸ƒå¼ç¯å¢ƒ: {is_distributed}")
            print(f"  - å¯ç”¨æ¨¡å‹å¹¶è¡Œ: {use_model_parallel}")
            print(f"  - GPUæ•°é‡: {gpu_count}")
            
            # å†³å®šè®­ç»ƒç­–ç•¥
            if use_model_parallel and gpu_count >= 2:
                if is_distributed:
                    print("âš ï¸ æ£€æµ‹åˆ°åˆ†å¸ƒå¼ç¯å¢ƒä¸æ¨¡å‹å¹¶è¡Œå†²çª")
                    # æ¸…é™¤åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡
                    dist_env_vars = ['RANK', 'LOCAL_RANK', 'WORLD_SIZE', 'MASTER_ADDR', 'MASTER_PORT']
                    for var in dist_env_vars:
                        if var in os.environ:
                            print(f"ğŸ§¹ æ¸…é™¤åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡: {var}")
                            del os.environ[var]
                    
                    self.training_strategy = "model_parallel_only"
                    print("âœ… é…ç½®ä¸ºçº¯æ¨¡å‹å¹¶è¡Œæ¨¡å¼")
                else:
                    self.training_strategy = "model_parallel_single_process"
                    print("âœ… é…ç½®ä¸ºå•è¿›ç¨‹æ¨¡å‹å¹¶è¡Œæ¨¡å¼")
                    
            elif is_distributed and not use_model_parallel:
                self.training_strategy = "distributed_data_parallel"
                print("âœ… é…ç½®ä¸ºåˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œæ¨¡å¼")
                
            else:
                self.training_strategy = "single_gpu"
                print("âœ… é…ç½®ä¸ºå•GPUè®­ç»ƒæ¨¡å¼")
                
                if gpu_count > 1:
                    print("ğŸ”§ å¼ºåˆ¶å•GPUæ¨¡å¼ï¼Œé™åˆ¶ä½¿ç”¨GPU 0")
                    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
            
            print(f"ğŸ¯ æœ€ç»ˆè®­ç»ƒç­–ç•¥: {self.training_strategy}")
            
        except Exception as e:
            print(f"âš ï¸ è®­ç»ƒç­–ç•¥é…ç½®å¤±è´¥: {e}")
            self.training_strategy = "single_gpu"
            print("ğŸ”„ å›é€€åˆ°å•GPUè®­ç»ƒæ¨¡å¼")

if __name__ == "__main__":
    test = TestPipeline() 