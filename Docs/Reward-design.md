æˆ‘æ¥åˆ†ææ‚¨çš„å¥–åŠ±å‡½æ•°è®¡ç®—æœºåˆ¶ã€‚æ ¹æ®`calculator.py`æ–‡ä»¶ï¼Œå¥–åŠ±è®¡ç®—ä¸»è¦åœ¨`RewardCalculator`ç±»çš„`_calculate_single_reward`æ–¹æ³•ä¸­å®ç°ã€‚
Ran tool
```mermaind

graph TD
    A[è¾“å…¥: prompt + completion] --> B[è§£æLLMè¾“å‡º]
    B --> C{æå–åˆ°ä»£ç ?}
    C -->|å¦| D[ç¼ºå°‘ä»£ç å—æƒ©ç½š<br/>-6.0åˆ†]
    C -->|æ˜¯| E[ä»£ç è´¨é‡è¯„ä¼°]
    E --> F[VerilogéªŒè¯]
    F --> G{éªŒè¯é€šè¿‡?}
    G -->|å¦| H[ç¼–è¯‘å¤±è´¥æƒ©ç½š<br/>-8.0åˆ†]
    G -->|æ˜¯| I[ä»¿çœŸæµ‹è¯•]
    I --> J{ç¼–è¯‘æˆåŠŸ?}
    J -->|å¦| K[ç¼–è¯‘å¤±è´¥<br/>-8.0åˆ†]
    J -->|æ˜¯| L{ä»¿çœŸè¿è¡ŒæˆåŠŸ?}
    L -->|å¦| M[ä»¿çœŸå´©æºƒ<br/>-4.0åˆ†]
    L -->|æ˜¯| N{è¾“å‡ºè§£ææˆåŠŸ?}
    N -->|å¦| O[è§£æé”™è¯¯<br/>-2.0åˆ†]
    N -->|æ˜¯| P[åŠŸèƒ½æµ‹è¯•è¯„åˆ†]
    P --> Q[è®¡ç®—å„ç»„ä»¶åˆ†æ•°]
    Q --> R[åŠ æƒæ±‚å’Œ]
    R --> S[æœ€ç»ˆå¥–åŠ±]

    style D fill:#ffcccc
    style H fill:#ffcccc
    style K fill:#ffcccc
    style M fill:#ffcccc
    style O fill:#ffcccc
    style S fill:#ccffcc
```
## å¥–åŠ±å‡½æ•°è¯¦ç»†åˆ†æ

æ‚¨çš„å¥–åŠ±å‡½æ•°é‡‡ç”¨**å¤šç»´åº¦åˆ†å±‚è®¡ç®—**çš„è®¾è®¡ï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªç»„ä»¶ï¼š

### 1. åŸºç¡€æƒ©ç½šæœºåˆ¶ ğŸ“Š

**ç¼ºå°‘ä»£ç å—æƒ©ç½š**ï¼š
```53:69:LLM/GRPO-Clean-2/grpo_project/rewards/calculator.py
        if code_part and code_part.strip():
            current_funnel_metrics["code_extracted"] = True
        else:
            penalty_type = self.reward_config.missing_code_block_penalty
            current_unscaled_components["base_compilation"] = penalty_type
            total_reward = self.reward_config.get_scaled_reward(penalty_type, training_step)
```

### 2. è¾“å‡ºé•¿åº¦æ•ˆç‡å¥–åŠ± ğŸ¯

æ ¹æ®tokenæ•°é‡åŠ¨æ€è°ƒæ•´ï¼š
```255:285:LLM/GRPO-Clean-2/grpo_project/rewards/calculator.py
        # è®¡ç®—è¾“å‡ºé•¿åº¦å¥–åŠ±
        output_token_count = 0
        if tokenizer and completion_str:
            try:
                # ä½¿ç”¨tokenizerè®¡ç®—tokenæ•°é‡
                tokens = tokenizer.encode(completion_str, add_special_tokens=False)
                output_token_count = len(tokens)
                current_funnel_metrics["output_token_count"] = output_token_count
                
                # è®¡ç®—é•¿åº¦æ•ˆç‡å¥–åŠ±
                if output_token_count < self.reward_config.min_length_threshold:
                    # å¤ªçŸ­çš„è¾“å‡ºç»™äºˆæƒ©ç½š
                    current_unscaled_components["length_efficiency"] = self.reward_config.min_length_penalty
                    logger.debug(f"{log_pref}: Output too short ({output_token_count} tokens), penalty applied")
                elif output_token_count <= self.reward_config.length_efficiency_threshold:
                    # åœ¨é«˜æ•ˆèŒƒå›´å†…ï¼Œç»™äºˆå¥–åŠ±
                    current_unscaled_components["length_efficiency"] = self.reward_config.optimal_length_bonus
                    logger.debug(f"{log_pref}: Optimal length ({output_token_count} tokens), bonus applied")
                elif output_token_count <= self.reward_config.length_penalty_threshold:
                    # ä¸­ç­‰é•¿åº¦ï¼Œä¸ç»™å¥–åŠ±ä¹Ÿä¸æƒ©ç½š
                    current_unscaled_components["length_efficiency"] = 0.0
                    logger.debug(f"{log_pref}: Medium length ({output_token_count} tokens), neutral")
                else:
                    # è¶…è¿‡é˜ˆå€¼ï¼Œç»™äºˆæƒ©ç½š
                    excess_tokens = output_token_count - self.reward_config.length_penalty_threshold
                    penalty = excess_tokens * self.reward_config.length_penalty_rate
                    # é™åˆ¶æœ€å¤§æƒ©ç½šé¿å…è¿‡åº¦æƒ©ç½š
                    penalty = max(penalty, -5.0)
                    current_unscaled_components["length_efficiency"] = penalty
                    logger.debug(f"{log_pref}: Output too long ({output_token_count} tokens), penalty: {penalty:.3f}")
```

### 3. ä»£ç è´¨é‡è¯„ä¼° ğŸ”§

ç›´æ¥é›†æˆè´¨é‡æŒ‡æ ‡ï¼š
```303:317:LLM/GRPO-Clean-2/grpo_project/rewards/calculator.py
        # Code Quality Assessment (Direct Integration)
        quality_metrics = assess_code_quality(code_part)
        current_unscaled_components["efficiency"] = (
            quality_metrics.get("efficiency", 0) * self.reward_config.code_efficiency_bonus +
            quality_metrics.get("structure", 0) * self.reward_config.synthesis_friendly_bonus -
            max(0, (1 - quality_metrics.get("complexity", 1)) * self.reward_config.code_complexity_penalty)
        )
        current_unscaled_components["readability"] = quality_metrics.get("readability", 0) * self.reward_config.code_readability_bonus
```

### 4. ä»¿çœŸæµ‹è¯•å¥–åŠ± âš¡

**ç¼–è¯‘å’Œä»¿çœŸå¥–åŠ±æœºåˆ¶**ï¼š
```340:395:LLM/GRPO-Clean-2/grpo_project/rewards/calculator.py
            if not sim_res.get("compilation_success"):
                current_unscaled_components["base_compilation"] = self.reward_config.compilation_failure
                logger.info(f"{log_pref}: Compilation FAILED. Error: {sim_res.get('error_message', 'No error message')}")
            else:
                current_unscaled_components["base_compilation"] = self.reward_config.compilation_success
                current_funnel_metrics["sim_ran_successfully"] = sim_res.get("simulation_run_success", False)

                if not sim_res.get("simulation_run_success"):
                    current_unscaled_components["functional"] = self.reward_config.simulation_crash
                    logger.info(f"{log_pref}: Simulation CRASHED. Details: {sim_res.get('error_message', 'No error message')}")
                elif not sim_res.get("parsing_success"):
                    current_unscaled_components["functional"] = self.reward_config.output_parse_error
                    logger.info(f"{log_pref}: Simulation output parsing FAILED. Details: {sim_res.get('error_message', 'No error message')}")
                else:
                    p = sim_res.get("passed_tests", 0)
                    total_tests_in_output = sim_res.get("total_tests_in_output", 0)
                    current_funnel_metrics["passed_tests"] = p

                    if total_tests_in_output > 0:
                        pass_ratio = p / total_tests_in_output
                        base_functional = pass_ratio * self.reward_config.max_functional_reward
                        if p > 1: # Apply bonus only if more than one test passed
                            bonus_factor = self.reward_config.test_pass_bonus_multiplier ** (p - 1)
                            base_functional *= min(bonus_factor, 2.0) # Cap bonus factor at 2.0
                        current_unscaled_components["functional"] = base_functional
                    elif sim_res.get("all_tests_passed_by_tb", False): # All tests passed but total_tests_in_output is 0 (e.g. no specific test count in output but TB indicates pass)
                         current_unscaled_components["functional"] = self.reward_config.max_functional_reward # Consider this full marks for functional
                         # This case might also imply all tests passed for robustness bonus
                    else:
                        current_unscaled_components["functional"] = 0 # Or a small penalty if expected tests > 0

                    # Robustness Rewards
                    if sim_res.get("all_tests_passed_by_tb", False) and p == total_tests_in_output:
                        current_unscaled_components["robustness"] += self.reward_config.all_tests_passed_bonus
                    # Check for edge case handling bonus (e.g. passing all tests when there are many)
                    if p == total_tests_in_output and total_tests_in_output >= 5: # Assuming 5 or more tests indicates edge case coverage
                        current_unscaled_components["robustness"] += self.reward_config.edge_case_handling_bonus
```

### 5. æœ€ç»ˆåŠ æƒè®¡ç®— ğŸ“ˆ

**å¤šç»´åº¦åŠ æƒæ±‚å’Œ**ï¼š
```425:436:LLM/GRPO-Clean-2/grpo_project/rewards/calculator.py
        # Final Reward Calculation
        unscaled_total_reward = (
            self.reward_config.functional_weight * current_unscaled_components["functional"] +
            self.reward_config.efficiency_weight * current_unscaled_components["efficiency"] +
            self.reward_config.readability_weight * current_unscaled_components["readability"] +
            self.reward_config.robustness_weight * current_unscaled_components["robustness"] +
            self.reward_config.length_efficiency_weight * current_unscaled_components["length_efficiency"] +
            current_unscaled_components["base_compilation"] # This is the base for compilation success/failure
        )

        final_scaled_reward = self.reward_config.get_scaled_reward(unscaled_total_reward, training_step)
```

## å¥–åŠ±é…ç½®å‚æ•° âš™ï¸

ä»è„šæœ¬ä¸­å¯ä»¥çœ‹åˆ°çš„é…ç½®ï¼š
Ran tool
## å¥–åŠ±è®¡ç®—æœºåˆ¶ç‰¹ç‚¹ âœ¨

### **1. æ¼æ–—å¼è¯„ä¼°** 
- æŒ‰é¡ºåºè¯„ä¼°ï¼š**ä»£ç æå–** â†’ **ç¼–è¯‘** â†’ **ä»¿çœŸ** â†’ **æµ‹è¯•**
- æ¯ä¸ªé˜¶æ®µå¤±è´¥éƒ½æœ‰å¯¹åº”çš„æƒ©ç½šï¼Œç¡®ä¿æ¨¡å‹å­¦ä¼šåŸºç¡€è¦æ±‚

### **2. å¤šç»´åº¦æƒé‡å¹³è¡¡**
å½“å‰è„šæœ¬ä¸­çš„æƒé‡é…ç½®ï¼š
- åŠŸèƒ½æ€§ï¼š**60%** (functional_weight = 0.6)  
- æ•ˆç‡ï¼š**20%** (efficiency_weight = 0.2)
- å¯è¯»æ€§ï¼š**10%** (readability_weight = 0.1)  
- é²æ£’æ€§ï¼š**10%** (robustness_weight = 0.1)
- é•¿åº¦æ•ˆç‡ï¼šä½¿ç”¨`length_efficiency_weight`

### **3. å¥–åŠ±èŒƒå›´**
- **æ­£å‘å¥–åŠ±**ï¼šç¼–è¯‘æˆåŠŸ(+2.0)ï¼Œæµ‹è¯•é€šè¿‡(æœ€å¤§+15.0)ï¼Œå…¨é€šè¿‡å¥–åŠ±(+5.0)
- **è´Ÿå‘æƒ©ç½š**ï¼šç¼–è¯‘å¤±è´¥(-4.0)ï¼Œç¼ºå°‘ä»£ç (-6.0)ï¼Œä»¿çœŸå´©æºƒ(-4.0)

### **4. è‡ªé€‚åº”æœºåˆ¶**
- **é•¿åº¦è‡ªé€‚åº”**ï¼šæ ¹æ®è¾“å‡ºtokenæ•°é‡åŠ¨æ€è°ƒæ•´å¥–åŠ±
- **æµ‹è¯•å¥–åŠ±é€’å¢**ï¼šé€šè¿‡æµ‹è¯•è¶Šå¤šï¼Œå¥–åŠ±é€’å¢ç³»æ•°è¶Šå¤§
- **è®­ç»ƒæ­¥æ•°ç¼©æ”¾**ï¼šé€šè¿‡`get_scaled_reward`æ ¹æ®è®­ç»ƒè¿›åº¦è°ƒæ•´

è¿™ç§è®¾è®¡å¼•å¯¼æ¨¡å‹ä»**åŸºç¡€çš„ä»£ç ç”Ÿæˆ**é€æ­¥æå‡åˆ°**åŠŸèƒ½å®Œæ•´ã€é«˜è´¨é‡çš„Verilogå®ç°**ï¼Œæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±è®¾è®¡ï¼