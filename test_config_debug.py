#!/usr/bin/env python3
"""é…ç½®å‚æ•°ä¼ é€’æµ‹è¯•"""

import sys
from transformers import HfArgumentParser
from grpo_project.configs import EnvConfig, ScriptConfig, EnhancedRewardConfig
from trl import GRPOConfig

def test_config_parsing():
    print("ğŸ”§ æµ‹è¯•é…ç½®å‚æ•°ä¼ é€’...")
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = HfArgumentParser((EnvConfig, ScriptConfig, EnhancedRewardConfig, GRPOConfig))
    env_cfg, script_cfg, reward_cfg, grpo_cfg = parser.parse_args_into_dataclasses()
    
    print("\nğŸ“ æ•°æ®é›†é…ç½®æ£€æŸ¥:")
    print(f"  - script_cfg.dataset_path: {getattr(script_cfg, 'dataset_path', 'MISSING')}")
    print(f"  - env_cfg.dataset_base_path: {getattr(env_cfg, 'dataset_base_path', 'MISSING')}")
    
    if hasattr(script_cfg, 'dataset_path') and script_cfg.dataset_path:
        import os
        inferred_base = os.path.dirname(script_cfg.dataset_path)
        print(f"  - ä»dataset_pathæ¨å¯¼çš„åŸºç¡€è·¯å¾„: {inferred_base}")
        
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if os.path.exists(script_cfg.dataset_path):
            print(f"  âœ… æ•°æ®é›†æ–‡ä»¶å­˜åœ¨: {script_cfg.dataset_path}")
        else:
            print(f"  âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {script_cfg.dataset_path}")
            
        if os.path.exists(inferred_base):
            print(f"  âœ… æ¨å¯¼çš„åŸºç¡€è·¯å¾„å­˜åœ¨: {inferred_base}")
        else:
            print(f"  âŒ æ¨å¯¼çš„åŸºç¡€è·¯å¾„ä¸å­˜åœ¨: {inferred_base}")
    
    base_path = env_cfg.dataset_base_path
    if base_path:
        import os
        if os.path.exists(base_path):
            print(f"  âœ… æ˜ç¡®æŒ‡å®šçš„åŸºç¡€è·¯å¾„å­˜åœ¨: {base_path}")
        else:
            print(f"  âŒ æ˜ç¡®æŒ‡å®šçš„åŸºç¡€è·¯å¾„ä¸å­˜åœ¨: {base_path}")
    else:
        print("  âš ï¸ æ²¡æœ‰æ˜ç¡®æŒ‡å®šåŸºç¡€è·¯å¾„")
    
    print("\nğŸ¯ å…³é”®é…ç½®:")
    print(f"  - model_name_or_path: {getattr(script_cfg, 'model_name_or_path', 'MISSING')}")
    print(f"  - output_dir_base: {getattr(env_cfg, 'output_dir_base', 'MISSING')}")
    print(f"  - learning_rate: {getattr(grpo_cfg, 'learning_rate', 'MISSING')}")
    
    # ğŸ”§ æ–°å¢ï¼šå¤šGPUé…ç½®è°ƒè¯•
    print("\nğŸš€ å¤šGPUé…ç½®æ£€æŸ¥:")
    print(f"  - use_model_parallel: {getattr(script_cfg, 'use_model_parallel', 'MISSING')}")
    print(f"  - ç±»å‹: {type(getattr(script_cfg, 'use_model_parallel', None))}")
    
    # æ£€æŸ¥GPUç¯å¢ƒ
    import torch
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        print(f"  - å¯ç”¨GPUæ•°é‡: {gpu_count}")
        for i in range(gpu_count):
            props = torch.cuda.get_device_properties(i)
            memory_gb = props.total_memory / 1024**3
            print(f"    GPU {i}: {props.name}, {memory_gb:.1f}GB")
    else:
        print("  - CUDAä¸å¯ç”¨")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    print("\nğŸ” ç¯å¢ƒå˜é‡æ£€æŸ¥:")
    import os
    dist_vars = ['RANK', 'LOCAL_RANK', 'WORLD_SIZE', 'MASTER_ADDR', 'MASTER_PORT']
    for var in dist_vars:
        value = os.environ.get(var, 'NOT_SET')
        print(f"  - {var}: {value}")
    
    print(f"  - CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'NOT_SET')}")
    
    return env_cfg, script_cfg

if __name__ == "__main__":
    try:
        env_cfg, script_cfg = test_config_parsing()
        print("\nâœ… é…ç½®è§£ææˆåŠŸ")
    except Exception as e:
        print(f"\nâŒ é…ç½®è§£æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1) 