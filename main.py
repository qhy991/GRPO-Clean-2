# main.py - å®Œæ•´ä¿®å¤ç‰ˆæœ¬
import os
import logging
import numpy as np
import torch
import gc
import sys
from dataclasses import asdict
import json
from typing import Dict, Any, Optional, List
from pathlib import Path

# --- BEGIN: PyTorch Safe Unpickling Configuration ---
logger_temp = logging.getLogger(__name__ + "_startup")
try:
    from numpy.core.multiarray import _reconstruct
    from numpy import dtype as numpy_dtype
    from numpy.dtypes import UInt32DType

    safe_globals_list = []
    if callable(_reconstruct): safe_globals_list.append(_reconstruct)
    if isinstance(numpy_dtype, type): safe_globals_list.append(numpy_dtype)
    if isinstance(np.ndarray, type): safe_globals_list.append(np.ndarray)
    if isinstance(UInt32DType, type): safe_globals_list.append(UInt32DType)

    # Add numpy scalar types
    numpy_scalar_types_to_add = [
        np.bool_, np.byte, np.ubyte, np.short, np.ushort, np.intc, np.uintc,
        np.int_, np.uint, np.longlong, np.ulonglong,
        np.half, np.float16, np.single, np.double, np.longdouble,
        np.csingle, np.cdouble, np.clongdouble,
        np.int8, np.int16, np.int32, np.int64,
        np.uint8, np.uint16, np.uint32, np.uint64,
        np.float32, np.float64
    ]
    
    for nt_class in numpy_scalar_types_to_add:
        if isinstance(nt_class, type):
            safe_globals_list.append(nt_class)

    if safe_globals_list:
        torch.serialization.add_safe_globals(safe_globals_list)
        logger_temp.info(f"Successfully updated torch safe global variables list with {len(safe_globals_list)} items.")

except Exception as e_globals:
    logger_temp.error(f"Error setting up torch safe globals: {e_globals}", exc_info=True)
# --- END: PyTorch Safe Unpickling Configuration ---

from transformers import HfArgumentParser
from trl import GRPOConfig

# Configuration imports
from grpo_project.configs import EnvConfig, ScriptConfig, EnhancedRewardConfig
from grpo_project.utils.logging import setup_global_logging
from grpo_project.utils.reporting_utils import PeriodicStatusReporter
from grpo_project.core.models import ModelManager
from grpo_project.data.dataset import load_and_prepare_dataset
from grpo_project.rewards.calculator import RewardCalculator
from grpo_project.curriculum.manager import setup_fixed_curriculum_manager
from grpo_project.utils import ExperienceBuffer

# Callbacks
from grpo_project.callbacks.monitoring import StepLoggingCallback, DetailedRewardCallback, RewardStabilityMonitor
from grpo_project.callbacks.persistence import CustomStatePersistenceCallback
from grpo_project.callbacks.inference import DetailedInferenceCallback
from grpo_project.callbacks.wandb import DetailedWandbCallback as TrainDetailedWandbCallback
from grpo_project.curriculum.callbacks import CurriculumProgressCallback, EnhancedCurriculumDebugCallback, OptimizedCurriculumCallback

logger = logging.getLogger(__name__)

class GRPOTrainingPipeline:
    def __init__(self):
        # 1. Load Configurations
        parser = HfArgumentParser((EnvConfig, ScriptConfig, EnhancedRewardConfig, GRPOConfig))
        self.env_cfg, self.script_cfg, self.reward_cfg, self.grpo_cfg = parser.parse_args_into_dataclasses()

        # Setup logging first
        self._setup_logging()
        logger.info("GRPOTrainingPipeline initialized.")
        self._log_configs()

        # Initialize core components
        self.model_manager = ModelManager(
            script_cfg=self.script_cfg,
            grpo_cfg=self.grpo_cfg,
            model_name_or_path=self.script_cfg.model_name_or_path,
            cache_dir=self.env_cfg.cache_dir
        )

        self.reward_calculator = RewardCalculator(
            reward_config=self.reward_cfg,
            simulator=None
        )

        self.curriculum_manager = None
        self.experience_buffer = None
        self.callbacks = []
        self.status_reporter = PeriodicStatusReporter(self.grpo_cfg.output_dir, report_interval=50)

        self.model = None
        self.tokenizer = None
        self.trainer = None

    def _setup_logging(self):
        """Setup logging with improved error handling"""
        import re
        from datetime import datetime

        run_specific_name_from_env = os.getenv("WANDB_RUN_NAME")
        if not run_specific_name_from_env:
            run_specific_name_from_env = f"{self.env_cfg.wandb_run_name_prefix}_{datetime.now().strftime('%Y%m%d-%H%M%S')}" \
                if self.env_cfg.wandb_run_name_prefix else f"run_{datetime.now().strftime('%Y%m%d-%H%M%S')}"

        sanitized_run_name = re.sub(r'[^\w\-.]', '_', run_specific_name_from_env)

        # Determine if resuming
        is_resuming = (
            self.grpo_cfg.resume_from_checkpoint and
            isinstance(self.grpo_cfg.resume_from_checkpoint, str) and
            os.path.isdir(self.grpo_cfg.resume_from_checkpoint)
        )

        if is_resuming:
            actual_output_dir = os.path.dirname(self.grpo_cfg.resume_from_checkpoint)
        else:
            actual_output_dir = os.path.join(self.env_cfg.output_dir_base, sanitized_run_name)

        if self.grpo_cfg.local_rank <= 0:
            os.makedirs(actual_output_dir, exist_ok=True)

        # Update config objects
        self.grpo_cfg.output_dir = actual_output_dir
        self.script_cfg.output_dir = actual_output_dir

        log_file_path = os.path.join(self.grpo_cfg.output_dir, "grpo_pipeline_log.txt")
        setup_global_logging(
            log_level=self.grpo_cfg.get_process_log_level(),
            log_file_path=log_file_path,
            local_rank=self.grpo_cfg.local_rank
        )
        logger.info(f"Global logging set up. Output directory: {self.grpo_cfg.output_dir}")

    def _log_configs(self):
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f"EnvConfig: \n{json.dumps(asdict(self.env_cfg), indent=2)}")
            logger.debug(f"ScriptConfig: \n{json.dumps(asdict(self.script_cfg), indent=2)}")
            logger.debug(f"EnhancedRewardConfig: \n{json.dumps(asdict(self.reward_cfg), indent=2)}")

    def _setup_model_and_tokenizer(self):
        """ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®è®¾ç½®æ¨¡å‹å’Œtokenizer"""
        try:
            logger.info("Setting up model and tokenizer...")
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šè°ƒç”¨ModelManagerçš„æ–¹æ³•
            self.model, self.tokenizer = self.model_manager.setup_model_and_tokenizer()
            
            # åº”ç”¨PEFTé€‚é…å™¨
            is_resuming = (
                self.grpo_cfg.resume_from_checkpoint and 
                isinstance(self.grpo_cfg.resume_from_checkpoint, str) and 
                os.path.isdir(self.grpo_cfg.resume_from_checkpoint)
            )
            
            self.model = self.model_manager.apply_peft_adapter(
                model=self.model,
                is_resuming=is_resuming,
                resume_checkpoint_path=str(self.grpo_cfg.resume_from_checkpoint) if is_resuming else None
            )
            
            logger.info("âœ… Model and tokenizer setup completed successfully.")
            return self.model, self.tokenizer
            
        except Exception as e:
            logger.error(f"âŒ Failed to setup model and tokenizer: {e}", exc_info=True)
            raise
    def _initialize_trainer(self):
        logger.info("Initializing GRPOTrainer...")

        reward_function = self._get_reward_function_with_context()

        self.trainer = GRPOTrainer(
            model=self.model,
            args=self.grpo_cfg,
            train_dataset=self.dataset_for_trainer,
            reward_funcs=[reward_function],
            tokenizer=self.tokenizer,
            callbacks=self.callbacks,
        )

        # ğŸ”§ é‡è¦ï¼šè®¾ç½® trainer_ref ä¸ºæ‰€æœ‰éœ€è¦çš„å›è°ƒ
        for cb in self.callbacks:
            if hasattr(cb, 'trainer_ref') and cb.trainer_ref is None:
                cb.trainer_ref = self.trainer
                logger.info(f"âœ… Set trainer_ref for {type(cb).__name__}")

        logger.info("GRPOTrainer initialized successfully.")

        # ğŸ”§ é¢å¤–çš„è¯¾ç¨‹å­¦ä¹ çŠ¶æ€éªŒè¯
        if self.curriculum_manager:
            logger.info("ğŸ“š æœ€ç»ˆè¯¾ç¨‹å­¦ä¹ çŠ¶æ€éªŒè¯:")
            logger.info(f"  - è¯¾ç¨‹ç®¡ç†å™¨ç±»å‹: {type(self.curriculum_manager).__name__}")
            
            # éªŒè¯è¯¾ç¨‹ç®¡ç†å™¨æ˜¯å¦æœ‰è°ƒè¯•æ—¥å¿—
            if hasattr(self.curriculum_manager, 'debug_log'):
                logger.info(f"  - è°ƒè¯•æ—¥å¿—æ¡ç›®: {len(self.curriculum_manager.debug_log)}")
                if self.curriculum_manager.debug_log:
                    logger.info(f"  - æœ€æ–°æ—¥å¿—: {self.curriculum_manager.debug_log[-1]}")
            
            # éªŒè¯å½“å‰é˜¶æ®µæ•°æ®é›†
            try:
                current_dataset = self.curriculum_manager.get_current_stage_dataset()
                logger.info(f"  - å½“å‰æ•°æ®é›†éªŒè¯: {len(current_dataset)} samples")
            except Exception as e:
                logger.error(f"  - æ•°æ®é›†éªŒè¯å¤±è´¥: {e}")

            # éªŒè¯è¯¾ç¨‹é˜¶æ®µé…ç½®
            for i, stage in enumerate(self.curriculum_manager.curriculum_stages):
                status = "ğŸ”„ å½“å‰" if i == self.curriculum_manager.current_stage else "â³ å¾…è¿›å…¥"
                logger.info(f"  - é˜¶æ®µ{i}: {stage.name} ({status})")

    def _initialize_components(self, dataset_processed):
        """Initialize components that depend on the processed dataset"""
        try:
            logger.info("Initializing training components...")
            
            # Experience buffer setup
            if self.script_cfg.enable_experience_replay:
                self.experience_buffer = ExperienceBuffer(max_size=self.script_cfg.experience_buffer_size)
                logger.info(f"Experience buffer initialized (size: {self.script_cfg.experience_buffer_size}).")
                
                # Restore experience buffer state if resuming
                if self.grpo_cfg.resume_from_checkpoint and os.path.isdir(self.grpo_cfg.resume_from_checkpoint):
                    buffer_state_path = os.path.join(self.grpo_cfg.resume_from_checkpoint, "enhanced_experience_buffer_state.json")
                    if os.path.exists(buffer_state_path):
                        try:
                            logger.info(f"Loading experience buffer state from: {buffer_state_path}")
                            with open(buffer_state_path, "r", encoding="utf-8") as f:
                                state_data = json.load(f)
                            self.experience_buffer.load_buffer_state(state_data)
                            logger.info("âœ… Experience buffer state loaded successfully.")
                        except Exception as e:
                            logger.warning(f"âš ï¸ Failed to load experience buffer state: {e}")

            # Curriculum learning setup
            if self.script_cfg.enable_curriculum:
                self.curriculum_manager = setup_fixed_curriculum_manager(self.script_cfg, dataset_processed)
                if self.curriculum_manager:
                    logger.info(f"Curriculum learning enabled: {type(self.curriculum_manager).__name__}.")
                    
                    # Restore curriculum state if resuming
                    if self.grpo_cfg.resume_from_checkpoint and os.path.isdir(self.grpo_cfg.resume_from_checkpoint):
                        curriculum_state_path = os.path.join(self.grpo_cfg.resume_from_checkpoint, "enhanced_curriculum_state.json")
                        if os.path.exists(curriculum_state_path):
                            try:
                                logger.info(f"Loading curriculum state from: {curriculum_state_path}")
                                with open(curriculum_state_path, "r", encoding="utf-8") as f:
                                    state_data = json.load(f)
                                self.curriculum_manager.load_curriculum_state(state_data)
                                logger.info("âœ… Curriculum state loaded successfully.")
                            except Exception as e:
                                logger.warning(f"âš ï¸ Failed to load curriculum state: {e}")

            # Setup callbacks
            self._setup_callbacks(dataset_processed)
            logger.info("âœ… All components initialized successfully.")
            
        except Exception as e:
            logger.error(f"âŒ Error during component initialization: {e}", exc_info=True)
            raise

    def _setup_callbacks(self, dataset_processed):
        """Setup all callbacks with enhanced curriculum debugging"""
        try:
            self.callbacks = []
            
            # Basic monitoring callbacks
            self.callbacks.append(StepLoggingCallback())
            self.callbacks.append(DetailedRewardCallback(self.script_cfg.output_dir))
            self.callbacks.append(RewardStabilityMonitor(self.script_cfg.output_dir))

            # ğŸ”§ ä¿®å¤ï¼šå¢å¼ºçš„è¯¾ç¨‹å­¦ä¹ å›è°ƒè®¾ç½®
            if self.curriculum_manager:
                # 1. æ·»åŠ åŸºç¡€çš„è¯¾ç¨‹è¿›åº¦å›è°ƒ
                curriculum_progress_cb = CurriculumProgressCallback(
                    curriculum_manager=self.curriculum_manager, 
                    trainer_ref=None,  # ç¨åè®¾ç½®
                    output_dir=self.script_cfg.output_dir
                )
                self.callbacks.append(curriculum_progress_cb)
                logger.info("âœ… æ·»åŠ  CurriculumProgressCallback")

                # 2. æ·»åŠ å¢å¼ºçš„è¯¾ç¨‹è°ƒè¯•å›è°ƒ
                enhanced_curriculum_cb = EnhancedCurriculumDebugCallback(
                    curriculum_manager=self.curriculum_manager, 
                    trainer_ref=None,  # ç¨åè®¾ç½®
                    output_dir=self.script_cfg.output_dir
                )
                self.callbacks.append(enhanced_curriculum_cb)
                logger.info("âœ… æ·»åŠ  EnhancedCurriculumDebugCallback")

                # 3. æ·»åŠ ä¼˜åŒ–çš„è¯¾ç¨‹å›è°ƒï¼ˆå¦‚æœéœ€è¦ï¼‰
                optimized_curriculum_cb = OptimizedCurriculumCallback(
                    curriculum_manager=self.curriculum_manager,
                    trainer_ref=None,  # ç¨åè®¾ç½®
                    output_dir=self.script_cfg.output_dir
                )
                self.callbacks.append(optimized_curriculum_cb)
                logger.info("âœ… æ·»åŠ  OptimizedCurriculumCallback")

            # State persistence callback
            self.callbacks.append(CustomStatePersistenceCallback(
                curriculum_manager=self.curriculum_manager, 
                experience_buffer=self.experience_buffer, 
                script_cfg=self.script_cfg
            ))

            # Inference callback (éœ€è¦ tokenizer)
            if self.tokenizer and dataset_processed and len(dataset_processed) > 0:
                sample_dataset_for_inf_cb = dataset_processed.select(
                    range(min(len(dataset_processed), self.script_cfg.callback_num_samples * 5))
                )

                if len(sample_dataset_for_inf_cb) > 0:
                    self.callbacks.append(DetailedInferenceCallback(
                        tokenizer=self.tokenizer, 
                        eval_dataset=sample_dataset_for_inf_cb,
                        num_samples=self.script_cfg.callback_num_samples,
                        eval_every_n_steps=self.script_cfg.callback_eval_every_n_steps,
                        max_new_tokens=self.grpo_cfg.max_completion_length,
                        max_seq_length=self.script_cfg.max_seq_length,
                        experience_buffer=self.experience_buffer,
                        output_dir=self.script_cfg.output_dir
                    ))
                    logger.info(f"DetailedInferenceCallback added with {len(sample_dataset_for_inf_cb)} samples.")
                else:
                    logger.warning("âš ï¸ No samples available for DetailedInferenceCallback.")

            # W&B callback
            if self.grpo_cfg.local_rank <= 0 and "wandb" in self.grpo_cfg.report_to:
                wandb_cb = TrainDetailedWandbCallback(
                    self.env_cfg, self.script_cfg, self.reward_cfg, self.experience_buffer
                )
                self.callbacks.append(wandb_cb)
                # å­˜å‚¨ wandb_callback å¼•ç”¨ä¾› reward function ä½¿ç”¨
                self.wandb_callback = wandb_cb
                logger.info("âœ… TrainDetailedWandbCallback added.")

            logger.info(f"Total callbacks prepared: {len(self.callbacks)}")
            
            # ğŸ”§ é‡è¦ï¼šç¡®ä¿è¯¾ç¨‹å­¦ä¹ å›è°ƒæœ‰è¯¦ç»†çš„åˆå§‹åŒ–æ—¥å¿—
            if self.curriculum_manager:
                logger.info("ğŸ“š è¯¾ç¨‹å­¦ä¹ å›è°ƒè¯¦ç»†ä¿¡æ¯:")
                logger.info(f"  - å½“å‰é˜¶æ®µ: {self.curriculum_manager.current_stage}")
                logger.info(f"  - æ€»é˜¶æ®µæ•°: {len(self.curriculum_manager.curriculum_stages)}")
                
                # è®°å½•å½“å‰é˜¶æ®µè¯¦æƒ…
                if self.curriculum_manager.current_stage < len(self.curriculum_manager.curriculum_stages):
                    current_stage = self.curriculum_manager.curriculum_stages[self.curriculum_manager.current_stage]
                    logger.info(f"  - å½“å‰é˜¶æ®µåç§°: {current_stage.name}")
                    logger.info(f"  - æ€§èƒ½é˜ˆå€¼: {current_stage.performance_threshold}")
                    logger.info(f"  - æœ€å°è¯„ä¼°æ¬¡æ•°: {current_stage.min_evaluations}")
                    
                    # è®°å½•æ•°æ®é›†å¤§å°
                    current_dataset = self.curriculum_manager.get_current_stage_dataset()
                    logger.info(f"  - å½“å‰é˜¶æ®µæ•°æ®é›†å¤§å°: {len(current_dataset)}")

        except Exception as e:
            logger.error(f"âŒ Error setting up callbacks: {e}", exc_info=True)
            # è‡³å°‘ä¿è¯åŸºæœ¬çš„å›è°ƒ
            self.callbacks = [StepLoggingCallback()]
            logger.warning("âš ï¸ Using minimal callback setup due to errors.")
    def get_reward_function(self):
        """Create reward function closure with enhanced debugging"""
        def reward_fn_closure(prompts: List[str], completions: List[str], **kwargs_from_trainer_dataset) -> List[float]:
            try:
                current_training_step = self.trainer.state.global_step if self.trainer and self.trainer.state else 0

                # ğŸ”§ æ·»åŠ è¯¦ç»†çš„å¥–åŠ±è®¡ç®—æ—¥å¿—
                if current_training_step % 10 == 0:  # æ¯10æ­¥è®°å½•ä¸€æ¬¡
                    logger.info(f"ğŸ¯ æ­¥æ•° {current_training_step}: å¼€å§‹å¥–åŠ±è®¡ç®—")
                    logger.info(f"  - æ‰¹æ¬¡å¤§å°: {len(prompts)}")
                    logger.info(f"  - å®Œæˆé•¿åº¦: {[len(c) for c in completions[:3]]}{'...' if len(completions) > 3 else ''}")

                batch_rewards_args = {
                    "prompts": prompts,
                    "completions": completions,
                    "testbench_paths": kwargs_from_trainer_dataset.get('testbench_path', []),
                    "expected_total_tests_list": kwargs_from_trainer_dataset.get('expected_total_tests', []),
                    "reference_verilog_paths": kwargs_from_trainer_dataset.get('reference_verilog_path', []),
                    "original_enhanced_prompts": kwargs_from_trainer_dataset.get('original_enhanced_prompt'),
                    "training_step": current_training_step,
                    "output_dir_for_debug": self.script_cfg.output_dir,
                    # ğŸ”§ ç¡®ä¿ä¼ é€’æ­£ç¡®çš„ wandb_callback å¼•ç”¨
                    "wandb_callback_obj": getattr(self, 'wandb_callback', None),
                    "experience_buffer_obj": self.experience_buffer,
                    "script_config_obj": self.script_cfg
                }
                
                rewards_list, aggregated_metrics = self.reward_calculator.calculate_batch_rewards(**batch_rewards_args)
                
                # ğŸ”§ æ·»åŠ å¥–åŠ±ç»Ÿè®¡æ—¥å¿—
                if current_training_step % 10 == 0:
                    reward_stats = {
                        'mean': np.mean(rewards_list) if rewards_list else 0,
                        'std': np.std(rewards_list) if rewards_list else 0,
                        'min': np.min(rewards_list) if rewards_list else 0,
                        'max': np.max(rewards_list) if rewards_list else 0
                    }
                    logger.info(f"  - å¥–åŠ±ç»Ÿè®¡: mean={reward_stats['mean']:.4f}, std={reward_stats['std']:.4f}")
                    logger.info(f"  - å¥–åŠ±èŒƒå›´: [{reward_stats['min']:.4f}, {reward_stats['max']:.4f}]")
                
                return rewards_list
                    
            except Exception as e:
                logger.error(f"âŒ Error in reward function at step {current_training_step}: {e}", exc_info=True)
                # è¿”å›é»˜è®¤å¥–åŠ±é¿å…è®­ç»ƒä¸­æ–­
                return [0.0] * len(prompts)
        
        return reward_fn_closure

    def train(self):
        """Main training function with comprehensive error handling"""
        try:
            logger.info("ğŸš€ Starting GRPO training process...")

            # 1. Setup model and tokenizer - ğŸ”§ ä¿®å¤è°ƒç”¨
            logger.info("ğŸ“ Step 1: Setting up model and tokenizer...")
            self._setup_model_and_tokenizer()  # ğŸ”§ è¿™é‡Œä¸å†è§£åŒ…ï¼Œå› ä¸ºå·²ç»åœ¨æ–¹æ³•å†…éƒ¨è®¾ç½®äº†self.modelå’Œself.tokenizer

            # 2. Load and preprocess data
            logger.info("ğŸ“ Step 2: Loading and preparing dataset...")
            dataset_processed = load_and_prepare_dataset(
                script_cfg=self.script_cfg,
                env_cfg=self.env_cfg,
                tokenizer=self.tokenizer
            )

            if not dataset_processed or len(dataset_processed) == 0:
                raise ValueError("âŒ Dataset is empty after processing!")

            # 3. Initialize components
            logger.info("ğŸ“ Step 3: Initializing training components...")
            self._initialize_components(dataset_processed)

            # 4. Determine training dataset
            dataset_for_trainer = dataset_processed
            if self.curriculum_manager:
                dataset_for_trainer = self.curriculum_manager.get_current_stage_dataset()
                current_stage_name = self.curriculum_manager.get_current_stage_info()['stage_name']
                logger.info(f"ğŸ“š Using curriculum dataset: {len(dataset_for_trainer)} samples from stage '{current_stage_name}'.")
            else:
                logger.info(f"ğŸ“š Using full processed dataset: {len(dataset_for_trainer)} samples.")

            if not dataset_for_trainer or len(dataset_for_trainer) == 0:
                raise ValueError("âŒ Training dataset is empty!")

            # 5. Create trainer
            logger.info("ğŸ“ Step 4: Creating GRPOTrainer...")
            from trl import GRPOTrainer
            
            self.trainer = GRPOTrainer(
                model=self.model,
                args=self.grpo_cfg,
                train_dataset=dataset_for_trainer,
                reward_funcs=[self.get_reward_function()],
                callbacks=self.callbacks
            )

            # Set trainer references for callbacks
            for cb in self.callbacks:
                if hasattr(cb, 'trainer_ref') and cb.trainer_ref is None:
                    cb.trainer_ref = self.trainer
                    logger.debug(f"Set trainer_ref for {type(cb).__name__}")

            # 6. Start training
            logger.info("ğŸ“ Step 5: Starting training...")
            logger.info(f"ğŸ¯ Training with {len(dataset_for_trainer)} examples.")
            
            train_result = self.trainer.train(resume_from_checkpoint=self.grpo_cfg.resume_from_checkpoint)
            logger.info("âœ… Training completed successfully!")

            # 7. Save artifacts
            if self.grpo_cfg.local_rank <= 0:
                self._save_training_artifacts(train_result)

        except Exception as e:
            logger.error(f"âŒ Training failed: {e}", exc_info=True)
            raise

    def _save_training_artifacts(self, train_result):
        """Save training artifacts"""
        try:
            logger.info("ğŸ’¾ Saving training artifacts...")
            
            # Save final model
            final_model_dir = os.path.join(self.grpo_cfg.output_dir, "final_model_adapter")
            self.trainer.save_model(final_model_dir)
            logger.info(f"âœ… Final model saved to: {final_model_dir}")

            # Save metrics and state
            metrics = train_result.metrics if hasattr(train_result, 'metrics') else {}
            
            if hasattr(self.trainer, 'log_metrics'):
                self.trainer.log_metrics("train_summary", metrics)
            
            if hasattr(self.trainer, 'save_metrics'):
                metrics_file = os.path.join(self.grpo_cfg.output_dir, "final_train_metrics.json")
                self.trainer.save_metrics("train_summary", metrics_file)
                logger.info(f"âœ… Metrics saved to: {metrics_file}")
            
            if hasattr(self.trainer, 'save_state'):
                self.trainer.save_state()
                logger.info("âœ… Trainer state saved.")

            logger.info(f"ğŸ‰ All training artifacts saved to: {self.grpo_cfg.output_dir}")
            
        except Exception as e:
            logger.error(f"âŒ Error saving training artifacts: {e}", exc_info=True)

    def cleanup(self):
        """Cleanup resources"""
        try:
            logger.info("ğŸ§¹ Cleaning up resources...")
            
            # Clear GPU memory
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                logger.info("âœ… GPU cache cleared.")
            
            # Force garbage collection
            gc.collect()
            logger.info("âœ… Garbage collection completed.")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Error during cleanup: {e}")


def main():
    """Main entry point with comprehensive error handling"""
    pipeline = None
    try:
        logger_temp.info("ğŸš€ Initializing GRPO Training Pipeline...")
        pipeline = GRPOTrainingPipeline()
        
        logger_temp.info("ğŸ¯ Starting training...")
        pipeline.train()
        
        logger_temp.info("âœ… Training completed successfully!")
        
    except KeyboardInterrupt:
        logger_temp.warning("âš ï¸ Training interrupted by user (Ctrl+C)")
        return 1
        
    except Exception as e:
        logger_temp.error(f"ğŸ’¥ Fatal error in training pipeline: {e}", exc_info=True)
        return 1
        
    finally:
        if pipeline:
            try:
                pipeline.cleanup()
            except Exception as cleanup_error:
                logger_temp.warning(f"âš ï¸ Error during cleanup: {cleanup_error}")
        
        logger_temp.info("ğŸ GRPO Training Pipeline execution finished.")
        return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)