# main.py - ä¿®å¤DDP/æ¨¡å‹å¹¶è¡Œå†²çªç‰ˆæœ¬
import os
import logging
import numpy as np
import torch
import gc
import sys
from dataclasses import asdict
import json
from typing import Dict, Any, Optional, List
from pathlib import Path
import re
import time

# --- BEGIN: PyTorch Safe Unpickling Configuration ---
logger_temp = logging.getLogger(__name__ + "_startup")
try:
    from numpy.core.multiarray import _reconstruct
    from numpy import dtype as numpy_dtype
    from numpy.dtypes import UInt32DType

    safe_globals_list = []
    if callable(_reconstruct): safe_globals_list.append(_reconstruct)
    if isinstance(numpy_dtype, type): safe_globals_list.append(numpy_dtype)
    if isinstance(np.ndarray, type): safe_globals_list.append(np.ndarray)
    if isinstance(UInt32DType, type): safe_globals_list.append(UInt32DType)

    # Add numpy scalar types
    numpy_scalar_types_to_add = [
        np.bool_, np.byte, np.ubyte, np.short, np.ushort, np.intc, np.uintc,
        np.int_, np.uint, np.longlong, np.ulonglong,
        np.half, np.float16, np.single, np.double, np.longdouble,
        np.csingle, np.cdouble, np.clongdouble,
        np.int8, np.int16, np.int32, np.int64,
        np.uint8, np.uint16, np.uint32, np.uint64,
        np.float32, np.float64
    ]
    
    for nt_class in numpy_scalar_types_to_add:
        if isinstance(nt_class, type):
            safe_globals_list.append(nt_class)

    if safe_globals_list:
        torch.serialization.add_safe_globals(safe_globals_list)
        logger_temp.info(f"Successfully updated torch safe global variables list with {len(safe_globals_list)} items.")

except Exception as e_globals:
    logger_temp.error(f"Error setting up torch safe globals: {e_globals}", exc_info=True)
# --- END: PyTorch Safe Unpickling Configuration ---

from transformers import HfArgumentParser
from trl import GRPOConfig

# Configuration imports
from grpo_project.configs import EnvConfig, ScriptConfig, EnhancedRewardConfig
from grpo_project.utils.logging import setup_global_logging
from grpo_project.utils.reporting_utils import PeriodicStatusReporter
from grpo_project.core.models import ModelManager
from grpo_project.data.dataset import load_and_prepare_dataset
from grpo_project.rewards.calculator import RewardCalculator
from grpo_project.curriculum.manager import setup_fixed_curriculum_manager
from grpo_project.utils import ExperienceBuffer

# Callbacks
from grpo_project.callbacks.monitoring import StepLoggingCallback, DetailedRewardCallback, RewardStabilityMonitor
from grpo_project.callbacks.persistence import CustomStatePersistenceCallback
from grpo_project.callbacks.inference import DetailedInferenceCallback
from grpo_project.callbacks.wandb import DetailedWandbCallback as TrainDetailedWandbCallback
from grpo_project.curriculum.callbacks import CurriculumProgressCallback, EnhancedCurriculumDebugCallback, OptimizedCurriculumCallback
from grpo_project.core.wandb_sync_manager import initialize_wandb_sync_manager, get_wandb_sync_manager

logger = logging.getLogger(__name__)

class GRPOTrainingPipeline:
    def __init__(self):
        # 1. Load Configurations
        parser = HfArgumentParser((EnvConfig, ScriptConfig, EnhancedRewardConfig, GRPOConfig))
        self.env_cfg, self.script_cfg, self.reward_cfg, self.grpo_cfg = parser.parse_args_into_dataclasses()
        
        # ğŸ”§ æ–°å¢ï¼šå¤šGPUç¯å¢ƒåˆå§‹æ£€æµ‹
        self._detect_multi_gpu_environment()
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šé…ç½®è®­ç»ƒç­–ç•¥ï¼ˆåœ¨GPUæ£€æµ‹ä¹‹åï¼‰
        self._configure_training_strategy()
        
        # ğŸ”§ åŒæ­¥é•¿åº¦é…ç½®ä»ScriptConfigåˆ°GRPOConfig
        self._sync_length_configs()
        
        # ğŸ”§ è°ƒè¯•ï¼šæ‰“å°æœ€ç»ˆé…ç½®çŠ¶æ€
        logger_temp.info(f"ğŸ¯ æœ€ç»ˆé…ç½®çŠ¶æ€:")
        logger_temp.info(f"  - multi_gpu_info: {getattr(self, 'multi_gpu_info', {})}")
        logger_temp.info(f"  - training_strategy: {getattr(self, 'training_strategy', 'unknown')}")

        # ğŸ”§ æ–°å¢ï¼šè‡ªåŠ¨é…ç½®WandBæ¢å¤
        self._configure_wandb_resume()

        # Setup logging first
        self._setup_logging()
        logger.info("GRPOTrainingPipeline initialized.")
        self._log_configs()

        # ğŸ”§ æ–°å¢ï¼šåˆå§‹åŒ–WandBåŒæ­¥ç®¡ç†å™¨
        self._setup_wandb_sync_manager()

        # ğŸ”§ æ–°å¢ï¼šå¤šGPUä¼˜åŒ–çš„ModelManageråˆå§‹åŒ–
        self.model_manager = ModelManager(
            script_cfg=self.script_cfg,
            grpo_cfg=self.grpo_cfg,
            model_name_or_path=self.script_cfg.model_name_or_path,
            cache_dir=getattr(self.env_cfg, 'cache_dir', None)
        )

        self.reward_calculator = RewardCalculator(
            reward_config=self.reward_cfg,
            simulator=None
        )

        self.curriculum_manager = None
        self.experience_buffer = None
        self.callbacks = []
        self.status_reporter = PeriodicStatusReporter(self.grpo_cfg.output_dir, report_interval=50)

        self.model = None
        self.tokenizer = None
        self.trainer = None
        
        # ğŸ”§ å¤šGPUçŠ¶æ€è·Ÿè¸ª - æ³¨æ„ï¼šè¿™äº›å°†åœ¨ä¸Šé¢çš„æ–¹æ³•ä¸­è®¾ç½®ï¼Œä¸è¦åœ¨è¿™é‡Œé‡å¤åˆå§‹åŒ–

    def _detect_multi_gpu_environment(self):
        """ğŸ”§ æ£€æµ‹å¹¶é…ç½®å¤šGPUç¯å¢ƒ"""
        try:
            logger_temp.info("ğŸ” æ£€æµ‹å¤šGPUç¯å¢ƒ...")
            
            # æ£€æŸ¥CUDAå¯ç”¨æ€§
            if not torch.cuda.is_available():
                logger_temp.warning("âš ï¸ CUDAä¸å¯ç”¨ï¼Œç¦ç”¨å¤šGPUæ¨¡å¼")
                self.multi_gpu_info = {
                    'gpu_count': 0,
                    'total_memory_gb': 0,
                    'average_memory_gb': 0,
                    'use_model_parallel': False
                }
                return
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥ç”¨æˆ·æ˜ç¡®æŒ‡å®šçš„æ¨¡å‹å¹¶è¡Œè®¾ç½®
            user_specified_model_parallel = getattr(self.script_cfg, 'use_model_parallel', None)
            logger_temp.info(f"ğŸ¯ é…ç½®è§£æè°ƒè¯•:")
            logger_temp.info(f"  - script_cfg.use_model_parallel: {user_specified_model_parallel}")
            logger_temp.info(f"  - ç±»å‹: {type(user_specified_model_parallel)}")
            
            # æ£€æŸ¥GPUæ•°é‡
            gpu_count = torch.cuda.device_count()
            logger_temp.info(f"ğŸ“Š æ£€æµ‹åˆ° {gpu_count} å¼ GPU")
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šä¼˜åŒ–åˆ¤æ–­é€»è¾‘
            if user_specified_model_parallel is False:
                logger_temp.info("ğŸ¯ ç”¨æˆ·æ˜ç¡®ç¦ç”¨æ¨¡å‹å¹¶è¡Œï¼Œå¼ºåˆ¶ä½¿ç”¨å•GPUæ¨¡å¼")
                # ğŸ”§ é™åˆ¶CUDAè®¾å¤‡å¯è§æ€§åˆ°å•GPU
                os.environ['CUDA_VISIBLE_DEVICES'] = '0'
                self.multi_gpu_info = {
                    'gpu_count': 1,  # å¼ºåˆ¶è®¾ä¸º1
                    'total_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1024**3 if gpu_count > 0 else 0,
                    'average_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1024**3 if gpu_count > 0 else 0,
                    'use_model_parallel': False
                }
                return
            
            # ğŸ”§ ä¿®å¤ï¼šå¦‚æœGPUæ•°é‡ä¸è¶³ä½†ç”¨æˆ·æ˜ç¡®è¦æ±‚æ¨¡å‹å¹¶è¡Œï¼Œç»™å‡ºè­¦å‘Šä½†ä¸å¼ºåˆ¶ç¦ç”¨
            if gpu_count < 2:
                if user_specified_model_parallel is True:
                    logger_temp.error(f"âŒ ç”¨æˆ·è¦æ±‚æ¨¡å‹å¹¶è¡Œä½†GPUæ•°é‡({gpu_count})ä¸è¶³ï¼")
                    logger_temp.error("ğŸ’¡ è¯·ç¡®ä¿æœ‰è‡³å°‘2å¼ GPUå¯ç”¨ï¼Œæˆ–è®¾ç½® --use_model_parallel false")
                    raise ValueError(f"GPUæ•°é‡ä¸è¶³ä»¥æ”¯æŒæ¨¡å‹å¹¶è¡Œï¼šéœ€è¦>=2å¼ GPUï¼Œå®é™…{gpu_count}å¼ ")
                else:
                    logger_temp.warning(f"âš ï¸ GPUæ•°é‡({gpu_count})å°‘äº2å¼ ï¼Œå°†ä½¿ç”¨å•GPUæ¨¡å¼")
                    self.multi_gpu_info = {
                        'gpu_count': gpu_count,
                        'total_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1024**3 if gpu_count > 0 else 0,
                        'average_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1024**3 if gpu_count > 0 else 0,
                        'use_model_parallel': False
                    }
                    return
            
            # æ£€æŸ¥GPUå±æ€§
            logger_temp.info("ğŸ”§ GPUè¯¦ç»†ä¿¡æ¯:")
            total_memory = 0
            for i in range(gpu_count):
                props = torch.cuda.get_device_properties(i)
                memory_gb = props.total_memory / 1024**3
                total_memory += memory_gb
                logger_temp.info(f"  GPU {i}: {props.name}, {memory_gb:.1f}GB")
                
                if memory_gb < 40:  # è‡³å°‘40GBæ‰æ¨èç”¨äºå¤§æ¨¡å‹
                    logger_temp.warning(f"âš ï¸ GPU {i} å†…å­˜({memory_gb:.1f}GB)å¯èƒ½ä¸è¶³ä»¥æ”¯æŒå¤§æ¨¡å‹è®­ç»ƒ")
            
            # ğŸ”§ ä¿®å¤ï¼šæ›´æ¸…æ™°çš„åˆ¤æ–­é€»è¾‘
            if user_specified_model_parallel is True:
                # ç”¨æˆ·æ˜ç¡®è¦æ±‚æ¨¡å‹å¹¶è¡Œ
                use_model_parallel = True
                logger_temp.info("âœ… ç”¨æˆ·æ˜ç¡®å¯ç”¨æ¨¡å‹å¹¶è¡Œæ¨¡å¼")
            elif user_specified_model_parallel is None and gpu_count >= 2:
                # ç”¨æˆ·æœªè®¾ç½®ï¼Œä¸”GPUæ•°é‡è¶³å¤Ÿï¼Œé»˜è®¤å¯ç”¨
                use_model_parallel = True
                logger_temp.info("âœ… è‡ªåŠ¨å¯ç”¨æ¨¡å‹å¹¶è¡Œæ¨¡å¼ï¼ˆGPUæ•°é‡>=2ï¼‰")
            else:
                # å…¶ä»–æƒ…å†µä½¿ç”¨å•GPU
                use_model_parallel = False
                logger_temp.info("ğŸ“± å°†ä½¿ç”¨å•GPUæ¨¡å¼")
            
            self.multi_gpu_info = {
                'gpu_count': gpu_count,
                'total_memory_gb': total_memory,
                'average_memory_gb': total_memory / gpu_count,
                'use_model_parallel': use_model_parallel
            }
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿åœ¨æ£€æµ‹é˜¶æ®µå°±è®¾ç½®æ­£ç¡®çš„å¤šGPUä¿¡æ¯
            logger_temp.info(f"ğŸ”§ å¤šGPUä¿¡æ¯è®¾ç½®å®Œæˆ:")
            logger_temp.info(f"  - gpu_count: {self.multi_gpu_info['gpu_count']}")
            logger_temp.info(f"  - use_model_parallel: {self.multi_gpu_info['use_model_parallel']}")
            logger_temp.info(f"  - total_memory_gb: {self.multi_gpu_info['total_memory_gb']:.1f}GB")
            
            if self.multi_gpu_info['use_model_parallel']:
                logger_temp.info("âœ… å¤šGPUæ¨¡å‹å¹¶è¡Œæ¨¡å¼å·²å¯ç”¨")
                logger_temp.info(f"  - æ€»GPUå†…å­˜: {total_memory:.1f}GB")
                logger_temp.info(f"  - å¹³å‡æ¯GPU: {total_memory/gpu_count:.1f}GB")
            
        except Exception as e:
            logger_temp.error(f"âŒ å¤šGPUç¯å¢ƒæ£€æµ‹å¤±è´¥: {e}")
            # è®¾ç½®å®‰å…¨çš„é»˜è®¤å€¼
            self.multi_gpu_info = {
                'gpu_count': 1,
                'total_memory_gb': 0,
                'average_memory_gb': 0,
                'use_model_parallel': False
            }
            raise

    def _configure_training_strategy(self):
        """ğŸ”§ å…³é”®ä¿®å¤ï¼šé…ç½®è®­ç»ƒç­–ç•¥ä»¥é¿å…DDP/æ¨¡å‹å¹¶è¡Œå†²çªï¼ˆå®Œå…¨ä¿®å¤ç‰ˆï¼‰"""
        try:
            logger_temp.info("ğŸ”§ é…ç½®è®­ç»ƒç­–ç•¥...")
            
            # ä»multi_gpu_infoè·å–æ­£ç¡®çš„æ¨¡å‹å¹¶è¡Œè®¾ç½®
            use_model_parallel = self.multi_gpu_info.get('use_model_parallel', False)
            gpu_count = self.multi_gpu_info.get('gpu_count', 1)

            # æ£€æµ‹åˆ†å¸ƒå¼ç¯å¢ƒï¼ˆé€šå¸¸é€šè¿‡torchrunæˆ–deepspeedå¯åŠ¨ï¼‰
            is_launched_as_distributed = 'RANK' in os.environ and 'WORLD_SIZE' in os.environ

            logger_temp.info(f"ğŸ”§ è®­ç»ƒç­–ç•¥é…ç½®è°ƒè¯•:")
            logger_temp.info(f"  - multi_gpu_info.use_model_parallel: {use_model_parallel}")
            logger_temp.info(f"  - multi_gpu_info.gpu_count: {gpu_count}")
            logger_temp.info(f"  - script_cfg.use_model_parallel: {getattr(self.script_cfg, 'use_model_parallel', 'MISSING')}")
            logger_temp.info(f"  - æ£€æµ‹åˆ°åˆ†å¸ƒå¼å¯åŠ¨: {is_launched_as_distributed}")

                    # ğŸ”§ æ£€æŸ¥ç”¨æˆ·æ˜ç¡®çš„æ¨¡å‹å¹¶è¡Œåå¥½ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
            user_specified_model_parallel_explicit = getattr(self.script_cfg, 'use_model_parallel', None)
            logger_temp.info(f"  - ç”¨æˆ·æ˜ç¡®çš„æ¨¡å‹å¹¶è¡Œè®¾ç½®: {user_specified_model_parallel_explicit}")
            
            # å¦‚æœç”¨æˆ·æ˜ç¡®è¦æ±‚æ¨¡å‹å¹¶è¡Œï¼Œå¼ºåˆ¶ä½¿ç”¨æ¨¡å‹å¹¶è¡Œ
            if user_specified_model_parallel_explicit is True:
                logger_temp.info("ğŸ”§ ç”¨æˆ·æ˜ç¡®è¦æ±‚æ¨¡å‹å¹¶è¡Œï¼Œå¼ºåˆ¶è®¾ç½®æ¨¡å‹å¹¶è¡Œç­–ç•¥")
                use_model_parallel = True
                self.training_strategy = "model_parallel_single_process"
                logger_temp.info("âœ… å·²å¼ºåˆ¶è®¾ç½®ä¸ºæ¨¡å‹å¹¶è¡Œæ¨¡å¼")
                return

            # ğŸ”§ æ£€æµ‹ç”¨æˆ·æ˜¯å¦å¯ç”¨FSDPï¼ˆåªæœ‰åœ¨æ²¡æœ‰æ˜ç¡®è¦æ±‚æ¨¡å‹å¹¶è¡Œæ—¶æ‰æ£€æŸ¥ï¼‰
            user_specified_fsdp_flag = getattr(self.script_cfg, 'use_fsdp', False)
            # transformers.TrainingArguments ä¼šæŠŠ --fsdp è§£æåˆ° grpo_cfg.fsdp (List[str] æˆ– str)
            fsdp_arg_from_grpo = getattr(self.grpo_cfg, 'fsdp', None)
            user_specified_fsdp = user_specified_fsdp_flag or (fsdp_arg_from_grpo is not None and fsdp_arg_from_grpo != "")
            logger_temp.info(f"  - script_cfg.use_fsdp: {user_specified_fsdp}")

            # å¦‚æœç”¨æˆ·è¦æ±‚FSDPï¼Œä¼˜å…ˆçº§æ¬¡é«˜
            if user_specified_fsdp:
                    if gpu_count < 1:
                        raise ValueError("FSDP éœ€è¦è‡³å°‘1å¼ å¯ç”¨GPUï¼")

                    # å¦‚æœå·²ç»ç”± torchrun ç­‰æ–¹å¼å¯åŠ¨å¤šè¿›ç¨‹ï¼Œåˆ™è®¤ä¸ºæ˜¯åˆ†å¸ƒå¼ FSDP
                    if is_launched_as_distributed:
                        self.training_strategy = "fsdp_distributed"
                    else:
                        self.training_strategy = "fsdp_single_process"

                    # ä½¿ç”¨FSDPæ—¶ç¦ç”¨æ¨¡å‹å¹¶è¡Œæ ‡è®°
                    self.multi_gpu_info['use_model_parallel'] = False
                    logger_temp.info(f"âœ… å·²è®¾ç½®è®­ç»ƒç­–ç•¥ä¸º {self.training_strategy}")
                    return  # æå‰ç»“æŸï¼Œé¿å…åç»­é€»è¾‘è¦†ç›–

            # å…³é”®å†³ç­–é€»è¾‘
            if use_model_parallel and gpu_count >= 2:
                if is_launched_as_distributed:
                    # å¦‚æœç”¨æˆ·åŒæ—¶å°è¯•DDPå’Œæ¨¡å‹å¹¶è¡Œï¼Œè¿™æ˜¯ä¸å…¼å®¹çš„ã€‚
                    # æˆ‘ä»¬ä¼˜å…ˆæ¨¡å‹å¹¶è¡Œï¼Œå¹¶ç¦ç”¨DDPã€‚
                    logger_temp.warning("âš ï¸ æ£€æµ‹åˆ°åˆ†å¸ƒå¼ç¯å¢ƒä¸æ¨¡å‹å¹¶è¡Œè¯·æ±‚å†²çªã€‚")
                    logger_temp.info("ğŸ”§ è§£å†³ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨æ¨¡å‹å¹¶è¡Œï¼Œå°†ç¦ç”¨DDPã€‚")
                    
                    # æ¸…é™¤åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡ä»¥é¿å…TRL/HFåº“çš„è‡ªåŠ¨DDPåˆå§‹åŒ–
                    dist_env_vars = ['RANK', 'LOCAL_RANK', 'WORLD_SIZE', 'MASTER_ADDR', 'MASTER_PORT']
                    for var in dist_env_vars:
                        if var in os.environ:
                            logger_temp.info(f"ğŸ§¹ æ¸…é™¤å†²çªçš„åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡: {var}={os.environ[var]}")
                            del os.environ[var]
                    
                    self.training_strategy = "model_parallel_single_process"
                    logger_temp.info("âœ… å·²é…ç½®ä¸ºå•è¿›ç¨‹æ¨¡å‹å¹¶è¡Œæ¨¡å¼ï¼ˆDDPå·²ç¦ç”¨ï¼‰ã€‚")
                else:
                    # è¿™æ˜¯é¢„æœŸçš„å•è¿›ç¨‹æ¨¡å‹å¹¶è¡Œåœºæ™¯ã€‚
                    self.training_strategy = "model_parallel_single_process"
                    logger_temp.info("âœ… é…ç½®ä¸ºå•è¿›ç¨‹æ¨¡å‹å¹¶è¡Œæ¨¡å¼ã€‚")
            
            elif is_launched_as_distributed:
                # ç­–ç•¥2ï¼šåˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ (DDP)
                self.training_strategy = "distributed_data_parallel"
                logger_temp.info("âœ… é…ç½®ä¸ºåˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼ˆDDPï¼‰æ¨¡å¼ã€‚")
                self.multi_gpu_info['use_model_parallel'] = False # ç¡®ä¿ä¸€è‡´æ€§
            
            else:
                # ç­–ç•¥3ï¼šå•GPUè®­ç»ƒ
                self.training_strategy = "single_gpu"
                logger_temp.info("âœ… é…ç½®ä¸ºå•GPUè®­ç»ƒæ¨¡å¼ã€‚")
                self.multi_gpu_info['use_model_parallel'] = False # ç¡®ä¿ä¸€è‡´æ€§
                if gpu_count > 1:
                    logger_temp.info("ğŸ”§ æ£€æµ‹åˆ°å¤šå¼ GPUä½†æœªä½¿ç”¨å¹¶è¡Œç­–ç•¥ï¼Œå°†é™åˆ¶ä½¿ç”¨GPU 0ã€‚")
                    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
                    self.multi_gpu_info['gpu_count'] = 1

            logger_temp.info(f"ğŸ¯ æœ€ç»ˆç¡®å®šçš„è®­ç»ƒç­–ç•¥: {self.training_strategy}")

        except Exception as e:
            logger_temp.error(f"âŒ è®­ç»ƒç­–ç•¥é…ç½®å¤±è´¥: {e}", exc_info=True)
            # å®‰å…¨å›é€€åˆ°å•GPUæ¨¡å¼
            self.training_strategy = "single_gpu"
            self.multi_gpu_info = { 'gpu_count': 1, 'use_model_parallel': False }
            logger_temp.info("ğŸ”„ å·²å›é€€åˆ°å®‰å…¨çš„å•GPUè®­ç»ƒæ¨¡å¼ã€‚")

    def _prepare_grpo_config_for_strategy(self):
        """ğŸ”§ æ ¹æ®è®­ç»ƒç­–ç•¥å‡†å¤‡GRPOé…ç½®ï¼ˆä¿®å¤åªè¯»å±æ€§é—®é¢˜ï¼‰"""
        import copy
        
        logger.info(f"ğŸ”§ ä¸º {self.training_strategy} å‡†å¤‡GRPOé…ç½®...")
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šåªä¿®æ”¹éåªè¯»å±æ€§
        try:
            grpo_cfg_copy = copy.deepcopy(self.grpo_cfg)
        except Exception as e:
            logger.warning(f"âš ï¸ æ·±æ‹·è´å¤±è´¥ï¼Œä½¿ç”¨åŸé…ç½®: {e}")
            grpo_cfg_copy = self.grpo_cfg
        
        # ğŸ”§ åªä¿®æ”¹å¯ä»¥ä¿®æ”¹çš„å±æ€§ï¼Œé¿å…åªè¯»å±æ€§
        try:
            # å…³é”®ï¼šç¦ç”¨å†…ç½®WandBä»¥é¿å…å†²çª
            if hasattr(grpo_cfg_copy, 'report_to') and "wandb" in grpo_cfg_copy.report_to:
                try:
                    grpo_cfg_copy.report_to = [r for r in grpo_cfg_copy.report_to if r != "wandb"]
                    logger.info("ğŸ”§ ç¦ç”¨GRPOTrainerå†…ç½®WandBæŠ¥å‘Š")
                except Exception as report_e:
                    logger.warning(f"âš ï¸ æ— æ³•ä¿®æ”¹report_toå±æ€§: {report_e}")
            
            # æ ¹æ®è®­ç»ƒç­–ç•¥è¿›è¡Œç‰¹å®šä¼˜åŒ–
            if self.training_strategy in ["model_parallel_only", "model_parallel_single_process"]:
                logger.info("ğŸš€ åº”ç”¨æ¨¡å‹å¹¶è¡Œä¼˜åŒ–...")
                
                # ä¼˜åŒ–æ•°æ®åŠ è½½è®¾ç½®
                if hasattr(grpo_cfg_copy, 'dataloader_num_workers'):
                    try:
                        original_workers = grpo_cfg_copy.dataloader_num_workers
                        grpo_cfg_copy.dataloader_num_workers = min(original_workers, 2)
                        if original_workers != grpo_cfg_copy.dataloader_num_workers:
                            logger.info(f"ğŸ”§ è°ƒæ•´dataloader_num_workers: {original_workers} -> {grpo_cfg_copy.dataloader_num_workers}")
                    except Exception as worker_e:
                        logger.warning(f"âš ï¸ æ— æ³•è°ƒæ•´dataloader_num_workers: {worker_e}")
                
                if hasattr(grpo_cfg_copy, 'dataloader_pin_memory'):
                    try:
                        grpo_cfg_copy.dataloader_pin_memory = False
                        logger.info("ğŸ”§ ç¦ç”¨dataloader_pin_memoryä»¥é¿å…å¤šGPUå†…å­˜é—®é¢˜")
                    except Exception as pin_e:
                        logger.warning(f"âš ï¸ æ— æ³•ç¦ç”¨dataloader_pin_memory: {pin_e}")
                
                # ç¦ç”¨DDPç›¸å…³è®¾ç½®
                if hasattr(grpo_cfg_copy, 'ddp_find_unused_parameters'):
                    try:
                        grpo_cfg_copy.ddp_find_unused_parameters = False
                        logger.info("ğŸ”§ ç¦ç”¨ddp_find_unused_parameters")
                    except Exception as ddp_e:
                        logger.warning(f"âš ï¸ æ— æ³•ç¦ç”¨ddp_find_unused_parameters: {ddp_e}")
                        
            elif self.training_strategy.startswith("fsdp"):
                logger.info("ğŸš€ åº”ç”¨ FSDP ç›¸å…³ä¼˜åŒ–â€¦")

                # ğŸ”§ å…³é”®ä¿®å¤ï¼šä¼˜å…ˆæ¸…é™¤å¯èƒ½å†²çªçš„å‚æ•°
                if hasattr(grpo_cfg_copy, 'fsdp_min_num_params'):
                    try:
                        # å®Œå…¨åˆ é™¤è¿™ä¸ªå±æ€§ï¼Œè€Œä¸æ˜¯è®¾ä¸º0
                        delattr(grpo_cfg_copy, 'fsdp_min_num_params')
                        logger.info("ğŸ”§ åˆ é™¤ fsdp_min_num_params å±æ€§ä»¥é¿å…å†²çª")
                    except Exception as min_params_e:
                        logger.warning(f"âš ï¸ æ— æ³•åˆ é™¤ fsdp_min_num_params: {min_params_e}")

                # å¯ç”¨ FSDP
                if hasattr(grpo_cfg_copy, 'fsdp'):
                    try:
                        if not grpo_cfg_copy.fsdp or grpo_cfg_copy.fsdp == "":
                            grpo_cfg_copy.fsdp = "full_shard"
                        logger.info(f"ğŸ”§ FSDP æ¨¡å¼: {grpo_cfg_copy.fsdp}")
                    except Exception as fsdp_e:
                        logger.warning(f"âš ï¸ æ— æ³•è®¾ç½® fsdp: {fsdp_e}")

                # æŒ‡å®š transformer layer ç±»åï¼ˆä»…åœ¨åˆ é™¤min_num_paramsåè®¾ç½®ï¼‰
                if hasattr(grpo_cfg_copy, 'fsdp_transformer_layer_cls_to_wrap'):
                    try:
                        grpo_cfg_copy.fsdp_transformer_layer_cls_to_wrap = "QWenBlock"
                        logger.info(f"ğŸ”§ è®¾ç½® fsdp_transformer_layer_cls_to_wrap: {grpo_cfg_copy.fsdp_transformer_layer_cls_to_wrap}")
                    except Exception as fsdp_layer_e:
                        logger.warning(f"âš ï¸ æ— æ³•è®¾ç½® fsdp_transformer_layer_cls_to_wrap: {fsdp_layer_e}")

                # è®¾ç½® fsdp_config å­—å…¸
                try:
                    fsdp_conf_dict = {
                        "transformer_layer_cls_to_wrap": "QWenBlock",
                        "backward_prefetch": "backward_pre", 
                        "forward_prefetch": False,
                        "use_orig_params": True,
                        "sync_module_states": True,
                        "limit_all_gathers": True,
                        "cpu_offload": False,  # ç¦ç”¨CPU offloadä»¥è·å¾—æ›´å¥½æ€§èƒ½
                        "xla": False  # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ·»åŠ å¿…éœ€çš„xlaé”®
                    }
                    grpo_cfg_copy.fsdp_config = fsdp_conf_dict
                    logger.info(f"ğŸ”§ æ³¨å…¥ fsdp_config: {fsdp_conf_dict}")
                except Exception as fsdp_conf_e:
                    logger.warning(f"âš ï¸ æ— æ³•æ³¨å…¥ fsdp_config: {fsdp_conf_e}")

            elif self.training_strategy == "distributed_data_parallel":
                logger.info("ğŸ”§ ä¿æŒåˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œè®¾ç½®...")
                # ä¿æŒåŸæœ‰è®¾ç½®ï¼Œåªç¦ç”¨WandB
                
            else:  # single_gpu
                logger.info("ğŸ”§ åº”ç”¨å•GPUä¼˜åŒ–...")
                # å•GPUæ—¶çš„ä¼˜åŒ–è®¾ç½®
            
            # ğŸ”§ é‡è¦ï¼šåªè¯»å–é…ç½®ä¿¡æ¯ï¼Œä¸ä¿®æ”¹åªè¯»å±æ€§
            if hasattr(grpo_cfg_copy, 'world_size'):
                logger.debug(f"å½“å‰world_size: {grpo_cfg_copy.world_size}")
            if hasattr(grpo_cfg_copy, 'local_rank'):
                logger.debug(f"å½“å‰local_rank: {grpo_cfg_copy.local_rank}")
            
            # ğŸ”§ æ–°å¢ï¼šç¡®ä¿åˆ†å¸ƒå¼ç›¸å…³ç¯å¢ƒå˜é‡æ¸…ç†ç”Ÿæ•ˆ
            if self.training_strategy in ["model_parallel_only", "model_parallel_single_process", "single_gpu"]:
                # æ¸…é™¤å¯èƒ½å¯¼è‡´åˆ†å¸ƒå¼åˆå§‹åŒ–çš„ç¯å¢ƒå˜é‡
                dist_env_vars = ['RANK', 'LOCAL_RANK', 'WORLD_SIZE', 'MASTER_ADDR', 'MASTER_PORT']
                for var in dist_env_vars:
                    if var in os.environ:
                        logger.info(f"ğŸ§¹ ç¡®ä¿æ¸…é™¤åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡: {var}")
                        del os.environ[var]
            
            logger.info(f"âœ… GRPOé…ç½®å·²é’ˆå¯¹{self.training_strategy}ä¼˜åŒ–")
            return grpo_cfg_copy
            
        except Exception as config_e:
            logger.error(f"âŒ é…ç½®ä¿®æ”¹å¤±è´¥: {config_e}")
            logger.info("ğŸ”„ å›é€€åˆ°æœ€å°ä¿®æ”¹ç­–ç•¥")
            
            # ğŸ”§ æœ€åçš„å›é€€æ–¹æ¡ˆï¼šåˆ›å»ºæœ€å°ä¿®æ”¹ç‰ˆæœ¬
            try:
                # ä»…å°è¯•ç¦ç”¨WandBï¼Œå…¶ä»–ä¿æŒä¸å˜
                if hasattr(self.grpo_cfg, 'report_to') and "wandb" in self.grpo_cfg.report_to:
                    # å¦‚æœreport_toæ˜¯å¯ä¿®æ”¹çš„ï¼Œåˆ›å»ºä¿®æ”¹ç‰ˆæœ¬
                    minimal_copy = copy.copy(self.grpo_cfg)
                    try:
                        minimal_copy.report_to = [r for r in minimal_copy.report_to if r != "wandb"]
                        logger.info("ğŸ”§ å›é€€æ–¹æ¡ˆï¼šä»…ç¦ç”¨WandBæŠ¥å‘Š")
                        return minimal_copy
                    except:
                        pass
                
                # ç»ˆæå›é€€ï¼šä½¿ç”¨åŸå§‹é…ç½®
                logger.warning("âš ï¸ ä½¿ç”¨åŸå§‹GRPOé…ç½®ï¼Œå¯èƒ½åŒ…å«WandBå†²çª")
                return self.grpo_cfg
                
            except Exception as fallback_e:
                logger.error(f"âŒ å›é€€æ–¹æ¡ˆä¹Ÿå¤±è´¥: {fallback_e}")
                # æœ€ç»ˆå›é€€ï¼šç›´æ¥è¿”å›åŸå§‹é…ç½®
                return self.grpo_cfg
        
    def _sync_length_configs(self):
        """ğŸ”§ åŒæ­¥é•¿åº¦é…ç½®ä»ScriptConfigåˆ°GRPOConfigï¼Œé¿å…å‚æ•°å†²çª"""
        logger.info("ğŸ”§ åŒæ­¥é•¿åº¦é…ç½®åˆ°GRPOConfig...")
        
        # å°†æˆ‘ä»¬çš„è„šæœ¬é…ç½®åŒæ­¥åˆ°GRPOé…ç½®
        self.grpo_cfg.max_prompt_length = self.script_cfg.script_max_prompt_length
        self.grpo_cfg.max_completion_length = self.script_cfg.script_max_completion_length
        
        logger.info(f"  âœ… GRPO max_prompt_length: {self.grpo_cfg.max_prompt_length}")
        logger.info(f"  âœ… GRPO max_completion_length: {self.grpo_cfg.max_completion_length}")

    def _setup_stable_environment(self):
        """ğŸ”§ è®¾ç½®ç¨³å®šçš„è®­ç»ƒç¯å¢ƒï¼Œæ”¯æŒå¤šGPUä¼˜åŒ–"""
        try:
            logger.info("ğŸ”§ è®¾ç½®ç¨³å®šè®­ç»ƒç¯å¢ƒ...")
            
            # ğŸ”§ æ ¹æ®è®­ç»ƒç­–ç•¥è®¾ç½®ç¯å¢ƒå˜é‡
            if self.training_strategy == "model_parallel_only":
                logger.info("ğŸš€ é…ç½®çº¯æ¨¡å‹å¹¶è¡Œç¯å¢ƒ...")
                
                stable_envs = {
                    "CUDA_LAUNCH_BLOCKING": "0",  # å¼‚æ­¥æ‰§è¡Œä»¥æé«˜æ€§èƒ½
                    "PYTORCH_CUDA_ALLOC_CONF": "expandable_segments:True,max_split_size_mb:256",
                    "NCCL_BLOCKING_WAIT": "1",
                    "NCCL_P2P_DISABLE": "0",  # å¯ç”¨P2Pé€šä¿¡
                    "NCCL_TIMEOUT": "7200",
                }
                
                # ç¡®ä¿æ²¡æœ‰åˆ†å¸ƒå¼ç›¸å…³çš„ç¯å¢ƒå˜é‡
                for dist_var in ['RANK', 'LOCAL_RANK', 'WORLD_SIZE', 'MASTER_ADDR', 'MASTER_PORT']:
                    if dist_var in os.environ:
                        del os.environ[dist_var]
                        logger.info(f"ğŸ§¹ æ¸…é™¤åˆ†å¸ƒå¼å˜é‡: {dist_var}")
                
            elif self.training_strategy == "distributed_data_parallel":
                logger.info("ğŸ”§ é…ç½®åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œç¯å¢ƒ...")
                
                stable_envs = {
                    "CUDA_LAUNCH_BLOCKING": "1",
                    "PYTORCH_CUDA_ALLOC_CONF": "max_split_size_mb:512",
                    "NCCL_BLOCKING_WAIT": "1",
                }
                
            else:
                # å•GPUæˆ–å•è¿›ç¨‹æ¨¡å‹å¹¶è¡Œ
                stable_envs = {
                    "CUDA_LAUNCH_BLOCKING": "0" if self.training_strategy == "model_parallel_single_process" else "1",
                    "PYTORCH_CUDA_ALLOC_CONF": "expandable_segments:True,max_split_size_mb:256",
                }
            
            # Flash Attentionä¼˜åŒ–ï¼ˆå¦‚æœæ”¯æŒï¼‰
            if getattr(self.script_cfg, 'enable_flash_attention', True):
                stable_envs["FLASH_ATTENTION_V2"] = "1"
                logger.info("âš¡ å¯ç”¨Flash Attention 2ä¼˜åŒ–")
            
            for key, value in stable_envs.items():
                if key not in os.environ:
                    os.environ[key] = value
                    logger.info(f"  è®¾ç½®ç¯å¢ƒå˜é‡: {key}={value}")
            
            # ğŸ”§ æ¢¯åº¦æ£€æŸ¥ç‚¹å¤„ç†
            if hasattr(self.grpo_cfg, 'gradient_checkpointing') and self.grpo_cfg.gradient_checkpointing:
                if self.training_strategy in ["model_parallel_only", "model_parallel_single_process"]:
                    if not getattr(self.script_cfg, 'gradient_checkpointing_compatible', False):
                        logger.warning("âš ï¸ æ¨¡å‹å¹¶è¡Œæ¨¡å¼ä¸‹è‡ªåŠ¨ç¦ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥é¿å…åŒæ­¥é—®é¢˜")
                        self.grpo_cfg.gradient_checkpointing = False
            
            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                for i in range(torch.cuda.device_count()):
                    torch.cuda.set_device(i)
                    torch.cuda.empty_cache()
                logger.info(f"âœ… å·²æ¸…ç† {torch.cuda.device_count()} å¼ GPUçš„å†…å­˜")
            
            logger.info("âœ… ç¨³å®šè®­ç»ƒç¯å¢ƒè®¾ç½®å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ç¨³å®šç¯å¢ƒè®¾ç½®å¤±è´¥: {e}")

    def _configure_wandb_resume(self):
        """ğŸ”§ è‡ªåŠ¨é…ç½®WandBæ¢å¤ï¼Œæ— éœ€å¤–éƒ¨è„šæœ¬"""
        try:
            # æ£€æŸ¥æ˜¯å¦ä»checkpointæ¢å¤
            is_resuming = (
                self.grpo_cfg.resume_from_checkpoint and
                isinstance(self.grpo_cfg.resume_from_checkpoint, str) and
                os.path.isdir(self.grpo_cfg.resume_from_checkpoint)
            )
            
            if not is_resuming:
                logger.info("ğŸš€ å¼€å§‹æ–°çš„è®­ç»ƒï¼Œæ— éœ€WandBæ¢å¤é…ç½®")
                # ç¡®ä¿æ¸…é™¤å¯èƒ½å­˜åœ¨çš„æ¢å¤ç›¸å…³ç¯å¢ƒå˜é‡
                for env_var in ["WANDB_RUN_ID", "WANDB_RESUME"]:
                    if env_var in os.environ:
                        del os.environ[env_var]
                        logger.info(f"ğŸ§¹ æ¸…é™¤ç¯å¢ƒå˜é‡: {env_var}")
                return
            
            checkpoint_path = Path(self.grpo_cfg.resume_from_checkpoint)
            logger.info(f"ğŸ”„ æ£€æµ‹åˆ°checkpointæ¢å¤: {checkpoint_path}")
            
            # å°è¯•ä»checkpointç›®å½•æå–WandB run ID
            run_id, run_url = self._extract_wandb_run_id(checkpoint_path)
            
            if run_id:
                # è®¾ç½®ç²¾ç¡®æ¢å¤
                os.environ["WANDB_RUN_ID"] = run_id
                os.environ["WANDB_RESUME"] = "must"
                logger.info(f"âœ… WandBç²¾ç¡®æ¢å¤é…ç½®:")
                logger.info(f"  - Run ID: {run_id}")
                logger.info(f"  - Resume Mode: must")
                if run_url:
                    logger.info(f"  - Run URL: {run_url}")
            else:
                # ä½¿ç”¨è‡ªåŠ¨æ¢å¤æ¨¡å¼
                os.environ["WANDB_RESUME"] = "allow"
                logger.info("âš ï¸ æœªæ‰¾åˆ°å…·ä½“çš„Run IDï¼Œä½¿ç”¨è‡ªåŠ¨æ¢å¤æ¨¡å¼")
                logger.info("  - Resume Mode: allow")
            
            logger.info("âœ… WandBæ¢å¤é…ç½®å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ WandBæ¢å¤é…ç½®å¤±è´¥: {e}")
            logger.info("ğŸ”„ å°†ä½¿ç”¨é»˜è®¤çš„WandBé…ç½®")

    def _setup_wandb_sync_manager(self):
        """ğŸ”§ è®¾ç½®WandBåŒæ­¥ç®¡ç†å™¨"""
        try:
            # ä»é…ç½®ä¸­è·å–é¡¹ç›®ä¿¡æ¯
            project_name = getattr(self.env_cfg, 'wandb_project', 'VerilogGRPO_Enhanced_MultiGPU')
            run_name = f"grpo_run_{os.path.basename(self.grpo_cfg.output_dir)}"
            
            # ğŸ”§ è®­ç»ƒç­–ç•¥æ ‡è¯†
            run_name = f"{self.training_strategy}_{run_name}"
            
            # åˆå§‹åŒ–åŒæ­¥ç®¡ç†å™¨
            sync_manager = initialize_wandb_sync_manager(
                output_dir=self.grpo_cfg.output_dir,
                project_name=project_name,
                run_name=run_name
            )
            
            logger.info("âœ… WandBåŒæ­¥ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            logger.info(f"  - é¡¹ç›®: {project_name}")
            logger.info(f"  - è¿è¡Œåç§°: {run_name}")
            logger.info(f"  - è¾“å‡ºç›®å½•: {self.grpo_cfg.output_dir}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ WandBåŒæ­¥ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.info("ğŸ”„ å°†ä½¿ç”¨åŸç”ŸWandBåŠŸèƒ½")

    def _setup_wandb_run(self):
        """ğŸ”§ è®¾ç½®WandBè¿è¡Œï¼Œå¤„ç†æ–­ç»­è®­ç»ƒ"""
        try:
            sync_manager = get_wandb_sync_manager()
            if not sync_manager:
                logger.warning("âš ï¸ WandBåŒæ­¥ç®¡ç†å™¨æœªæ‰¾åˆ°ï¼Œè·³è¿‡WandBè¿è¡Œè®¾ç½®")
                return
                
            # æ£€æŸ¥æ˜¯å¦ä»checkpointæ¢å¤
            resume_from_checkpoint = None
            if (self.grpo_cfg.resume_from_checkpoint and 
                isinstance(self.grpo_cfg.resume_from_checkpoint, str) and
                os.path.isdir(self.grpo_cfg.resume_from_checkpoint)):
                resume_from_checkpoint = self.grpo_cfg.resume_from_checkpoint
                
            # ğŸ”§ å‡†å¤‡é…ç½®ï¼ŒåŒ…å«è®­ç»ƒç­–ç•¥ä¿¡æ¯
            config = {
                "model_name_or_path": self.script_cfg.model_name_or_path,
                "learning_rate": self.grpo_cfg.learning_rate,
                "per_device_train_batch_size": self.grpo_cfg.per_device_train_batch_size,
                "max_seq_length": self.script_cfg.max_seq_length,
                "callback_eval_every_n_steps": self.script_cfg.callback_eval_every_n_steps,
                "lora_rank": getattr(self.script_cfg, 'lora_rank', None),
                "curriculum_enabled": self.curriculum_manager is not None,
                "resume_from_checkpoint": resume_from_checkpoint,
                # ğŸ”§ è®­ç»ƒç­–ç•¥ç›¸å…³é…ç½®
                "training_strategy": self.training_strategy,
                "use_model_parallel": self.multi_gpu_info.get('use_model_parallel', False),
                "gpu_count": self.multi_gpu_info.get('gpu_count', 1),
                "total_gpu_memory_gb": self.multi_gpu_info.get('total_memory_gb', 0),
                "max_memory_per_gpu": getattr(self.script_cfg, 'max_memory_per_gpu', 'auto'),
            }
            
            # è®¾ç½®WandBè¿è¡Œ
            success = sync_manager.setup_wandb_run(
                resume_from_checkpoint=resume_from_checkpoint,
                config=config
            )
            
            if success:
                logger.info("âœ… WandBè¿è¡Œè®¾ç½®æˆåŠŸ")
            else:
                logger.warning("âš ï¸ WandBè¿è¡Œè®¾ç½®å¤±è´¥ï¼Œå°†ä½¿ç”¨æœ¬åœ°æ—¥å¿—")
                
        except Exception as e:
            logger.warning(f"âš ï¸ WandBè¿è¡Œè®¾ç½®å¼‚å¸¸: {e}")

    def _extract_wandb_run_id(self, checkpoint_path: Path) -> tuple[Optional[str], Optional[str]]:
        """ä»checkpointç›®å½•ä¸­æå–WandB run ID"""
        try:
            # æ–¹æ³•1: ä»wandbç›®å½•ä¸­æŸ¥æ‰¾
            parent_dir = checkpoint_path.parent
            wandb_dir = parent_dir / "wandb"
            
            if wandb_dir.exists():
                logger.info(f"ğŸ” åœ¨ {wandb_dir} ä¸­æŸ¥æ‰¾WandB runä¿¡æ¯...")
                
                # æŸ¥æ‰¾runç›®å½•
                run_dirs = list(wandb_dir.glob("run-*"))
                if run_dirs:
                    latest_run_dir = sorted(run_dirs)[-1]
                    logger.info(f"ğŸ“ æ‰¾åˆ°runç›®å½•: {latest_run_dir.name}")
                    
                    # æå–run ID (æ ¼å¼: run-20231201_123456-abcd1234)
                    run_name = latest_run_dir.name
                    if "-" in run_name:
                        parts = run_name.split("-")
                        if len(parts) >= 3:
                            run_id = parts[-1]  # æœ€åä¸€éƒ¨åˆ†æ˜¯run ID
                            logger.info(f"âœ… æå–åˆ°run ID: {run_id}")
                            
                            # å°è¯•è¯»å–runä¿¡æ¯
                            run_info_file = latest_run_dir / "files" / "wandb-metadata.json"
                            run_url = None
                            if run_info_file.exists():
                                try:
                                    with open(run_info_file, 'r') as f:
                                        metadata = json.load(f)
                                        run_url = metadata.get('url', '')
                                        if run_url:
                                            logger.info(f"ğŸ”— æ‰¾åˆ°Run URL: {run_url}")
                                except Exception as e:
                                    logger.warning(f"âš ï¸ è¯»å–metadataå¤±è´¥: {e}")
                            
                            return run_id, run_url
            
            # æ–¹æ³•2: æ£€æŸ¥ç¯å¢ƒå˜é‡ä¸­æ˜¯å¦å·²æœ‰run ID
            env_run_id = os.getenv("WANDB_RUN_ID")
            if env_run_id:
                logger.info(f"ğŸ”„ ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„WandB run ID: {env_run_id}")
                return env_run_id, None
            
            logger.info("âŒ æœªèƒ½æ‰¾åˆ°WandB run ID")
            return None, None
            
        except Exception as e:
            logger.warning(f"âš ï¸ æå–WandB run IDæ—¶å‡ºé”™: {e}")
            return None, None

    def _setup_logging(self):
        """Setup logging with improved error handling"""
        from datetime import datetime

        run_specific_name_from_env = os.getenv("WANDB_RUN_NAME")
        if not run_specific_name_from_env:
            # ğŸ”§ åŒ…å«è®­ç»ƒç­–ç•¥æ ‡è¯†
            prefix = getattr(self.env_cfg, 'wandb_run_name_prefix', 'grpo')
            prefix = f"{self.training_strategy}_{prefix}"
            
            run_specific_name_from_env = f"{prefix}_{datetime.now().strftime('%Y%m%d-%H%M%S')}"

        sanitized_run_name = re.sub(r'[^\w\-.]', '_', run_specific_name_from_env)

        # Determine if resuming
        is_resuming = (
            self.grpo_cfg.resume_from_checkpoint and
            isinstance(self.grpo_cfg.resume_from_checkpoint, str) and
            os.path.isdir(self.grpo_cfg.resume_from_checkpoint)
        )

        if is_resuming:
            actual_output_dir = os.path.dirname(self.grpo_cfg.resume_from_checkpoint)
        else:
            output_dir_base = getattr(self.env_cfg, 'output_dir_base', './outputs')
            actual_output_dir = os.path.join(output_dir_base, sanitized_run_name)

        if self.grpo_cfg.local_rank <= 0:
            os.makedirs(actual_output_dir, exist_ok=True)

        # Update config objects
        self.grpo_cfg.output_dir = actual_output_dir
        if hasattr(self.script_cfg, 'output_dir'):
            self.script_cfg.output_dir = actual_output_dir

        log_file_path = os.path.join(self.grpo_cfg.output_dir, "grpo_pipeline_log.txt")
        setup_global_logging(
            log_level=self.grpo_cfg.get_process_log_level(),
            log_file_path=log_file_path,
            local_rank=self.grpo_cfg.local_rank
        )
        logger.info(f"Global logging set up. Output directory: {self.grpo_cfg.output_dir}")

    def _log_configs(self):
        """ğŸ”§ å¢å¼ºé…ç½®æ—¥å¿—ï¼ŒåŒ…å«è®­ç»ƒç­–ç•¥ä¿¡æ¯"""
        # ğŸ”§ æ•°æ®é›†è·¯å¾„é…ç½®è°ƒè¯•
        logger.info("ğŸ“ æ•°æ®é›†é…ç½®:")
        logger.info(f"  - dataset_path: {getattr(self.script_cfg, 'dataset_path', 'None')}")
        logger.info(f"  - env_cfg.dataset_base_path: {getattr(self.env_cfg, 'dataset_base_path', 'None')}")
        if hasattr(self.script_cfg, 'dataset_path') and self.script_cfg.dataset_path:
            import os
            inferred_base = os.path.dirname(self.script_cfg.dataset_path)
            logger.info(f"  - ä»dataset_pathæ¨å¯¼çš„åŸºç¡€è·¯å¾„: {inferred_base}")
        
        # ğŸ”§ è®­ç»ƒç­–ç•¥ä¿¡æ¯
        logger.info("ğŸ¯ è®­ç»ƒç­–ç•¥é…ç½®:")
        logger.info(f"  - ç­–ç•¥: {self.training_strategy}")
        logger.info(f"  - GPUæ•°é‡: {self.multi_gpu_info.get('gpu_count', 0)}")
        logger.info(f"  - æ¨¡å‹å¹¶è¡Œ: {'âœ…' if self.multi_gpu_info.get('use_model_parallel', False) else 'âŒ'}")
        
        if self.multi_gpu_info:
            logger.info(f"  - æ€»GPUå†…å­˜: {self.multi_gpu_info.get('total_memory_gb', 0):.1f}GB")
            logger.info(f"  - å¹³å‡æ¯GPU: {self.multi_gpu_info.get('average_memory_gb', 0):.1f}GB")
        
        # é•¿åº¦é…ç½®ä¿¡æ¯
        logger.info("ğŸ“ é•¿åº¦é…ç½®:")
        logger.info(f"  - æ€»åºåˆ—é•¿åº¦: {self.script_cfg.max_seq_length}")
        logger.info(f"  - æœ€å¤§æç¤ºé•¿åº¦: {self.script_cfg.script_max_prompt_length} ({self.script_cfg.script_max_prompt_length/self.script_cfg.max_seq_length*100:.1f}%)")
        logger.info(f"  - æœ€å¤§è¾“å‡ºé•¿åº¦: {self.script_cfg.script_max_completion_length} ({self.script_cfg.script_max_completion_length/self.script_cfg.max_seq_length*100:.1f}%)")
        logger.info(f"  - åˆ†é…ç­–ç•¥: {self.script_cfg.length_allocation_strategy}")
        
        # è®­ç»ƒé…ç½®ä¿¡æ¯
        logger.info("ğŸ¯ è®­ç»ƒé…ç½®:")
        logger.info(f"  - æ¯GPUæ‰¹æ¬¡å¤§å°: {self.grpo_cfg.per_device_train_batch_size}")
        logger.info(f"  - æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {self.grpo_cfg.gradient_accumulation_steps}")
        logger.info(f"  - å­¦ä¹ ç‡: {self.grpo_cfg.learning_rate}")
        
        # è®¡ç®—æœ‰æ•ˆæ‰¹æ¬¡å¤§å°
        if self.training_strategy in ["model_parallel_only", "model_parallel_single_process"]:
            # æ¨¡å‹å¹¶è¡Œæ—¶ï¼Œæœ‰æ•ˆæ‰¹æ¬¡å¤§å°ä¸ä¼šå› GPUæ•°é‡å€å¢ï¼ˆå› ä¸ºæ¨¡å‹æ˜¯åˆ†å¸ƒçš„ï¼Œä¸æ˜¯æ•°æ®ï¼‰
            effective_batch_size = (self.grpo_cfg.per_device_train_batch_size * 
                                  self.grpo_cfg.gradient_accumulation_steps)
            logger.info(f"  - æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch_size} (æ¨¡å‹å¹¶è¡Œ)")
        elif self.training_strategy == "distributed_data_parallel":
            effective_batch_size = (self.grpo_cfg.per_device_train_batch_size * 
                                  self.grpo_cfg.gradient_accumulation_steps * 
                                  self.multi_gpu_info.get('gpu_count', 1))
            logger.info(f"  - æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch_size} (æ•°æ®å¹¶è¡Œ)")
        else:
            effective_batch_size = (self.grpo_cfg.per_device_train_batch_size * 
                                  self.grpo_cfg.gradient_accumulation_steps)
            logger.info(f"  - æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch_size} (å•GPU)")
        
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f"EnvConfig: \n{json.dumps(asdict(self.env_cfg), indent=2)}")
            logger.debug(f"ScriptConfig: \n{json.dumps(asdict(self.script_cfg), indent=2)}")
            logger.debug(f"EnhancedRewardConfig: \n{json.dumps(asdict(self.reward_cfg), indent=2)}")

    def _setup_model_and_tokenizer(self):
        """ğŸ”§ å¢å¼ºæ¨¡å‹å’Œtokenizerè®¾ç½®ï¼Œè§£å†³DDPå†²çª"""
        try:
            logger.info("ğŸ”§ è®¾ç½®æ¨¡å‹å’Œtokenizer...")
            
            # ğŸ”§ è®°å½•è®¾ç½®å‰çš„çŠ¶æ€
            if self.training_strategy in ["model_parallel_only", "model_parallel_single_process"]:
                logger.info(f"ğŸš€ å‡†å¤‡{self.training_strategy}æ¨¡å‹è®¾ç½®...")
                logger.info(f"  - ç›®æ ‡GPUæ•°é‡: {self.multi_gpu_info.get('gpu_count', 1)}")
                logger.info(f"  - æ¯GPUå†…å­˜é™åˆ¶: {getattr(self.script_cfg, 'max_memory_per_gpu', '75GiB')}")
                
                # è®°å½•è®¾ç½®å‰çš„GPUå†…å­˜çŠ¶æ€
                logger.info("ğŸ“Š è®¾ç½®å‰GPUå†…å­˜çŠ¶æ€:")
                for i in range(self.multi_gpu_info.get('gpu_count', 1)):
                    if torch.cuda.is_available() and i < torch.cuda.device_count():
                        allocated = torch.cuda.memory_allocated(i) / 1024**3
                        reserved = torch.cuda.memory_reserved(i) / 1024**3
                        total = torch.cuda.get_device_properties(i).total_memory / 1024**3
                        logger.info(f"  GPU {i}: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved, {total:.1f}GB total")
            
            # è°ƒç”¨ModelManagerçš„æ–¹æ³•
            start_time = time.time()
            self.model, self.tokenizer = self.model_manager.setup_model_and_tokenizer()
            setup_time = time.time() - start_time
            
            logger.info(f"â±ï¸ æ¨¡å‹åŠ è½½è€—æ—¶: {setup_time:.2f}ç§’")
            
            # ğŸ”§ è®°å½•æ¨¡å‹åŠ è½½åçš„çŠ¶æ€
            if self.training_strategy in ["model_parallel_only", "model_parallel_single_process"]:
                logger.info("ğŸ“Š æ¨¡å‹åŠ è½½åGPUå†…å­˜çŠ¶æ€:")
                total_allocated = 0
                for i in range(self.multi_gpu_info.get('gpu_count', 1)):
                    if torch.cuda.is_available() and i < torch.cuda.device_count():
                        allocated = torch.cuda.memory_allocated(i) / 1024**3
                        reserved = torch.cuda.memory_reserved(i) / 1024**3
                        total_allocated += allocated
                        logger.info(f"  GPU {i}: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved")
                
                logger.info(f"  æ€»åˆ†é…å†…å­˜: {total_allocated:.2f}GB")
                if self.multi_gpu_info.get('gpu_count', 1) > 1:
                    logger.info(f"  å¹³å‡æ¯GPU: {total_allocated/self.multi_gpu_info['gpu_count']:.2f}GB")
                
                # éªŒè¯æ¨¡å‹åˆ†å¸ƒ
                if hasattr(self.model, 'hf_device_map'):
                    logger.info("ğŸ—ºï¸ æ¨¡å‹è®¾å¤‡åˆ†å¸ƒéªŒè¯:")
                    device_counts = {}
                    for layer_name, device in self.model.hf_device_map.items():
                        device_str = str(device)
                        device_counts[device_str] = device_counts.get(device_str, 0) + 1
                    
                    for device, count in device_counts.items():
                        logger.info(f"  {device}: {count} å±‚")
                    
                    # æ£€æŸ¥æ˜¯å¦çœŸæ­£å®ç°äº†æ¨¡å‹å¹¶è¡Œ
                    if len(device_counts) > 1:
                        logger.info("âœ… ç¡®è®¤å®ç°æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒ")
                    else:
                        logger.warning("âš ï¸ æ¨¡å‹ä¼¼ä¹æ²¡æœ‰åˆ†å¸ƒåˆ°å¤šä¸ªè®¾å¤‡")
                else:
                    logger.warning("âš ï¸ æœªæ‰¾åˆ°è®¾å¤‡æ˜ å°„ä¿¡æ¯")
            
            # åº”ç”¨PEFTé€‚é…å™¨
            is_resuming = (
                self.grpo_cfg.resume_from_checkpoint and 
                isinstance(self.grpo_cfg.resume_from_checkpoint, str) and 
                os.path.isdir(self.grpo_cfg.resume_from_checkpoint)
            )
            
            logger.info("ğŸ”§ åº”ç”¨PEFTé€‚é…å™¨...")
            adapter_start_time = time.time()
            self.model = self.model_manager.apply_peft_adapter(
                model=self.model,
                is_resuming=is_resuming,
                resume_checkpoint_path=str(self.grpo_cfg.resume_from_checkpoint) if is_resuming else None
            )
            adapter_time = time.time() - adapter_start_time
            logger.info(f"â±ï¸ PEFTé€‚é…å™¨è®¾ç½®è€—æ—¶: {adapter_time:.2f}ç§’")
            
            # ğŸ”§ éªŒè¯æœ€ç»ˆæ¨¡å‹çŠ¶æ€
            if hasattr(self.model_manager, 'get_model_memory_info'):
                memory_info = self.model_manager.get_model_memory_info()
                logger.info("ğŸ“ˆ æœ€ç»ˆæ¨¡å‹å†…å­˜ä¿¡æ¯:")
                logger.info(f"  - æ€»å‚æ•°: {memory_info.get('model_parameters', 0):,}")
                logger.info(f"  - å¯è®­ç»ƒå‚æ•°: {memory_info.get('trainable_parameters', 0):,}")
                
                gpu_memory = memory_info.get('gpu_memory', {})
                for gpu_id, gpu_info in gpu_memory.items():
                    logger.info(f"  - {gpu_id}: {gpu_info.get('allocated', 0):.2f}GB allocated")
            
            logger.info("âœ… Model and tokenizer setup completed successfully.")
            return self.model, self.tokenizer
            
        except Exception as e:
            logger.error(f"âŒ Failed to setup model and tokenizer: {e}", exc_info=True)
            
            # å¤šGPUé”™è¯¯æ—¶çš„è¯¦ç»†è¯Šæ–­
            if self.training_strategy in ["model_parallel_only", "model_parallel_single_process"]:
                logger.error("ğŸ” æ¨¡å‹å¹¶è¡Œè®¾ç½®å¤±è´¥ï¼Œè¿›è¡Œè¯Šæ–­:")
                try:
                    for i in range(self.multi_gpu_info.get('gpu_count', 1)):
                        if torch.cuda.is_available() and i < torch.cuda.device_count():
                            allocated = torch.cuda.memory_allocated(i) / 1024**3
                            reserved = torch.cuda.memory_reserved(i) / 1024**3
                            logger.error(f"  GPU {i}: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved")
                            
                            # æ£€æŸ¥æ˜¯å¦æœ‰è¿›ç¨‹å ç”¨GPU
                            if allocated > 1.0:  # è¶…è¿‡1GBå¯èƒ½æœ‰å…¶ä»–è¿›ç¨‹
                                logger.error(f"  âš ï¸ GPU {i} å¯èƒ½è¢«å…¶ä»–è¿›ç¨‹å ç”¨")
                                
                except Exception as diag_e:
                    logger.error(f"è¯Šæ–­è¿‡ç¨‹ä¸­å‡ºé”™: {diag_e}")
            
            raise

    def _initialize_components(self, dataset_processed):
        """Initialize components that depend on the processed dataset"""
        try:
            logger.info("Initializing training components...")
            
            # ç»éªŒå›æ”¾ç¼“å†²åŒºè®¾ç½®
            if self.script_cfg.enable_experience_replay:
                # æ ¹æ®è®­ç»ƒç­–ç•¥è°ƒæ•´ç¼“å†²åŒºå¤§å°
                buffer_size = self.script_cfg.experience_buffer_size
                if self.training_strategy in ["model_parallel_only", "model_parallel_single_process"]:
                    buffer_size = max(buffer_size, 1500)  # æ¨¡å‹å¹¶è¡Œæ—¶å¯ä»¥ä½¿ç”¨æ›´å¤§ç¼“å†²åŒº
                    logger.info(f"ğŸš€ æ¨¡å‹å¹¶è¡Œæ¨¡å¼ä¸‹ä½¿ç”¨å¢å¼ºçš„ç»éªŒå›æ”¾ç¼“å†²åŒº: {buffer_size}")
                
                self.experience_buffer = ExperienceBuffer(max_size=buffer_size)
                logger.info(f"Experience buffer initialized (size: {buffer_size}).")
                
                # Restore experience buffer state if resuming
                if self.grpo_cfg.resume_from_checkpoint and os.path.isdir(self.grpo_cfg.resume_from_checkpoint):
                    buffer_state_path = os.path.join(self.grpo_cfg.resume_from_checkpoint, "enhanced_experience_buffer_state.json")
                    if os.path.exists(buffer_state_path):
                        try:
                            logger.info(f"Loading experience buffer state from: {buffer_state_path}")
                            with open(buffer_state_path, "r", encoding="utf-8") as f:
                                state_data = json.load(f)
                            self.experience_buffer.load_buffer_state(state_data)
                            logger.info("âœ… Experience buffer state loaded successfully.")
                        except Exception as e:
                            logger.warning(f"âš ï¸ Failed to load experience buffer state: {e}")

            # Curriculum learning setup
            if self.script_cfg.enable_curriculum:
                self.curriculum_manager = setup_fixed_curriculum_manager(self.script_cfg, dataset_processed)
                if self.curriculum_manager:
                    logger.info(f"Curriculum learning enabled: {type(self.curriculum_manager).__name__}.")
                    
                    # Restore curriculum state if resuming
                    if self.grpo_cfg.resume_from_checkpoint and os.path.isdir(self.grpo_cfg.resume_from_checkpoint):
                        curriculum_state_path = os.path.join(self.grpo_cfg.resume_from_checkpoint, "enhanced_curriculum_state.json")
                        if os.path.exists(curriculum_state_path):
                            try:
                                logger.info(f"Loading curriculum state from: {curriculum_state_path}")
                                with open(curriculum_state_path, "r", encoding="utf-8") as f:
                                    state_data = json.load(f)
                                self.curriculum_manager.load_curriculum_state(state_data)
                                logger.info("âœ… Curriculum state loaded successfully.")
                            except Exception as e:
                                logger.warning(f"âš ï¸ Failed to load curriculum state: {e}")

            # Setup callbacks
            self._setup_callbacks(dataset_processed)
            logger.info("âœ… All components initialized successfully.")
            
        except Exception as e:
            logger.error(f"âŒ Error during component initialization: {e}", exc_info=True)
            raise

    def _setup_callbacks(self, dataset_processed):
        """ğŸ”§ å¢å¼ºå›è°ƒè®¾ç½®ï¼Œé¿å…DDPå†²çª"""
        try:
            self.callbacks = []
            
            # Basic monitoring callbacks
            self.callbacks.append(StepLoggingCallback())
            self.callbacks.append(DetailedRewardCallback(self.grpo_cfg.output_dir))
            self.callbacks.append(RewardStabilityMonitor(self.grpo_cfg.output_dir))

            # è¯¾ç¨‹å­¦ä¹ å›è°ƒè®¾ç½®
            if self.curriculum_manager:
                # æ ¹æ®è®­ç»ƒç­–ç•¥è°ƒæ•´æ€§èƒ½æ£€æŸ¥é—´éš”
                check_interval = self.script_cfg.curriculum_performance_check_interval
                if self.training_strategy in ["model_parallel_only", "model_parallel_single_process"] and check_interval < 10:
                    check_interval = 10
                    logger.info(f"ğŸ”§ æ¨¡å‹å¹¶è¡Œæ¨¡å¼ä¸‹è°ƒæ•´è¯¾ç¨‹æ£€æŸ¥é—´éš”: {check_interval}")
                
                # æ·»åŠ è¯¾ç¨‹å­¦ä¹ ç›¸å…³å›è°ƒ
                curriculum_progress_cb = CurriculumProgressCallback(
                    curriculum_manager=self.curriculum_manager, 
                    trainer_ref=None,
                    output_dir=self.grpo_cfg.output_dir,
                    performance_check_interval=check_interval
                )
                self.callbacks.append(curriculum_progress_cb)
                logger.info("âœ… æ·»åŠ  CurriculumProgressCallback")

                enhanced_curriculum_cb = EnhancedCurriculumDebugCallback(
                    curriculum_manager=self.curriculum_manager, 
                    trainer_ref=None,
                    output_dir=self.grpo_cfg.output_dir
                )
                self.callbacks.append(enhanced_curriculum_cb)
                logger.info("âœ… æ·»åŠ  EnhancedCurriculumDebugCallback")

                optimized_curriculum_cb = OptimizedCurriculumCallback(
                    curriculum_manager=self.curriculum_manager,
                    trainer_ref=None,
                    output_dir=self.grpo_cfg.output_dir
                )
                self.callbacks.append(optimized_curriculum_cb)
                logger.info("âœ… æ·»åŠ  OptimizedCurriculumCallback")

            # State persistence callback
            self.callbacks.append(CustomStatePersistenceCallback(
                curriculum_manager=self.curriculum_manager, 
                experience_buffer=self.experience_buffer, 
                script_cfg=self.script_cfg
            ))

            # æ¨ç†å›è°ƒè®¾ç½®
            if self.tokenizer and dataset_processed and len(dataset_processed) > 0:
                # æ ¹æ®è®­ç»ƒç­–ç•¥è°ƒæ•´æ¨ç†å‚æ•°
                eval_samples = self.script_cfg.callback_num_samples
                eval_interval = self.script_cfg.callback_eval_every_n_steps
                
                if self.training_strategy in ["model_parallel_only", "model_parallel_single_process"]:
                    if eval_interval < 15:
                        eval_interval = 15
                        logger.info(f"ğŸ”§ æ¨¡å‹å¹¶è¡Œæ¨¡å¼ä¸‹è°ƒæ•´æ¨ç†å›è°ƒé—´éš”: {eval_interval}")
                
                sample_dataset_for_inf_cb = dataset_processed.select(
                    range(min(len(dataset_processed), eval_samples * 5))
                )

                if len(sample_dataset_for_inf_cb) > 0:
                    # å°è¯•ä½¿ç”¨æµå¼å¼•å¯¼æ¨ç†å›è°ƒ
                    try:
                        from grpo_project.utils.streaming_guidance import create_streaming_inference_callback, GuidanceConfig
                        
                        guidance_config = GuidanceConfig(
                            min_reasoning_length=getattr(self.script_cfg, 'min_reasoning_length', 60),
                            guidance_trigger_threshold=getattr(self.script_cfg, 'guidance_trigger_threshold', 40),
                            max_guidance_attempts=getattr(self.script_cfg, 'max_guidance_attempts', 2),
                            guidance_tokens_limit=getattr(self.script_cfg, 'guidance_tokens_limit', 25)
                        )
                        
                        streaming_callback = create_streaming_inference_callback(
                            model=self.model,
                            tokenizer=self.tokenizer,
                            eval_dataset=sample_dataset_for_inf_cb,
                            num_samples=eval_samples,
                            eval_every_n_steps=eval_interval,
                            max_new_tokens=self.script_cfg.script_max_completion_length,
                            max_seq_length=self.script_cfg.max_seq_length,
                            experience_buffer=self.experience_buffer,
                            output_dir=self.grpo_cfg.output_dir,
                            guidance_config=guidance_config
                        )
                        
                        self.callbacks.append(streaming_callback)
                        logger.info(f"âœ… æµå¼å¼•å¯¼æ¨ç†å›è°ƒå·²æ·»åŠ  (samples: {len(sample_dataset_for_inf_cb)}, interval: {eval_interval})")
                        
                    except ImportError:
                        # é™çº§åˆ°æ ‡å‡†æ¨ç†å›è°ƒ
                        logger.warning("âš ï¸ æµå¼å¼•å¯¼æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†æ¨ç†å›è°ƒ")
                        
                        if hasattr(DetailedInferenceCallback, '__init__'):
                            inference_cb = DetailedInferenceCallback(
                                tokenizer=self.tokenizer,
                                eval_dataset=sample_dataset_for_inf_cb,
                                num_samples=eval_samples,
                                eval_every_n_steps=eval_interval,
                                max_new_tokens=self.script_cfg.script_max_completion_length,
                                max_seq_length=self.script_cfg.max_seq_length,
                                experience_buffer=self.experience_buffer,
                                output_dir=self.grpo_cfg.output_dir
                            )
                            self.callbacks.append(inference_cb)
                            logger.info("âœ… æ ‡å‡†æ¨ç†å›è°ƒå·²æ·»åŠ ")
                else:
                    logger.warning("âš ï¸ æ ·æœ¬æ•°æ®ä¸è¶³ï¼Œè·³è¿‡æ¨ç†å›è°ƒ")

            # WandBå›è°ƒè®¾ç½®
            if self.grpo_cfg.local_rank <= 0 and "wandb" in self.grpo_cfg.report_to:
                try:
                    from grpo_project.callbacks.wandb_sync_callback import SyncedWandbCallback
                    wandb_cb = SyncedWandbCallback(
                        env_cfg=self.env_cfg, 
                        script_cfg=self.script_cfg, 
                        reward_cfg=self.reward_cfg, 
                        experience_buffer=self.experience_buffer
                    )
                    self.callbacks.append(wandb_cb)
                    self.wandb_callback = wandb_cb
                    logger.info("âœ… SyncedWandbCallback added (æ›¿ä»£åŸç”ŸWandB).")
                except ImportError:
                    logger.warning("âš ï¸ SyncedWandbCallbackä¸å¯ç”¨ï¼Œè·³è¿‡WandBå›è°ƒ")

            logger.info(f"Total callbacks prepared: {len(self.callbacks)}")
            
        except Exception as e:
            logger.error(f"âŒ Error setting up callbacks: {e}", exc_info=True)
            self.callbacks = [StepLoggingCallback()]
            logger.warning("âš ï¸ Using minimal callback setup due to errors.")

    def get_reward_function(self):
        """Create optimized reward function closure with enhanced efficiency"""
        reward_call_count = 0  # ç”¨äºå‡å°‘æ—¥å¿—é¢‘ç‡
        
        def optimized_reward_fn_closure(prompts: List[str], completions: List[str], **kwargs_from_trainer_dataset) -> List[float]:
            nonlocal reward_call_count
            reward_call_count += 1
            
            try:
                current_training_step = self.trainer.state.global_step if self.trainer and self.trainer.state else 0

                # ğŸ”§ ä¼˜åŒ–1: å¤§å¹…å‡å°‘æ—¥å¿—é¢‘ç‡ä»¥æé«˜æ•ˆç‡
                log_interval = 50 if self.training_strategy in ["model_parallel_only", "model_parallel_single_process"] else 30
                
                if reward_call_count % log_interval == 1:
                    logger.info(f"ğŸ¯ æ­¥æ•° {current_training_step}: å¥–åŠ±è®¡ç®— #{reward_call_count}")
                    logger.info(f"  - æ‰¹æ¬¡å¤§å°: {len(prompts)}")
                    logger.info(f"  - å®Œæˆé•¿åº¦ç»Ÿè®¡: min={min(len(c) for c in completions)}, max={max(len(c) for c in completions)}, avg={sum(len(c) for c in completions)/len(completions):.1f}")

                # ğŸ”§ ä¼˜åŒ–2: é¢„æ„å»ºå‚æ•°å­—å…¸ï¼Œå‡å°‘é‡å¤æ„å»ºå¼€é”€
                batch_rewards_args = {
                    "prompts": prompts,
                    "completions": completions,
                    "testbench_paths": kwargs_from_trainer_dataset.get('testbench_path', []),
                    "expected_total_tests_list": kwargs_from_trainer_dataset.get('expected_total_tests', []),
                    "reference_verilog_paths": kwargs_from_trainer_dataset.get('reference_verilog_path', []),
                    "original_enhanced_prompts": kwargs_from_trainer_dataset.get('original_enhanced_prompt'),
                    "training_step": current_training_step,
                    "output_dir_for_debug": self.grpo_cfg.output_dir,
                    "wandb_callback_obj": getattr(self, 'wandb_callback', None),
                    "experience_buffer_obj": self.experience_buffer,
                    "script_config_obj": self.script_cfg
                }
                
                # ğŸ”§ ä¼˜åŒ–3: å¿«é€Ÿå¥–åŠ±è®¡ç®—
                start_time = time.time()
                rewards_list, aggregated_metrics = self.reward_calculator.calculate_batch_rewards(**batch_rewards_args)
                calc_time = time.time() - start_time
                
                # ğŸ”§ ä¼˜åŒ–4: æ•ˆç‡ç›‘æ§å’Œç»Ÿè®¡
                if reward_call_count % log_interval == 1:
                    reward_stats = {
                        'mean': np.mean(rewards_list) if rewards_list else 0,
                        'std': np.std(rewards_list) if rewards_list else 0,
                        'min': np.min(rewards_list) if rewards_list else 0,
                        'max': np.max(rewards_list) if rewards_list else 0
                    }
                    avg_calc_time_per_sample = calc_time / len(prompts) if prompts else 0
                    
                    logger.info(f"  - å¥–åŠ±ç»Ÿè®¡: mean={reward_stats['mean']:.4f}, std={reward_stats['std']:.4f}")
                    logger.info(f"  - å¥–åŠ±èŒƒå›´: [{reward_stats['min']:.4f}, {reward_stats['max']:.4f}]")
                    logger.info(f"  - è®¡ç®—æ•ˆç‡: {calc_time:.2f}s total, {avg_calc_time_per_sample:.3f}s/sample")
                    
                    # æ€§èƒ½è­¦å‘Š
                    if avg_calc_time_per_sample > 2.0:
                        logger.warning(f"âš ï¸ å¥–åŠ±è®¡ç®—è¾ƒæ…¢ ({avg_calc_time_per_sample:.3f}s/sample)ï¼Œè€ƒè™‘ä¼˜åŒ–")
                
                return rewards_list
                    
            except Exception as e:
                logger.error(f"âŒ å¥–åŠ±å‡½æ•°é”™è¯¯ (step {current_training_step}, call #{reward_call_count}): {e}", exc_info=True)
                # ğŸ”§ ä¼˜åŒ–5: è¿”å›åˆç†çš„é»˜è®¤å¥–åŠ±è€Œéé›¶å€¼
                return [0.1] * len(prompts)  # å°æ­£å€¼é¿å…è®­ç»ƒåœæ»
        
        return optimized_reward_fn_closure

    def train(self):
        """ğŸ”§ ä¿®å¤è®­ç»ƒå‡½æ•°ï¼Œè§£å†³DDP/æ¨¡å‹å¹¶è¡Œå†²çª"""
        try:
            logger.info("ğŸš€ Starting enhanced GRPO training process...")
            logger.info(f"ğŸ¯ è®­ç»ƒç­–ç•¥: {self.training_strategy}")
            
            # è®¾ç½®ç¨³å®šçš„è®­ç»ƒç¯å¢ƒ
            self._setup_stable_environment()
            
            # é…ç½®WandBæ¢å¤
            logger.info("ğŸ“ Step 0: Configuring WandB resume settings...")
            self._configure_wandb_resume()
            
            # è®¾ç½®WandBåŒæ­¥ç®¡ç†å™¨è¿è¡Œ
            logger.info("ğŸ“ Step 0.5: Setting up WandB sync manager run...")
            self._setup_wandb_run()

            # 1. Setup model and tokenizer
            logger.info("ğŸ“ Step 1: Setting up model and tokenizer...")
            self._setup_model_and_tokenizer()

            # 2. Load and preprocess data
            logger.info("ğŸ“ Step 2: Loading and preparing dataset...")
            dataset_processed = load_and_prepare_dataset(
                script_cfg=self.script_cfg,
                env_cfg=self.env_cfg,
                tokenizer=self.tokenizer
            )

            if not dataset_processed or len(dataset_processed) == 0:
                raise ValueError("âŒ Dataset is empty after processing!")

            # 3. Initialize components
            logger.info("ğŸ“ Step 3: Initializing training components...")
            self._initialize_components(dataset_processed)

            # 4. Determine training dataset
            dataset_for_trainer = dataset_processed
            if self.curriculum_manager:
                dataset_for_trainer = self.curriculum_manager.get_current_stage_dataset()
                current_stage_name = self.curriculum_manager.get_current_stage_info()['stage_name']
                logger.info(f"ğŸ“š Using curriculum dataset: {len(dataset_for_trainer)} samples from stage '{current_stage_name}'.")
            else:
                logger.info(f"ğŸ“š Using full processed dataset: {len(dataset_for_trainer)} samples.")

            if not dataset_for_trainer or len(dataset_for_trainer) == 0:
                raise ValueError("âŒ Training dataset is empty!")

            # å­˜å‚¨æ•°æ®é›†å¼•ç”¨
            self.dataset_for_trainer = dataset_for_trainer

            # 5. ğŸ”§ å…³é”®ä¿®å¤ï¼šæ ¹æ®è®­ç»ƒç­–ç•¥åˆ›å»ºGRPOTrainer
            logger.info("ğŸ“ Step 4: Creating GRPOTrainer...")
            from trl import GRPOTrainer
            
            # å‡†å¤‡GRPOé…ç½®
            grpo_cfg_for_trainer = self._prepare_grpo_config_for_strategy()
            
            # ğŸ”§ å¼ºåˆ¶ä¿®å¤ï¼šæ— è®ºæ£€æµ‹åˆ°ä»€ä¹ˆè®­ç»ƒç­–ç•¥ï¼Œéƒ½åº”ç”¨è®¾å¤‡ä¸€è‡´æ€§ä¿®å¤
            logger.info(f"ğŸ”§ å¼ºåˆ¶åº”ç”¨è®¾å¤‡ä¿®å¤ - æ£€æµ‹åˆ°è®­ç»ƒç­–ç•¥: {self.training_strategy}")
            logger.info(f"ğŸ”§ GPUæ•°é‡: {self.multi_gpu_info.get('gpu_count', 1)}")
            logger.info(f"ğŸ”§ æ¨¡å‹å¹¶è¡Œè®¾ç½®: {self.multi_gpu_info.get('use_model_parallel', False)}")
            
            # å¦‚æœæœ‰å¤šå¼ GPUï¼Œå¼ºåˆ¶åº”ç”¨æ¨¡å‹å¹¶è¡Œç­–ç•¥
            if self.multi_gpu_info.get('gpu_count', 1) > 1 and self.multi_gpu_info.get('use_model_parallel', False):
                logger.info("ğŸ”§ å¼ºåˆ¶ä¿®æ­£è®­ç»ƒç­–ç•¥ä¸ºæ¨¡å‹å¹¶è¡Œ")
                self.training_strategy = "model_parallel_single_process"
            
            self.trainer = GRPOTrainer(
                model=self.model,
                args=grpo_cfg_for_trainer,
                train_dataset=dataset_for_trainer,
                reward_funcs=[self.get_reward_function()],
                callbacks=self.callbacks
            )

            # è®¾ç½®trainerå¼•ç”¨
            for cb in self.callbacks:
                if hasattr(cb, 'trainer_ref') and cb.trainer_ref is None:
                    cb.trainer_ref = self.trainer
                    logger.debug(f"Set trainer_ref for {type(cb).__name__}")

            # 6. ğŸ”§ å¼ºåˆ¶åº”ç”¨è®¾å¤‡ä¸€è‡´æ€§ä¿®å¤
            logger.info("ğŸ”§ æ— æ¡ä»¶åº”ç”¨è®¾å¤‡ä¸€è‡´æ€§ä¿®å¤...")
            self._apply_universal_device_fix()
            
            # 7. è®­ç»ƒå‰æœ€ç»ˆæ£€æŸ¥
            if self.training_strategy in ["model_parallel_only", "model_parallel_single_process"]:
                logger.info("ğŸš€ æ¨¡å‹å¹¶è¡Œè®­ç»ƒå‰æœ€ç»ˆæ£€æŸ¥:")
                self._final_model_parallel_check(dataset_for_trainer)

            # 8. Start training
            logger.info("ğŸ“ Step 5: Starting training...")
            logger.info(f"ğŸ¯ Training with {len(dataset_for_trainer)} examples using {self.training_strategy}.")
            
            training_start_time = time.time()
            
            train_result = self.trainer.train(resume_from_checkpoint=self.grpo_cfg.resume_from_checkpoint)
            
            training_end_time = time.time()
            training_duration = training_end_time - training_start_time
            
            logger.info("âœ… Training completed successfully!")
            logger.info(f"â±ï¸ æ€»è®­ç»ƒæ—¶é—´: {training_duration:.2f}ç§’ ({training_duration/60:.2f}åˆ†é’Ÿ)")
            
            # è®­ç»ƒåçŠ¶æ€æ£€æŸ¥
            if self.training_strategy in ["model_parallel_only", "model_parallel_single_process"]:
                self._post_training_check()

            # 9. Save artifacts
            if self.grpo_cfg.local_rank <= 0:
                self._save_training_artifacts(train_result)

        except Exception as e:
            logger.error(f"âŒ Training failed: {e}", exc_info=True)
            
            # è¯¦ç»†çš„é”™è¯¯è¯Šæ–­
            self._diagnose_training_failure(e)
            
            raise



    def _apply_universal_device_fix(self):
        """ğŸ”§ é€šç”¨è®¾å¤‡ä¸€è‡´æ€§ä¿®å¤ï¼ˆé€‚ç”¨äºæ‰€æœ‰GPUé…ç½®ï¼‰"""
        try:
            logger.info("ğŸ”§ åº”ç”¨é€šç”¨è®¾å¤‡ä¸€è‡´æ€§ä¿®å¤...")
            
            # FSDP å·²ç»ç”± transformers å†…éƒ¨å¤„ç†è®¾å¤‡åˆ†å¸ƒï¼Œè¿™é‡Œè·³è¿‡è¡¥ä¸
            if self.training_strategy and self.training_strategy.startswith("fsdp"):
                logger.info("â„¹ï¸ FSDP ç­–ç•¥ä¸‹è·³è¿‡é€šç”¨è®¾å¤‡è¡¥ä¸")
                return
            
            if not self.trainer:
                logger.warning("âš ï¸ è®­ç»ƒå™¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡è®¾å¤‡ä¿®å¤")
                return
            
            # è·å–æ¨¡å‹è®¾å¤‡ä¿¡æ¯
            if hasattr(self.trainer.model, 'parameters'):
                model_devices = set()
                for param in self.trainer.model.parameters():
                    model_devices.add(param.device)
                
                if model_devices:
                    primary_device = list(model_devices)[0]
                    logger.info(f"ğŸ”§ æ¨¡å‹ä¸»è®¾å¤‡: {primary_device}")
                    logger.info(f"ğŸ”§ æ£€æµ‹åˆ°è®¾å¤‡æ•°é‡: {len(model_devices)}")
                    
                    # å¦‚æœæ˜¯æ¨¡å‹å¹¶è¡Œï¼Œæ˜¾ç¤ºè®¾å¤‡åˆ†å¸ƒ
                    if len(model_devices) > 1:
                        logger.info("ğŸš€ æ¨¡å‹å¹¶è¡Œè®¾å¤‡åˆ†å¸ƒ:")
                        for i, device in enumerate(sorted(model_devices, key=str)):
                            logger.info(f"  - è®¾å¤‡ {i+1}: {device}")
                    
                    # ğŸ”§ åº”ç”¨è®¾å¤‡ä¿®å¤è¡¥ä¸
                    self._patch_grpo_device_consistency()
                    
                    # ğŸ”§ å¢å¼ºtokenizerè®¾å¤‡ä¸€è‡´æ€§
                    self._ensure_tokenizer_device_consistency()
                    
                    logger.info("âœ… é€šç”¨è®¾å¤‡ä¿®å¤å®Œæˆ")
                else:
                    logger.warning("âš ï¸ æ— æ³•è·å–æ¨¡å‹è®¾å¤‡ä¿¡æ¯")
            else:
                logger.warning("âš ï¸ æ¨¡å‹æ²¡æœ‰å‚æ•°ä¿¡æ¯")
                
        except Exception as e:
            logger.error(f"âŒ é€šç”¨è®¾å¤‡ä¿®å¤å¤±è´¥: {e}")
    
    def _ensure_tokenizer_device_consistency(self):
        """ğŸ”§ ç¡®ä¿tokenizerè®¾å¤‡ä¸€è‡´æ€§"""
        try:
            if hasattr(self.trainer, 'tokenizer') and self.trainer.tokenizer:
                # ç¡®ä¿tokenizerçš„ç‰¹æ®Štokenåœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                if hasattr(self.trainer.tokenizer, 'pad_token_id') and self.trainer.tokenizer.pad_token_id is not None:
                    logger.debug("ğŸ”§ tokenizerè®¾å¤‡ä¸€è‡´æ€§æ£€æŸ¥å®Œæˆ")
                    
        except Exception as e:
            logger.debug(f"âš ï¸ tokenizerè®¾å¤‡ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {e}")
    
    def _add_grpo_error_fallback(self):
        """ğŸ”§ ç®€åŒ–çš„GRPOé”™è¯¯å¤„ç†å›é€€æœºåˆ¶"""
        logger.info("ğŸ”§ GRPOé”™è¯¯å¤„ç†å›é€€æœºåˆ¶å·²åŒ…å«åœ¨è®¾å¤‡ä¸€è‡´æ€§è¡¥ä¸ä¸­")

    def _fix_trainer_device_consistency(self):
        """ğŸ”§ ä¿®å¤è®­ç»ƒå™¨è®¾å¤‡ä¸€è‡´æ€§é—®é¢˜ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        try:
            logger.info("ğŸ”§ ä¿®å¤è®­ç»ƒå™¨è®¾å¤‡ä¸€è‡´æ€§...")
            
            # è·å–ä¸»è®¾å¤‡ï¼ˆæ¨¡å‹çš„ç¬¬ä¸€ä¸ªå‚æ•°æ‰€åœ¨è®¾å¤‡ï¼‰
            model_device = next(self.trainer.model.parameters()).device
            logger.info(f"  - æ¨¡å‹ä¸»è®¾å¤‡: {model_device}")
            
            # ğŸ”§ æ›´å…¨é¢çš„è®¾å¤‡ä¸€è‡´æ€§ä¿®å¤
            if self.training_strategy in ["model_parallel_only", "model_parallel_single_process"]:
                # ç¡®ä¿tokenizeråœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                if hasattr(self.trainer, 'tokenizer') and self.trainer.tokenizer:
                    if hasattr(self.trainer.tokenizer, 'to'):
                        self.trainer.tokenizer.to(model_device)
                
                # æ£€æŸ¥å¹¶ä¿®å¤æ•°æ®collatorçš„è®¾å¤‡è®¾ç½®
                if hasattr(self.trainer, 'data_collator'):
                    if hasattr(self.trainer.data_collator, 'tokenizer'):
                        if hasattr(self.trainer.data_collator.tokenizer, 'to'):
                            self.trainer.data_collator.tokenizer.to(model_device)
                
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šåº”ç”¨è®¾å¤‡ä¸€è‡´æ€§è¡¥ä¸ï¼ˆå·²åœ¨_apply_universal_device_fixä¸­å¤„ç†ï¼‰
                logger.info("ğŸ”§ è®¾å¤‡ä¸€è‡´æ€§è¡¥ä¸å°†åœ¨_apply_universal_device_fixä¸­åº”ç”¨")
            
            logger.info("âœ… è®¾å¤‡ä¸€è‡´æ€§ä¿®å¤å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ è®¾å¤‡ä¸€è‡´æ€§ä¿®å¤å¤±è´¥: {e}")
    
    def _patch_grpo_device_consistency(self):
        """ğŸ”§ é«˜æ•ˆGRPOè®¾å¤‡ä¸€è‡´æ€§è¡¥ä¸ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            logger.info("ğŸ”§ åº”ç”¨é«˜æ•ˆGRPOè®¾å¤‡ä¸€è‡´æ€§è¡¥ä¸...")
            
            if not hasattr(self.trainer, '_generate_and_score_completions'):
                logger.warning("âš ï¸ æœªæ‰¾åˆ° _generate_and_score_completions æ–¹æ³•")
                return
                
            original_method = self.trainer._generate_and_score_completions
            device_fix_count = 0  # ç”¨äºå‡å°‘æ—¥å¿—é¢‘ç‡
            
            def optimized_device_fix(batch):
                """ä¼˜åŒ–çš„è®¾å¤‡é”™è¯¯ä¿®å¤ï¼Œå‡å°‘å¼€é”€"""
                nonlocal device_fix_count
                
                try:
                    # ğŸ”§ ä¼˜åŒ–1: ç¼“å­˜ä¸»è®¾å¤‡ä¿¡æ¯ï¼Œé¿å…é‡å¤æŸ¥è¯¢
                    if not hasattr(optimized_device_fix, '_cached_primary_device'):
                        optimized_device_fix._cached_primary_device = next(self.trainer.model.parameters()).device
                    
                    primary_device = optimized_device_fix._cached_primary_device
                    
                    # ğŸ”§ ä¼˜åŒ–2: å¿«é€Ÿæ£€æŸ¥æ˜¯å¦éœ€è¦è®¾å¤‡è½¬æ¢
                    needs_device_fix = False
                    if isinstance(batch, dict):
                        for key, value in batch.items():
                            if torch.is_tensor(value) and value.device != primary_device:
                                needs_device_fix = True
                                break
                    elif torch.is_tensor(batch) and batch.device != primary_device:
                        needs_device_fix = True
                    
                    # ğŸ”§ ä¼˜åŒ–3: åªåœ¨éœ€è¦æ—¶è¿›è¡Œè®¾å¤‡è½¬æ¢
                    if needs_device_fix:
                        def fast_to_device(obj):
                            if torch.is_tensor(obj):
                                return obj.to(primary_device, non_blocking=True)
                            elif isinstance(obj, dict):
                                return {k: fast_to_device(v) for k, v in obj.items()}
                            elif isinstance(obj, (list, tuple)):
                                return type(obj)(fast_to_device(item) for item in obj)
                            return obj
                        
                        prepared_batch = fast_to_device(batch)
                    else:
                        prepared_batch = batch
                    
                    # ğŸ”§ ä¼˜åŒ–4: åœ¨ä¸»è®¾å¤‡ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œ
                    with torch.cuda.device(primary_device):
                        return original_method(prepared_batch)
                        
                except RuntimeError as e:
                    if "Expected all tensors to be on the same device" in str(e):
                        device_fix_count += 1
                        
                        # ğŸ”§ ä¼˜åŒ–5: å‡å°‘è­¦å‘Šæ—¥å¿—é¢‘ç‡
                        if device_fix_count % 10 == 1:  # æ¯10æ¬¡åªè®°å½•ä¸€æ¬¡
                            logger.warning(f"ğŸ”§ GRPOè®¾å¤‡ä¸ä¸€è‡´ä¿®å¤ (#{device_fix_count}): {str(e)[:80]}...")
                        
                        # ğŸ”§ ä¼˜åŒ–6: æ›´æ¿€è¿›çš„è®¾å¤‡ç»Ÿä¸€ç­–ç•¥
                        try:
                            def force_device_unification(obj):
                                if torch.is_tensor(obj):
                                    return obj.to('cuda:0', non_blocking=True)
                                elif isinstance(obj, dict):
                                    return {k: force_device_unification(v) for k, v in obj.items()}
                                elif isinstance(obj, (list, tuple)):
                                    return type(obj)(force_device_unification(item) for item in obj)
                                return obj
                            
                            unified_batch = force_device_unification(batch)
                            
                            # ğŸ”§ ä¼˜åŒ–7: ä½¿ç”¨cuda:0è®¾å¤‡ä¸Šä¸‹æ–‡å’Œç¦ç”¨æ··åˆç²¾åº¦
                            with torch.cuda.device('cuda:0'):
                                with torch.autocast('cuda', enabled=False):
                                    result = original_method(unified_batch)
                            
                            if device_fix_count % 20 == 1:
                                logger.debug(f"âœ… è®¾å¤‡ç»Ÿä¸€ä¿®å¤æˆåŠŸ (#{device_fix_count})")
                            
                            return result
                            
                        except Exception as fallback_e:
                            if device_fix_count % 50 == 1:
                                logger.error(f"âŒ è®¾å¤‡ä¿®å¤å¤±è´¥ (#{device_fix_count}): {fallback_e}")
                            raise e
                    else:
                        raise e
                        
                except Exception as other_e:
                    logger.error(f"âŒ GRPOè®¾å¤‡ä¿®å¤é‡åˆ°å…¶ä»–é”™è¯¯: {other_e}")
                    raise other_e
            
            # åº”ç”¨ä¼˜åŒ–è¡¥ä¸
            self.trainer._generate_and_score_completions = optimized_device_fix
            logger.info("âœ… é«˜æ•ˆGRPOè®¾å¤‡ä¸€è‡´æ€§è¡¥ä¸å·²åº”ç”¨")
            
        except Exception as e:
            logger.error(f"âŒ åº”ç”¨GRPOè¡¥ä¸å¤±è´¥: {e}")

    def _final_model_parallel_check(self, dataset_for_trainer):
        """ğŸ”§ æ¨¡å‹å¹¶è¡Œè®­ç»ƒå‰çš„æœ€ç»ˆæ£€æŸ¥"""
        logger.info("ğŸš€ æ¨¡å‹å¹¶è¡Œè®­ç»ƒå‰æœ€ç»ˆæ£€æŸ¥:")
        logger.info(f"  - æ•°æ®é›†å¤§å°: {len(dataset_for_trainer)}")
        logger.info(f"  - æ¯GPUæ‰¹æ¬¡å¤§å°: {self.grpo_cfg.per_device_train_batch_size}")
        logger.info(f"  - æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {self.grpo_cfg.gradient_accumulation_steps}")
        
        effective_batch_size = (self.grpo_cfg.per_device_train_batch_size * 
                              self.grpo_cfg.gradient_accumulation_steps)
        logger.info(f"  - æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch_size}")
        
        # GPUå†…å­˜æ£€æŸ¥
        logger.info("ğŸ“Š è®­ç»ƒå‰GPUå†…å­˜çŠ¶æ€:")
        for i in range(self.multi_gpu_info.get('gpu_count', 1)):
            if torch.cuda.is_available() and i < torch.cuda.device_count():
                allocated = torch.cuda.memory_allocated(i) / 1024**3
                reserved = torch.cuda.memory_reserved(i) / 1024**3
                total = torch.cuda.get_device_properties(i).total_memory / 1024**3
                usage_percent = (allocated / total) * 100
                logger.info(f"    GPU {i}: {allocated:.2f}GB/{total:.1f}GB ({usage_percent:.1f}%)")
                
                # å†…å­˜è­¦å‘Š
                if usage_percent > 90:
                    logger.warning(f"âš ï¸ GPU {i} å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ ({usage_percent:.1f}%)ï¼Œå¯èƒ½å¯¼è‡´OOM")
                elif usage_percent > 75:
                    logger.warning(f"âš ï¸ GPU {i} å†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜ ({usage_percent:.1f}%)ï¼Œè¯·æ³¨æ„ç›‘æ§")
        
        # éªŒè¯æ¨¡å‹ç¡®å®æ˜¯åˆ†å¸ƒå¼çš„
        if hasattr(self.model, 'hf_device_map'):
            device_set = set(str(device) for device in self.model.hf_device_map.values())
            if len(device_set) > 1:
                logger.info(f"âœ… ç¡®è®¤æ¨¡å‹å·²åˆ†å¸ƒåˆ° {len(device_set)} ä¸ªè®¾å¤‡: {sorted(device_set)}")
            else:
                logger.warning(f"âš ï¸ è­¦å‘Šï¼šæ¨¡å‹ä¼¼ä¹åªåœ¨å•ä¸€è®¾å¤‡ {device_set} ä¸Š")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡æ®‹ç•™
        dist_vars = ['RANK', 'LOCAL_RANK', 'WORLD_SIZE', 'MASTER_ADDR', 'MASTER_PORT']
        remaining_vars = [var for var in dist_vars if var in os.environ]
        if remaining_vars:
            logger.warning(f"âš ï¸ æ£€æµ‹åˆ°æ®‹ç•™çš„åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡: {remaining_vars}")
            logger.warning("è¿™å¯èƒ½å¯¼è‡´DDPå†²çªï¼Œå»ºè®®æ¸…é™¤è¿™äº›å˜é‡")

    def _post_training_check(self):
        """ğŸ”§ è®­ç»ƒåçŠ¶æ€æ£€æŸ¥"""
        logger.info("ğŸ“Š è®­ç»ƒå®ŒæˆåGPUå†…å­˜çŠ¶æ€:")
        for i in range(self.multi_gpu_info.get('gpu_count', 1)):
            if torch.cuda.is_available() and i < torch.cuda.device_count():
                allocated = torch.cuda.memory_allocated(i) / 1024**3
                reserved = torch.cuda.memory_reserved(i) / 1024**3
                logger.info(f"    GPU {i}: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved")

    def _diagnose_training_failure(self, error):
        """ğŸ”§ è®­ç»ƒå¤±è´¥æ—¶çš„è¯¦ç»†è¯Šæ–­"""
        logger.error("ğŸ” è®­ç»ƒå¤±è´¥è¯Šæ–­:")
        logger.error(f"  - é”™è¯¯ç±»å‹: {type(error).__name__}")
        logger.error(f"  - è®­ç»ƒç­–ç•¥: {self.training_strategy}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯DDPç›¸å…³é”™è¯¯
        error_str = str(error).lower()
        if any(keyword in error_str for keyword in ['dtensor', 'distributed', 'ddp', 'nccl']):
            logger.error("ğŸš¨ æ£€æµ‹åˆ°åˆ†å¸ƒå¼/DDPç›¸å…³é”™è¯¯!")
            logger.error("ğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
            logger.error("  1. ç¡®ä¿æ²¡æœ‰åŒæ—¶ä½¿ç”¨æ¨¡å‹å¹¶è¡Œå’ŒDDP")
            logger.error("  2. æ£€æŸ¥ç¯å¢ƒå˜é‡ä¸­æ˜¯å¦æœ‰åˆ†å¸ƒå¼è®¾ç½®")
            logger.error("  3. å°è¯•ä½¿ç”¨pure model parallelæ¨¡å¼")
            
            # æ£€æŸ¥ç¯å¢ƒå˜é‡
            dist_vars = ['RANK', 'LOCAL_RANK', 'WORLD_SIZE', 'MASTER_ADDR', 'MASTER_PORT']
            for var in dist_vars:
                if var in os.environ:
                    logger.error(f"  å‘ç°åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡: {var}={os.environ[var]}")
        
        # GPUçŠ¶æ€è¯Šæ–­
        if self.multi_gpu_info.get('use_model_parallel', False):
            logger.error("ğŸ” å¤šGPUçŠ¶æ€è¯Šæ–­:")
            try:
                for i in range(self.multi_gpu_info.get('gpu_count', 1)):
                    if torch.cuda.is_available() and i < torch.cuda.device_count():
                        allocated = torch.cuda.memory_allocated(i) / 1024**3
                        reserved = torch.cuda.memory_reserved(i) / 1024**3
                        logger.error(f"    GPU {i}: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved")
            except Exception as diag_e:
                logger.error(f"GPUè¯Šæ–­å¤±è´¥: {diag_e}")

    def _save_training_artifacts(self, train_result):
        """ğŸ”§ å¢å¼ºè®­ç»ƒäº§ç‰©ä¿å­˜ï¼ŒåŒ…å«è®­ç»ƒç­–ç•¥ä¿¡æ¯"""
        try:
            logger.info("ğŸ’¾ Saving training artifacts...")
            
            # Save final model
            final_model_dir = os.path.join(self.grpo_cfg.output_dir, "final_model_adapter")
            self.trainer.save_model(final_model_dir)
            logger.info(f"âœ… Final model saved to: {final_model_dir}")

            # ä¿å­˜è®­ç»ƒç­–ç•¥å’Œå¤šGPUä¿¡æ¯
            training_info = {
                "training_strategy": self.training_strategy,
                "multi_gpu_info": self.multi_gpu_info,
                "training_completed": True,
                "final_step": getattr(self.trainer.state, 'global_step', 0) if self.trainer and self.trainer.state else 0
            }
            
            training_info_file = os.path.join(self.grpo_cfg.output_dir, "training_strategy_info.json")
            with open(training_info_file, 'w') as f:
                json.dump(training_info, f, indent=2)
            logger.info(f"âœ… Training strategy info saved to: {training_info_file}")

            # Save metrics and state
            metrics = train_result.metrics if hasattr(train_result, 'metrics') else {}
            
            # æ·»åŠ è®­ç»ƒç­–ç•¥ç›¸å…³æŒ‡æ ‡
            metrics['training_strategy'] = self.training_strategy
            metrics['use_model_parallel'] = self.multi_gpu_info.get('use_model_parallel', False)
            metrics['gpu_count'] = self.multi_gpu_info.get('gpu_count', 1)
            
            if self.multi_gpu_info.get('use_model_parallel', False):
                metrics['total_gpu_memory_gb'] = self.multi_gpu_info['total_memory_gb']
                
                # ä¿å­˜æœ€ç»ˆå†…å­˜ä½¿ç”¨æƒ…å†µ
                final_memory_usage = {}
                for i in range(self.multi_gpu_info['gpu_count']):
                    if torch.cuda.is_available() and i < torch.cuda.device_count():
                        allocated = torch.cuda.memory_allocated(i) / 1024**3
                        reserved = torch.cuda.memory_reserved(i) / 1024**3
                        final_memory_usage[f'gpu_{i}_allocated_gb'] = allocated
                        final_memory_usage[f'gpu_{i}_reserved_gb'] = reserved
                
                metrics.update(final_memory_usage)
            
            # ä¿å­˜æŒ‡æ ‡
            if hasattr(self.trainer, 'log_metrics'):
                self.trainer.log_metrics("train_summary", metrics)
            
            metrics_file = os.path.join(self.grpo_cfg.output_dir, "final_train_metrics.json")
            try:
                with open(metrics_file, 'w') as f:
                    json.dump(metrics, f, indent=2)
                logger.info(f"âœ… Metrics saved to: {metrics_file}")
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to save metrics to file: {e}")
            
            if hasattr(self.trainer, 'save_state'):
                self.trainer.save_state()
                logger.info("âœ… Trainer state saved.")

            # ä¿å­˜æ¨¡å‹åˆ†å¸ƒä¿¡æ¯ï¼ˆå¦‚æœä½¿ç”¨æ¨¡å‹å¹¶è¡Œï¼‰
            if (self.training_strategy in ["model_parallel_only", "model_parallel_single_process"] and 
                hasattr(self.model, 'hf_device_map')):
                device_map_file = os.path.join(self.grpo_cfg.output_dir, "model_device_map.json")
                try:
                    # è½¬æ¢device mapä¸ºå¯åºåˆ—åŒ–æ ¼å¼
                    serializable_device_map = {}
                    for layer_name, device in self.model.hf_device_map.items():
                        serializable_device_map[layer_name] = str(device)
                    
                    with open(device_map_file, 'w') as f:
                        json.dump(serializable_device_map, f, indent=2)
                    logger.info(f"âœ… Model device map saved to: {device_map_file}")
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to save device map: {e}")

            logger.info(f"ğŸ‰ All training artifacts saved to: {self.grpo_cfg.output_dir}")
            
        except Exception as e:
            logger.error(f"âŒ Error saving training artifacts: {e}", exc_info=True)

    def cleanup(self):
        """ğŸ”§ å¢å¼ºèµ„æºæ¸…ç†ï¼Œæ”¯æŒä¸åŒè®­ç»ƒç­–ç•¥"""
        try:
            logger.info("ğŸ§¹ Cleaning up resources...")
            
            # 1. æ¸…ç†WandBåŒæ­¥ç®¡ç†å™¨
            try:
                sync_manager = get_wandb_sync_manager()
                if sync_manager:
                    sync_manager.finish()
                    logger.info("âœ… WandBåŒæ­¥ç®¡ç†å™¨å·²æ¸…ç†")
            except Exception as e:
                logger.warning(f"âš ï¸ WandBåŒæ­¥ç®¡ç†å™¨æ¸…ç†å¤±è´¥: {e}")
            
            # 2. æ ¹æ®è®­ç»ƒç­–ç•¥æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                logger.info(f"ğŸ§¹ æ¸…ç† {gpu_count} å¼ GPUçš„å†…å­˜...")
                
                total_memory_freed = 0
                for i in range(gpu_count):
                    try:
                        # è®°å½•æ¸…ç†å‰çš„å†…å­˜
                        before_allocated = torch.cuda.memory_allocated(i) / 1024**3
                        before_reserved = torch.cuda.memory_reserved(i) / 1024**3
                        
                        # è®¾ç½®è®¾å¤‡å¹¶æ¸…ç†
                        torch.cuda.set_device(i)
                        torch.cuda.empty_cache()
                        
                        # è®°å½•æ¸…ç†åçš„å†…å­˜
                        after_allocated = torch.cuda.memory_allocated(i) / 1024**3
                        after_reserved = torch.cuda.memory_reserved(i) / 1024**3
                        
                        freed = before_reserved - after_reserved
                        total_memory_freed += freed
                        
                        logger.info(f"    GPU {i}: é‡Šæ”¾ {freed:.2f}GB (å‰©ä½™: {after_allocated:.2f}GB allocated, {after_reserved:.2f}GB reserved)")
                        
                    except Exception as gpu_e:
                        logger.warning(f"âš ï¸ GPU {i} æ¸…ç†å¤±è´¥: {gpu_e}")
                
                logger.info(f"âœ… æ€»å…±é‡Šæ”¾GPUå†…å­˜: {total_memory_freed:.2f}GB")
            
            # 3. Force garbage collection
            gc.collect()
            logger.info("âœ… Garbage collection completed.")
            
            # 4. è®­ç»ƒç­–ç•¥æ€»ç»“
            logger.info("ğŸ“Š è®­ç»ƒæ€»ç»“:")
            logger.info(f"    - è®­ç»ƒç­–ç•¥: {self.training_strategy}")
            if self.multi_gpu_info:
                logger.info(f"    - ä½¿ç”¨GPUæ•°é‡: {self.multi_gpu_info.get('gpu_count', 0)}")
                logger.info(f"    - æ¨¡å‹å¹¶è¡Œ: {'âœ…' if self.multi_gpu_info.get('use_model_parallel', False) else 'âŒ'}")
                if self.multi_gpu_info.get('use_model_parallel', False):
                    logger.info(f"    - æ€»GPUå†…å­˜: {self.multi_gpu_info.get('total_memory_gb', 0):.1f}GB")
                    logger.info(f"    - å†…å­˜æ•ˆç‡: é«˜")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Error during cleanup: {e}")


def main():
    """ğŸ”§ å¢å¼ºä¸»å…¥å£ï¼Œæ”¯æŒè®­ç»ƒç­–ç•¥é”™è¯¯å¤„ç†"""
    pipeline = None
    try:
        logger_temp.info("ğŸš€ Initializing Enhanced GRPO Training Pipeline...")
        pipeline = GRPOTrainingPipeline()
        
        # è®­ç»ƒç­–ç•¥éªŒè¯å’Œæ‘˜è¦
        if hasattr(pipeline, 'training_strategy'):
            logger_temp.info("ğŸ”§ è®­ç»ƒç­–ç•¥æ‘˜è¦:")
            logger_temp.info(f"    ç­–ç•¥: {pipeline.training_strategy}")
            if hasattr(pipeline, 'multi_gpu_info') and pipeline.multi_gpu_info:
                logger_temp.info(f"    GPUæ•°é‡: {pipeline.multi_gpu_info.get('gpu_count', 0)}")
                logger_temp.info(f"    æ¨¡å‹å¹¶è¡Œ: {'âœ…' if pipeline.multi_gpu_info.get('use_model_parallel', False) else 'âŒ'}")
                logger_temp.info(f"    æ€»å†…å­˜: {pipeline.multi_gpu_info.get('total_memory_gb', 0):.1f}GB")
        
        logger_temp.info("ğŸ¯ Starting training...")
        pipeline.train()
        
        logger_temp.info("âœ… Training completed successfully!")
        
    except KeyboardInterrupt:
        logger_temp.warning("âš ï¸ Training interrupted by user (Ctrl+C)")
        
        # è®­ç»ƒä¸­æ–­æ—¶çš„æ¸…ç†
        if pipeline and hasattr(pipeline, 'training_strategy'):
            logger_temp.info(f"ğŸ§¹ {pipeline.training_strategy}è®­ç»ƒä¸­æ–­ï¼Œæ­£åœ¨æ¸…ç†èµ„æº...")
            try:
                if (hasattr(pipeline, 'multi_gpu_info') and 
                    pipeline.multi_gpu_info.get('use_model_parallel', False) and 
                    torch.cuda.is_available()):
                    for i in range(torch.cuda.device_count()):
                        torch.cuda.set_device(i)
                        torch.cuda.empty_cache()
                logger_temp.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")
            except Exception as cleanup_e:
                logger_temp.warning(f"âš ï¸ æ¸…ç†è¿‡ç¨‹ä¸­å‡ºé”™: {cleanup_e}")
        
        return 1
        
    except Exception as e:
        logger_temp.error(f"ğŸ’¥ Fatal error in training pipeline: {e}", exc_info=True)
        
        # é”™è¯¯æ—¶çš„è¯¦ç»†è¯Šæ–­
        if pipeline and hasattr(pipeline, 'training_strategy'):
            logger_temp.error(f"ğŸ” {pipeline.training_strategy}è®­ç»ƒå¤±è´¥ï¼Œè¿›è¡Œæœ€ç»ˆè¯Šæ–­:")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯DDPå†²çªé”™è¯¯
            error_str = str(e).lower()
            if any(keyword in error_str for keyword in ['dtensor', 'distributed', 'ddp', 'mixed', 'broadcast']):
                logger_temp.error("ğŸš¨ æ£€æµ‹åˆ°DDP/æ¨¡å‹å¹¶è¡Œå†²çªé”™è¯¯!")
                logger_temp.error("ğŸ’¡ è§£å†³å»ºè®®:")
                logger_temp.error("  1. ç¡®ä¿ä½¿ç”¨å•è¿›ç¨‹å¯åŠ¨: python main.py (ä¸ä½¿ç”¨torchrun)")
                logger_temp.error("  2. æ¸…é™¤æ‰€æœ‰åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡:")
                logger_temp.error("     unset RANK LOCAL_RANK WORLD_SIZE MASTER_ADDR MASTER_PORT")
                logger_temp.error("  3. è®¾ç½®çº¯æ¨¡å‹å¹¶è¡Œæ¨¡å¼: --use_model_parallel true")
                logger_temp.error("  4. æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–è¿›ç¨‹åœ¨ä½¿ç”¨GPU")
            
            # GPUçŠ¶æ€è¯Šæ–­
            try:
                if (hasattr(pipeline, 'multi_gpu_info') and 
                    pipeline.multi_gpu_info.get('use_model_parallel', False) and 
                    torch.cuda.is_available()):
                    for i in range(pipeline.multi_gpu_info.get('gpu_count', 0)):
                        if i < torch.cuda.device_count():
                            allocated = torch.cuda.memory_allocated(i) / 1024**3
                            reserved = torch.cuda.memory_reserved(i) / 1024**3
                            logger_temp.error(f"    GPU {i}: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved")
            except Exception as diag_e:
                logger_temp.error(f"æœ€ç»ˆè¯Šæ–­å¤±è´¥: {diag_e}")
        
        return 1
        
    finally:
        if pipeline:
            try:
                pipeline.cleanup()
            except Exception as cleanup_error:
                logger_temp.warning(f"âš ï¸ Error during cleanup: {cleanup_error}")
        
        logger_temp.info("ğŸ Enhanced GRPO Training Pipeline execution finished.")
        return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)# main.py - ä¿®å¤DDP/æ¨¡å‹å¹¶è¡Œå†²çªç‰ˆæœ¬