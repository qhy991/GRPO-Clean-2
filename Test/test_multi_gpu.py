#!/usr/bin/env python3

import os
import torch

print("ğŸ” å¤šGPUç¯å¢ƒæµ‹è¯•")
print("="*50)

# 1. æ£€æŸ¥CUDAå¯ç”¨æ€§
print(f"CUDAå¯ç”¨æ€§: {torch.cuda.is_available()}")
if not torch.cuda.is_available():
    print("âŒ CUDAä¸å¯ç”¨ï¼Œé€€å‡ºæµ‹è¯•")
    exit(1)

# 2. æ£€æŸ¥GPUæ•°é‡
gpu_count = torch.cuda.device_count()
print(f"GPUæ•°é‡: {gpu_count}")

# 3. æ£€æŸ¥ç¯å¢ƒå˜é‡
cuda_visible = os.environ.get('CUDA_VISIBLE_DEVICES', 'æœªè®¾ç½®')
print(f"CUDA_VISIBLE_DEVICES: {cuda_visible}")

# 4. æ£€æŸ¥æ¯ä¸ªGPUçŠ¶æ€
print("\nGPUè¯¦ç»†ä¿¡æ¯:")
for i in range(gpu_count):
    props = torch.cuda.get_device_properties(i)
    memory_gb = props.total_memory / 1024**3
    print(f"  GPU {i}: {props.name}, {memory_gb:.1f}GB")

# 5. æµ‹è¯•GPUé€šä¿¡
if gpu_count >= 2:
    print("\nğŸ”— æµ‹è¯•GPUé€šä¿¡...")
    try:
        device0 = torch.device('cuda:0')
        device1 = torch.device('cuda:1')
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        x = torch.randn(100, 100, device=device0)
        print(f"  åœ¨GPU 0åˆ›å»ºæ•°æ®: {x.shape}")
        
        # ç§»åŠ¨åˆ°GPU 1
        y = x.to(device1)
        print(f"  ç§»åŠ¨åˆ°GPU 1: {y.device}")
        
        # ç§»åŠ¨å›GPU 0
        z = y.to(device0)
        print(f"  ç§»åŠ¨å›GPU 0: {z.device}")
        
        # éªŒè¯æ•°æ®ä¸€è‡´æ€§
        is_consistent = torch.allclose(x, z)
        print(f"  æ•°æ®ä¸€è‡´æ€§: {is_consistent}")
        
        if is_consistent:
            print("âœ… GPUé€šä¿¡æµ‹è¯•æˆåŠŸ")
        else:
            print("âŒ GPUé€šä¿¡æµ‹è¯•å¤±è´¥ - æ•°æ®ä¸ä¸€è‡´")
            
    except Exception as e:
        print(f"âŒ GPUé€šä¿¡æµ‹è¯•å¤±è´¥: {e}")
else:
    print("âš ï¸ GPUæ•°é‡ä¸è¶³ï¼Œè·³è¿‡é€šä¿¡æµ‹è¯•")

# 6. æ¨¡æ‹Ÿé…ç½®æ£€æŸ¥
print("\nğŸ¯ æ¨¡æ‹Ÿè®­ç»ƒé…ç½®:")
use_model_parallel = gpu_count >= 2
print(f"  æ¨èæ¨¡å‹å¹¶è¡Œ: {use_model_parallel}")
print(f"  æ¨èè®­ç»ƒç­–ç•¥: {'model_parallel_single_process' if use_model_parallel else 'single_gpu'}")

print("\n" + "="*50)
print("å¤šGPUç¯å¢ƒæµ‹è¯•å®Œæˆ") 