#!/usr/bin/env python3
"""ç®€å•çš„GRPOè®¾å¤‡é”™è¯¯ä¿®å¤"""

import torch
import logging

logger = logging.getLogger(__name__)

def apply_simple_grpo_fix(trainer):
    """åº”ç”¨ç®€å•çš„GRPOä¿®å¤ï¼Œä¸“é—¨è§£å†³eos_idxè®¾å¤‡é”™è¯¯"""
    try:
        logger.info("ğŸ”§ åº”ç”¨ç®€å•GRPOä¿®å¤...")
        
        if not hasattr(trainer, '_generate_and_score_completions'):
            logger.warning("âš ï¸ æœªæ‰¾åˆ° _generate_and_score_completions æ–¹æ³•")
            return False
            
        original_method = trainer._generate_and_score_completions
        
        def patched_method(batch):
            """ä¿®å¤eos_idxè®¾å¤‡é”™è¯¯çš„è¡¥ä¸"""
            try:
                # ç›´æ¥è°ƒç”¨åŸå§‹æ–¹æ³•
                return original_method(batch)
            except RuntimeError as e:
                if "Expected all tensors to be on the same device" in str(e) and "cuda:1 and cuda:0" in str(e):
                    logger.warning(f"ğŸ”§ æ£€æµ‹åˆ°GRPO eos_idxè®¾å¤‡é”™è¯¯ï¼Œåº”ç”¨ç´§æ€¥ä¿®å¤: {e}")
                    
                    # æ–¹æ³•ï¼šå¼ºåˆ¶æ‰€æœ‰è¾“å…¥åˆ°cuda:0ï¼Œç„¶åé‡è¯•
                    try:
                        def force_to_cuda0(obj):
                            if torch.is_tensor(obj):
                                return obj.to('cuda:0', non_blocking=True)
                            elif isinstance(obj, dict):
                                return {k: force_to_cuda0(v) for k, v in obj.items()}
                            elif isinstance(obj, (list, tuple)):
                                return type(obj)(force_to_cuda0(item) for item in obj)
                            else:
                                return obj
                        
                        cuda0_batch = force_to_cuda0(batch)
                        logger.info("ğŸ”§ å·²å¼ºåˆ¶æ‰€æœ‰å¼ é‡åˆ°cuda:0ï¼Œé‡è¯•...")
                        
                        with torch.cuda.device('cuda:0'):
                            result = original_method(cuda0_batch)
                        
                        logger.info("âœ… ç´§æ€¥ä¿®å¤æˆåŠŸ")
                        return result
                        
                    except Exception as fix_e:
                        logger.error(f"âŒ ç´§æ€¥ä¿®å¤å¤±è´¥: {fix_e}")
                        raise e
                else:
                    raise e
        
        # åº”ç”¨è¡¥ä¸
        trainer._generate_and_score_completions = patched_method
        logger.info("âœ… ç®€å•GRPOä¿®å¤å·²åº”ç”¨")
        return True
        
    except Exception as e:
        logger.error(f"âŒ åº”ç”¨ç®€å•GRPOä¿®å¤å¤±è´¥: {e}")
        return False

def test_simple_fix():
    """æµ‹è¯•ç®€å•ä¿®å¤æ–¹æ³•"""
    print("ğŸ§ª æµ‹è¯•ç®€å•GRPOä¿®å¤...")
    
    # æ¨¡æ‹Ÿbatchæ•°æ®
    test_batch = [
        torch.randn(2, 10, device='cuda:0'),
        torch.randn(2, 10, device='cuda:0')
    ]
    
    # æ¨¡æ‹Ÿtrainer
    class MockTrainer:
        def __init__(self):
            pass
        
        def _generate_and_score_completions(self, batch):
            # æ¨¡æ‹Ÿé”™è¯¯
            raise RuntimeError("Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0!")
    
    trainer = MockTrainer()
    
    # åº”ç”¨ä¿®å¤
    success = apply_simple_grpo_fix(trainer)
    
    if success:
        try:
            result = trainer._generate_and_score_completions(test_batch)
            print("âœ… ä¿®å¤æµ‹è¯•æˆåŠŸ")
        except Exception as e:
            print(f"âŒ ä¿®å¤æµ‹è¯•å¤±è´¥: {e}")
    else:
        print("âŒ ä¿®å¤åº”ç”¨å¤±è´¥")

if __name__ == "__main__":
    if torch.cuda.is_available():
        test_simple_fix()
    else:
        print("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•æµ‹è¯•") 