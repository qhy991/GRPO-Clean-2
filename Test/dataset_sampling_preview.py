#!/usr/bin/env python3
"""
æ•°æ®é›†åˆ†å±‚æŠ½æ ·é¢„è§ˆè„šæœ¬
ç”¨äºåˆ†ææ•°æ®é›†çš„ç±»åˆ«åˆ†å¸ƒï¼Œå¹¶é¢„è§ˆåˆ†å±‚æŠ½æ ·æ•ˆæœ
"""

import json
import logging
from collections import defaultdict, Counter
from datasets import load_dataset
import argparse

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def analyze_dataset_distribution(dataset_path: str):
    """åˆ†ææ•°æ®é›†çš„ç±»åˆ«åˆ†å¸ƒ"""
    logger.info(f"ğŸ“Š åˆ†ææ•°æ®é›†: {dataset_path}")
    
    # åŠ è½½æ•°æ®é›†
    try:
        if dataset_path.endswith('.jsonl'):
            dataset = load_dataset('json', data_files=dataset_path, split='train')
        else:
            dataset = load_dataset('json', data_files=dataset_path, split='train')
        
        logger.info(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ: {len(dataset)} æ¡è®°å½•")
    except Exception as e:
        logger.error(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        return
    
    # åˆ†ææ•°æ®é›†å­—æ®µ
    logger.info(f"ğŸ“‹ æ•°æ®é›†å­—æ®µ: {dataset.column_names}")
    
    # åˆ†æå„ä¸ªæ½œåœ¨åˆ†ç±»å­—æ®µçš„åˆ†å¸ƒ
    classification_fields = ['level', 'category', 'difficulty', 'complexity_score']
    field_distributions = {}
    
    for field in classification_fields:
        if field in dataset.column_names:
            field_values = dataset[field]
            
            # å¯¹äºæ•°å€¼å­—æ®µï¼Œè¿›è¡Œåˆ†æ¡¶
            if field == 'complexity_score':
                bucketed_values = []
                for value in field_values:
                    if value is None:
                        bucketed_values.append('unknown')
                    elif isinstance(value, (int, float)):
                        if value <= 3:
                            bucketed_values.append('simple')
                        elif value <= 7:
                            bucketed_values.append('medium')
                        else:
                            bucketed_values.append('complex')
                    else:
                        bucketed_values.append('unknown')
                field_values = bucketed_values
            
            # ç»Ÿè®¡åˆ†å¸ƒ
            distribution = Counter(field_values)
            field_distributions[field] = distribution
            
            logger.info(f"ğŸ“ˆ {field} åˆ†å¸ƒ:")
            total = len(field_values)
            for value, count in distribution.most_common():
                percentage = (count / total) * 100
                logger.info(f"  {value}: {count} ({percentage:.1f}%)")
        else:
            logger.warning(f"âš ï¸ å­—æ®µ '{field}' ä¸å­˜åœ¨äºæ•°æ®é›†ä¸­")
    
    # åˆ†æç»„åˆç±»åˆ«åˆ†å¸ƒ
    if len(field_distributions) > 1:
        logger.info("ğŸ” åˆ†æç»„åˆç±»åˆ«åˆ†å¸ƒ...")
        combined_categories = defaultdict(int)
        
        for idx in range(len(dataset)):
            example = dataset[idx]
            category_parts = []
            
            for field in classification_fields:
                if field in example and field in field_distributions:
                    value = example[field]
                    
                    # å¯¹complexity_scoreè¿›è¡Œåˆ†æ¡¶
                    if field == 'complexity_score' and isinstance(value, (int, float)):
                        if value <= 3:
                            value = 'simple'
                        elif value <= 7:
                            value = 'medium'
                        else:
                            value = 'complex'
                    
                    category_parts.append(f"{field}:{value}")
            
            combined_key = '|'.join(category_parts) if category_parts else 'unknown'
            combined_categories[combined_key] += 1
        
        logger.info("ğŸ“Š ç»„åˆç±»åˆ«åˆ†å¸ƒ (å‰10ä¸ª):")
        total = len(dataset)
        for category, count in sorted(combined_categories.items(), key=lambda x: x[1], reverse=True)[:10]:
            percentage = (count / total) * 100
            logger.info(f"  {category}: {count} ({percentage:.1f}%)")
        
        logger.info(f"ğŸ“‹ æ€»å…±æœ‰ {len(combined_categories)} ä¸ªä¸åŒçš„ç»„åˆç±»åˆ«")
    
    return field_distributions, dataset

def preview_stratified_sampling(dataset, sample_ratio=0.1, stratify_columns=['level', 'category']):
    """é¢„è§ˆåˆ†å±‚æŠ½æ ·æ•ˆæœ"""
    logger.info(f"ğŸ¯ é¢„è§ˆåˆ†å±‚æŠ½æ ·æ•ˆæœ (é‡‡æ ·æ¯”ä¾‹: {sample_ratio*100:.1f}%)")
    
    # å¯¼å…¥åˆ†å±‚æŠ½æ ·å‡½æ•°
    from grpo_project.data.dataset import stratified_sample_dataset
    
    # æ‰§è¡Œåˆ†å±‚æŠ½æ ·
    sampled_dataset = stratified_sample_dataset(
        dataset=dataset,
        sample_ratio=sample_ratio,
        stratify_columns=stratify_columns,
        min_samples_per_category=1,
        random_seed=42
    )
    
    return sampled_dataset

def main():
    parser = argparse.ArgumentParser(description='æ•°æ®é›†åˆ†å±‚æŠ½æ ·é¢„è§ˆå·¥å…·')
    parser.add_argument('--dataset_path', type=str, 
                       default='/home/qhy/Research/LLM/GRPO-RV/dataset/all-with-module-2.jsonl',
                       help='æ•°æ®é›†è·¯å¾„')
    parser.add_argument('--sample_ratio', type=float, default=0.1,
                       help='é‡‡æ ·æ¯”ä¾‹ (é»˜è®¤: 0.1 å³ 10%)')
    parser.add_argument('--stratify_columns', type=str, default='level,category',
                       help='åˆ†å±‚å­—æ®µï¼Œç”¨é€—å·åˆ†éš” (é»˜è®¤: level,category)')
    
    args = parser.parse_args()
    
    # è§£æåˆ†å±‚å­—æ®µ
    stratify_columns = [col.strip() for col in args.stratify_columns.split(',') if col.strip()]
    
    logger.info("=" * 60)
    logger.info("ğŸ“Š æ•°æ®é›†åˆ†å±‚æŠ½æ ·é¢„è§ˆå·¥å…·")
    logger.info("=" * 60)
    
    # åˆ†æåŸå§‹æ•°æ®é›†åˆ†å¸ƒ
    field_distributions, dataset = analyze_dataset_distribution(args.dataset_path)
    
    if dataset is None:
        return
    
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ¯ åˆ†å±‚æŠ½æ ·é¢„è§ˆ")
    logger.info("=" * 60)
    
    # é¢„è§ˆåˆ†å±‚æŠ½æ ·æ•ˆæœ
    try:
        sampled_dataset = preview_stratified_sampling(
            dataset=dataset,
            sample_ratio=args.sample_ratio,
            stratify_columns=stratify_columns
        )
        
        logger.info(f"\nâœ… åˆ†å±‚æŠ½æ ·é¢„è§ˆå®Œæˆï¼")
        logger.info(f"åŸå§‹æ•°æ®: {len(dataset)} æ¡")
        logger.info(f"é‡‡æ ·å: {len(sampled_dataset)} æ¡")
        logger.info(f"å®é™…é‡‡æ ·æ¯”ä¾‹: {len(sampled_dataset)/len(dataset)*100:.2f}%")
        
    except Exception as e:
        logger.error(f"âŒ åˆ†å±‚æŠ½æ ·é¢„è§ˆå¤±è´¥: {e}")
        logger.error("è¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£… grpo_project åŒ…")

if __name__ == "__main__":
    main() 