#!/usr/bin/env python3
"""
è®­ç»ƒç¨³å®šæ€§ä¿®å¤è„šæœ¬
è§£å†³SIGSEGVæ®µé”™è¯¯å’Œæ¢¯åº¦æ£€æŸ¥ç‚¹é—®é¢˜
"""

import os
import sys
import torch
import gc
import signal
import logging
from pathlib import Path

logger = logging.getLogger(__name__)

def setup_signal_handlers():
    """è®¾ç½®ä¿¡å·å¤„ç†å™¨ä»¥ä¼˜é›…åœ°å¤„ç†ä¸­æ–­"""
    def signal_handler(signum, frame):
        logger.warning(f"æ¥æ”¶åˆ°ä¿¡å· {signum}ï¼Œå¼€å§‹æ¸…ç†...")
        cleanup_resources()
        sys.exit(1)
    
    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)
    if hasattr(signal, 'SIGSEGV'):
        signal.signal(signal.SIGSEGV, signal_handler)

def cleanup_resources():
    """æ¸…ç†GPUå†…å­˜å’Œå…¶ä»–èµ„æº"""
    try:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        gc.collect()
        logger.info("èµ„æºæ¸…ç†å®Œæˆ")
    except Exception as e:
        logger.error(f"èµ„æºæ¸…ç†å¤±è´¥: {e}")

def fix_gradient_checkpointing_config():
    """ä¿®å¤æ¢¯åº¦æ£€æŸ¥ç‚¹é…ç½®"""
    config_files = [
        "run_enhanced_grpo_training.sh",
        "grpo_project/configs/training.py"
    ]
    
    for config_file in config_files:
        if os.path.exists(config_file):
            logger.info(f"ä¿®å¤é…ç½®æ–‡ä»¶: {config_file}")
            # è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„é…ç½®ä¿®å¤é€»è¾‘
    
def set_stable_training_env():
    """è®¾ç½®ç¨³å®šçš„è®­ç»ƒç¯å¢ƒå˜é‡"""
    stable_envs = {
        # ç¦ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ç›¸å…³çš„ä¸ç¨³å®šç‰¹æ€§
        "TORCH_USE_CUDA_DSA": "1",  # å¯ç”¨CUDAè®¾å¤‡ç«¯æ–­è¨€
        "CUDA_LAUNCH_BLOCKING": "1",  # åŒæ­¥CUDAæ“ä½œä¾¿äºè°ƒè¯•
        "TORCH_DISTRIBUTED_DEBUG": "DETAIL",  # è¯¦ç»†çš„åˆ†å¸ƒå¼è°ƒè¯•ä¿¡æ¯
        "NCCL_DEBUG": "INFO",  # NCCLè°ƒè¯•ä¿¡æ¯
        "PYTORCH_CUDA_ALLOC_CONF": "max_split_size_mb:128",  # é™åˆ¶CUDAå†…å­˜åˆ†å‰²
    }
    
    for key, value in stable_envs.items():
        os.environ[key] = value
        logger.info(f"è®¾ç½®ç¯å¢ƒå˜é‡: {key}={value}")

def check_system_resources():
    """æ£€æŸ¥ç³»ç»Ÿèµ„æºæ˜¯å¦å……è¶³"""
    try:
        # æ£€æŸ¥GPUå†…å­˜
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                memory_allocated = torch.cuda.memory_allocated(i) / 1024**3
                memory_reserved = torch.cuda.memory_reserved(i) / 1024**3
                memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
                
                logger.info(f"GPU {i}: åˆ†é… {memory_allocated:.2f}GB, é¢„ç•™ {memory_reserved:.2f}GB, æ€»è®¡ {memory_total:.2f}GB")
                
                if memory_allocated / memory_total > 0.9:
                    logger.warning(f"GPU {i} å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory_allocated/memory_total*100:.1f}%")
                    return False
        
        return True
    except Exception as e:
        logger.error(f"èµ„æºæ£€æŸ¥å¤±è´¥: {e}")
        return False

def apply_stability_fixes():
    """åº”ç”¨æ‰€æœ‰ç¨³å®šæ€§ä¿®å¤"""
    logger.info("ğŸ”§ å¼€å§‹åº”ç”¨è®­ç»ƒç¨³å®šæ€§ä¿®å¤...")
    
    # 1. è®¾ç½®ä¿¡å·å¤„ç†å™¨
    setup_signal_handlers()
    logger.info("âœ… ä¿¡å·å¤„ç†å™¨è®¾ç½®å®Œæˆ")
    
    # 2. è®¾ç½®ç¨³å®šçš„ç¯å¢ƒå˜é‡
    set_stable_training_env()
    logger.info("âœ… ç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆ")
    
    # 3. æ£€æŸ¥ç³»ç»Ÿèµ„æº
    if not check_system_resources():
        logger.warning("âš ï¸ ç³»ç»Ÿèµ„æºä¸è¶³ï¼Œå»ºè®®æ¸…ç†GPUå†…å­˜åé‡è¯•")
    else:
        logger.info("âœ… ç³»ç»Ÿèµ„æºæ£€æŸ¥é€šè¿‡")
    
    # 4. ä¿®å¤æ¢¯åº¦æ£€æŸ¥ç‚¹é…ç½®
    fix_gradient_checkpointing_config()
    logger.info("âœ… æ¢¯åº¦æ£€æŸ¥ç‚¹é…ç½®ä¿®å¤å®Œæˆ")
    
    logger.info("ğŸ‰ æ‰€æœ‰ç¨³å®šæ€§ä¿®å¤åº”ç”¨å®Œæˆï¼")

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('training_stability_fix.log')
        ]
    )
    
    apply_stability_fixes() 