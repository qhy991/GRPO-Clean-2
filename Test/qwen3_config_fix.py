#!/usr/bin/env python3
"""
Qwen3 Model Configuration Fix Script
ä¿®å¤Qwen3æ¨¡å‹çš„é…ç½®é—®é¢˜ï¼ŒåŒ…æ‹¬èŠå¤©æ¨¡æ¿å’Œtokenizerè®¾ç½®
"""

import os
import json
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import Optional

def get_qwen3_chat_template():
    """è·å–Qwen3çš„æ­£ç¡®èŠå¤©æ¨¡æ¿"""
    return """{% for message in messages %}
{%- if message['role'] == 'system' %}
<|im_start|>system
{{ message['content'] }}<|im_end|>
{% elif message['role'] == 'user' %}
<|im_start|>user
{{ message['content'] }}<|im_end|>
{% elif message['role'] == 'assistant' %}
<|im_start|>assistant
{{ message['content'] }}<|im_end|>
{% endif %}
{% endfor %}
{%- if add_generation_prompt %}
<|im_start|>assistant
{% endif %}"""

def fix_qwen3_tokenizer(model_path: str, cache_dir: Optional[str] = None):
    """ä¿®å¤Qwen3 tokenizeré…ç½®"""
    print(f"ğŸ”§ æ­£åœ¨ä¿®å¤Qwen3 tokenizeré…ç½®: {model_path}")
    
    try:
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            cache_dir=cache_dir,
            trust_remote_code=True,
            use_fast=False  # Qwen3å»ºè®®ä½¿ç”¨slow tokenizer
        )
        
        # è®¾ç½®æ­£ç¡®çš„èŠå¤©æ¨¡æ¿
        tokenizer.chat_template = get_qwen3_chat_template()
        
        # ç¡®ä¿ç‰¹æ®Štokenæ­£ç¡®è®¾ç½®
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # éªŒè¯tokenè®¾ç½®
        print(f"âœ… EOS token: {tokenizer.eos_token}")
        print(f"âœ… PAD token: {tokenizer.pad_token}")
        print(f"âœ… Chat template set: {tokenizer.chat_template is not None}")
        
        # æµ‹è¯•èŠå¤©æ¨¡æ¿
        test_messages = [
            {"role": "user", "content": "Hello, how are you?"}
        ]
        
        try:
            formatted = tokenizer.apply_chat_template(
                test_messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            print(f"âœ… èŠå¤©æ¨¡æ¿æµ‹è¯•æˆåŠŸ:")
            print(f"   è¾“å…¥: {test_messages}")
            print(f"   è¾“å‡º: {repr(formatted)}")
        except Exception as e:
            print(f"âŒ èŠå¤©æ¨¡æ¿æµ‹è¯•å¤±è´¥: {e}")
            return False
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¿®å¤å¤±è´¥: {e}")
        return False

def create_optimized_training_config():
    """åˆ›å»ºä¼˜åŒ–çš„è®­ç»ƒé…ç½®"""
    config = {
        "model_config": {
            "model_name_or_path": "/home/share/Qwen3-8B/models--Qwen--Qwen3-8B/snapshots/a80f5e57cce20e57b65145f4213844dec1a80834",
            "trust_remote_code": True,
            "use_fast_tokenizer": False,
            "torch_dtype": "bfloat16"
        },
        
        "training_config": {
            # ä¼˜åŒ–çš„å­¦ä¹ ç‡è®¾ç½®
            "learning_rate": 2e-5,  # æé«˜å­¦ä¹ ç‡
            "warmup_ratio": 0.1,    # å‡å°‘warmup
            "lr_scheduler_type": "cosine_with_restarts",
            
            # ä¼˜åŒ–çš„æ‰¹å¤„ç†è®¾ç½®
            "per_device_train_batch_size": 2,  # å¢åŠ æ‰¹å¤„ç†å¤§å°
            "gradient_accumulation_steps": 16,  # å¢åŠ æ¢¯åº¦ç´¯ç§¯
            "dataloader_num_workers": 4,
            
            # ä¼˜åŒ–çš„ç”Ÿæˆå‚æ•°
            "gen_temperature": 0.8,
            "gen_top_k": 50,
            "gen_top_p": 0.9,
            "gen_repetition_penalty": 1.02,
            "gen_length_penalty": 1.0
        },
        
        "reward_config": {
            # é‡æ–°å¹³è¡¡å¥–åŠ±å‡½æ•°
            "compilation_success": 3.0,      # å¢åŠ æˆåŠŸå¥–åŠ±
            "compilation_failure": -2.0,     # å‡å°‘å¤±è´¥æƒ©ç½š
            "simulation_crash": -2.0,
            "test_pass_base_reward": 2.0,
            "test_pass_bonus_multiplier": 1.5,
            "max_functional_reward": 20.0,   # å¢åŠ æœ€å¤§å¥–åŠ±
            "all_tests_passed_bonus": 8.0,   # å¢åŠ é€šè¿‡æ‰€æœ‰æµ‹è¯•çš„å¥–åŠ±
            
            # æƒé‡è°ƒæ•´
            "functional_weight": 0.8,        # å¢åŠ åŠŸèƒ½æƒé‡
            "efficiency_weight": 0.1,
            "readability_weight": 0.05,
            "robustness_weight": 0.05
        },
        
        "curriculum_config": {
            # é™ä½è¯¾ç¨‹å­¦ä¹ é˜ˆå€¼
            "curriculum_performance_threshold_1": 0.4,  # ä»0.75é™ä½åˆ°0.4
            "curriculum_performance_threshold_2": 0.5,  # ä»0.70é™ä½åˆ°0.5
            "curriculum_performance_threshold_3": 0.6,  # ä»0.65é™ä½åˆ°0.6
            "curriculum_min_evaluations": 3,
            "curriculum_performance_check_interval": 3  # æ›´é¢‘ç¹æ£€æŸ¥
        },
        
        "guidance_config": {
            # æ›´ç§¯æçš„æŒ‡å¯¼ç­–ç•¥
            "enable_streaming_guidance": True,
            "min_reasoning_length": 80,      # å¢åŠ æœ€å°æ¨ç†é•¿åº¦
            "guidance_trigger_threshold": 30, # é™ä½è§¦å‘é˜ˆå€¼
            "max_guidance_attempts": 3,      # å¢åŠ æœ€å¤§å°è¯•æ¬¡æ•°
            "guidance_tokens_limit": 40      # å¢åŠ æŒ‡å¯¼tokené™åˆ¶
        }
    }
    
    return config

def generate_fixed_training_command():
    """ç”Ÿæˆä¿®å¤åçš„è®­ç»ƒå‘½ä»¤"""
    config = create_optimized_training_config()
    
    command = f"""torchrun \\
    --nproc_per_node 2 \\
    --master_addr localhost \\
    --master_port 14524 \\
    /home/qhy/Research/LLM/GRPO-Clean-2/main.py \\
    --hf_endpoint "https://hf-mirror.com" \\
    --http_proxy "http://10.130.148.206:7890" \\
    --https_proxy "http://10.130.148.206:7890" \\
    --wandb_project "VerilogGRPO_Enhanced_8B_Fixed" \\
    --wandb_entity "qhy0227-tsinghua-university" \\
    --wandb_run_name_prefix "qwen3-fixed-v1" \\
    --model_name_or_path "{config['model_config']['model_name_or_path']}" \\
    --stage1_adapter_path "/home/share/ReasoningV/Qwen3/ckpts/sft-qwen3-lora-5epoch-origen200k-S1-20250429_211831/checkpoint-34695/" \\
    --dataset_path "/home/qhy/Research/LLM/GRPO-RV/dataset/all-with-module-2.jsonl" \\
    --dataset_base_path /home/qhy/Research/LLM/GRPO-RV/dataset \\
    --output_dir_base "./enhanced_grpo_8B_runs_fixed" \\
    --lora_rank 16 \\
    --lora_alpha 32 \\
    --lora_dropout 0.05 \\
    --lora_target_modules q_proj k_proj v_proj o_proj gate_proj up_proj down_proj \\
    --max_seq_length 5120 \\
    --script_max_prompt_length 1024 \\
    --script_max_completion_length 4096 \\
    --length_allocation_strategy custom \\
    --callback_num_samples 2 \\
    --callback_eval_every_n_steps 3 \\
    --learning_rate {config['training_config']['learning_rate']} \\
    --warmup_ratio {config['training_config']['warmup_ratio']} \\
    --lr_scheduler_type {config['training_config']['lr_scheduler_type']} \\
    --per_device_train_batch_size {config['training_config']['per_device_train_batch_size']} \\
    --gradient_accumulation_steps {config['training_config']['gradient_accumulation_steps']} \\
    --gen_temperature {config['training_config']['gen_temperature']} \\
    --gen_top_k {config['training_config']['gen_top_k']} \\
    --gen_top_p {config['training_config']['gen_top_p']} \\
    --gen_repetition_penalty {config['training_config']['gen_repetition_penalty']} \\
    --compilation_success {config['reward_config']['compilation_success']} \\
    --compilation_failure {config['reward_config']['compilation_failure']} \\
    --simulation_crash {config['reward_config']['simulation_crash']} \\
    --test_pass_base_reward {config['reward_config']['test_pass_base_reward']} \\
    --test_pass_bonus_multiplier {config['reward_config']['test_pass_bonus_multiplier']} \\
    --max_functional_reward {config['reward_config']['max_functional_reward']} \\
    --all_tests_passed_bonus {config['reward_config']['all_tests_passed_bonus']} \\
    --functional_weight {config['reward_config']['functional_weight']} \\
    --efficiency_weight {config['reward_config']['efficiency_weight']} \\
    --readability_weight {config['reward_config']['readability_weight']} \\
    --robustness_weight {config['reward_config']['robustness_weight']} \\
    --curriculum_performance_threshold_1 {config['curriculum_config']['curriculum_performance_threshold_1']} \\
    --curriculum_performance_threshold_2 {config['curriculum_config']['curriculum_performance_threshold_2']} \\
    --curriculum_performance_threshold_3 {config['curriculum_config']['curriculum_performance_threshold_3']} \\
    --curriculum_min_evaluations {config['curriculum_config']['curriculum_min_evaluations']} \\
    --curriculum_performance_check_interval {config['curriculum_config']['curriculum_performance_check_interval']} \\
    --enable_streaming_guidance {str(config['guidance_config']['enable_streaming_guidance']).lower()} \\
    --min_reasoning_length {config['guidance_config']['min_reasoning_length']} \\
    --guidance_trigger_threshold {config['guidance_config']['guidance_trigger_threshold']} \\
    --max_guidance_attempts {config['guidance_config']['max_guidance_attempts']} \\
    --guidance_tokens_limit {config['guidance_config']['guidance_tokens_limit']} \\
    --num_train_epochs 4 \\
    --logging_strategy "steps" \\
    --logging_steps 3 \\
    --save_strategy "steps" \\
    --save_steps 15 \\
    --save_total_limit 5 \\
    --report_to "wandb" \\
    --remove_unused_columns False \\
    --num_generations 2 \\
    --max_prompt_length 1024 \\
    --max_completion_length 4096 \\
    --optim "adamw_torch" \\
    --ddp_find_unused_parameters False \\
    --seed 42 \\
    --dataloader_pin_memory \\
    --bf16 \\
    --cache_dir "/home/qhy/Research/LLM/GRPO-Clean-2/.enhanced_cache_v2/models"
"""
    
    return command

if __name__ == "__main__":
    print("ğŸ”§ Qwen3é…ç½®ä¿®å¤å·¥å…·")
    print("=" * 50)
    
    # 1. ä¿®å¤tokenizeré…ç½®
    model_path = "/home/share/Qwen3-8B/models--Qwen--Qwen3-8B/snapshots/a80f5e57cce20e57b65145f4213844dec1a80834"
    cache_dir = "/home/qhy/Research/LLM/GRPO-Clean-2/.enhanced_cache_v2/models"
    
    success = fix_qwen3_tokenizer(model_path, cache_dir)
    
    if success:
        print("\nâœ… Qwen3 tokenizeré…ç½®ä¿®å¤æˆåŠŸï¼")
    else:
        print("\nâŒ Qwen3 tokenizeré…ç½®ä¿®å¤å¤±è´¥ï¼")
    
    # 2. ç”Ÿæˆä¼˜åŒ–çš„è®­ç»ƒé…ç½®
    print("\nğŸ“ ç”Ÿæˆä¼˜åŒ–çš„è®­ç»ƒé…ç½®...")
    config = create_optimized_training_config()
    
    with open("optimized_qwen3_config.json", "w", encoding="utf-8") as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    print("âœ… é…ç½®å·²ä¿å­˜åˆ°: optimized_qwen3_config.json")
    
    # 3. ç”Ÿæˆä¿®å¤åçš„è®­ç»ƒå‘½ä»¤
    print("\nğŸš€ ç”Ÿæˆä¿®å¤åçš„è®­ç»ƒå‘½ä»¤...")
    command = generate_fixed_training_command()
    
    with open("fixed_training_command.sh", "w", encoding="utf-8") as f:
        f.write("#!/bin/bash\n")
        f.write("# Qwen3 ä¼˜åŒ–è®­ç»ƒå‘½ä»¤\n")
        f.write("# ä¿®å¤äº†å­¦ä¹ ç‡ã€æ‰¹å¤„ç†å¤§å°ã€å¥–åŠ±å‡½æ•°å’Œè¯¾ç¨‹å­¦ä¹ é˜ˆå€¼\n\n")
        f.write(command)
    
    print("âœ… è®­ç»ƒå‘½ä»¤å·²ä¿å­˜åˆ°: fixed_training_command.sh")
    print("\nğŸ¯ ä¸»è¦ä¿®å¤å†…å®¹:")
    print("   - å­¦ä¹ ç‡: 6e-6 â†’ 2e-5")
    print("   - æ‰¹å¤„ç†å¤§å°: 1 â†’ 2")
    print("   - æ¢¯åº¦ç´¯ç§¯: 8 â†’ 16")
    print("   - ç¼–è¯‘æˆåŠŸå¥–åŠ±: 2.0 â†’ 3.0")
    print("   - ç¼–è¯‘å¤±è´¥æƒ©ç½š: -4.0 â†’ -2.0")
    print("   - è¯¾ç¨‹å­¦ä¹ é˜ˆå€¼: 0.75 â†’ 0.4")
    print("   - æ¨ç†é•¿åº¦è¦æ±‚: 60 â†’ 80")
    
    print("\nğŸ”§ ä¸‹ä¸€æ­¥æ“ä½œ:")
    print("   1. è¿è¡Œæ­¤è„šæœ¬éªŒè¯tokenizeré…ç½®")
    print("   2. ä½¿ç”¨ç”Ÿæˆçš„ fixed_training_command.sh é‡æ–°è®­ç»ƒ")
    print("   3. ç›‘æ§è®­ç»ƒæ—¥å¿—ä¸­çš„ç¼–è¯‘æˆåŠŸç‡å’Œå¥–åŠ±å˜åŒ–") 