#!/usr/bin/env python3

import os
import torch
import sys
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import get_peft_model, LoraConfig

print("ğŸ”§ è®¾å¤‡ä¸€è‡´æ€§æµ‹è¯•")
print("=" * 60)

# æ£€æŸ¥åŸºç¡€ç¯å¢ƒ
print("1. åŸºç¡€ç¯å¢ƒæ£€æŸ¥:")
print(f"   CUDAå¯ç”¨: {torch.cuda.is_available()}")
print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")

if torch.cuda.device_count() < 2:
    print("âŒ éœ€è¦è‡³å°‘2å¼ GPUè¿›è¡Œæµ‹è¯•")
    exit(1)

# è®¾ç½®è®¾å¤‡æ˜ å°„
device_map = {
    'model.embed_tokens': 'cuda:0',
    'model.layers.0': 'cuda:0',
    'model.layers.1': 'cuda:0',
    'model.layers.2': 'cuda:1',
    'model.layers.3': 'cuda:1',
    'model.norm': 'cuda:1',
    'lm_head': 'cuda:1'
}

print("\n2. æ¨¡å‹åŠ è½½æµ‹è¯•:")
model_path = "/home/share/Qwen3-8B/models--Qwen--Qwen3-8B/snapshots/a80f5e57cce20e57b65145f4213844dec1a80834"

try:
    # åŠ è½½tokenizer
    print("   åŠ è½½tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½æ¨¡å‹ï¼ˆæ¨¡æ‹Ÿå¤šGPUåˆ†å¸ƒï¼‰
    print("   åŠ è½½æ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",  # è®©ç³»ç»Ÿè‡ªåŠ¨åˆ†é…
        max_memory={0: "35GiB", 1: "35GiB"}
    )
    
    print("   æ¨¡å‹è®¾å¤‡åˆ†å¸ƒ:")
    if hasattr(model, 'hf_device_map'):
        device_counts = {}
        for layer, device in model.hf_device_map.items():
            device_str = str(device)
            device_counts[device_str] = device_counts.get(device_str, 0) + 1
        
        for device, count in device_counts.items():
            print(f"     {device}: {count} å±‚")
    
    # åº”ç”¨LoRA
    print("   åº”ç”¨LoRA...")
    lora_config = LoraConfig(
        r=32,
        lora_alpha=64,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        lora_dropout=0.05,
        task_type="CAUSAL_LM"
    )
    model = get_peft_model(model, lora_config)
    
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    
except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    exit(1)

print("\n3. è®¾å¤‡ä¸€è‡´æ€§æµ‹è¯•:")

# åˆ›å»ºæµ‹è¯•è¾“å…¥
test_text = "Hello, this is a test."
inputs = tokenizer(test_text, return_tensors="pt", padding=True)

print(f"   è¾“å…¥è®¾å¤‡: {inputs['input_ids'].device}")

# æµ‹è¯•æ¨ç†
try:
    print("   æµ‹è¯•å‰å‘ä¼ æ’­...")
    
    # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
    input_device = next(model.parameters()).device
    print(f"   æ¨¡å‹ä¸»è®¾å¤‡: {input_device}")
    
    # å°†è¾“å…¥ç§»åŠ¨åˆ°æ¨¡å‹çš„ç¬¬ä¸€ä¸ªå‚æ•°æ‰€åœ¨è®¾å¤‡
    inputs_on_device = {k: v.to(input_device) for k, v in inputs.items()}
    print(f"   è¾“å…¥ç§»åŠ¨åˆ°: {inputs_on_device['input_ids'].device}")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        outputs = model(**inputs_on_device)
    
    print(f"   è¾“å‡ºè®¾å¤‡: {outputs.logits.device}")
    print(f"   è¾“å‡ºå½¢çŠ¶: {outputs.logits.shape}")
    print("âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
    
except Exception as e:
    print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()

print("\n4. ç”Ÿæˆæµ‹è¯•:")

try:
    print("   æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ...")
    
    # å‡†å¤‡ç”Ÿæˆè¾“å…¥
    prompt = "The future of AI is"
    inputs = tokenizer(prompt, return_tensors="pt")
    
    # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®è®¾å¤‡
    input_device = next(model.parameters()).device
    inputs = {k: v.to(input_device) for k, v in inputs.items()}
    
    # ç”Ÿæˆ
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=10,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )
    
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"   ç”Ÿæˆæ–‡æœ¬: {generated_text}")
    print("âœ… æ–‡æœ¬ç”ŸæˆæˆåŠŸ")
    
except Exception as e:
    print(f"âŒ æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {e}")
    import traceback
    traceback.print_exc()

print("\n" + "=" * 60)
print("è®¾å¤‡ä¸€è‡´æ€§æµ‹è¯•å®Œæˆ")

# å†…å­˜æ¸…ç†
del model
del tokenizer
torch.cuda.empty_cache()
print("âœ… å†…å­˜æ¸…ç†å®Œæˆ") 